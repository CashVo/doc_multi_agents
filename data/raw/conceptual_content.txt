 

source: https://pytorch.org/tutorials/beginner/basics/intro.html 
content: 

Note
Click here
to download the full example code
Learn the Basics ||
Quickstart ||
Tensors ||
Datasets & DataLoaders ||
Transforms ||
Build Model ||
Autograd ||
Optimization ||
Save & Load Model
Learn the Basics¶
Authors:
Suraj Subramanian,
Seth Juarez,
Cassie Breviu,
Dmitry Soshnikov,
Ari Bornstein
Most machine learning workflows involve working with data, creating models, optimizing model
parameters, and saving the trained models. This tutorial introduces you to a complete ML workflow
implemented in PyTorch, with links to learn more about each of these concepts.
We’ll use the FashionMNIST dataset to train a neural network that predicts if an input image belongs
to one of the following classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker,
Bag, or Ankle boot.
This tutorial assumes a basic familiarity with Python and Deep Learning concepts.
Running the Tutorial Code¶
You can run this tutorial in a couple of ways:
In the cloud: This is the easiest way to get started! Each section has a “Run in Microsoft Learn” and “Run in Google Colab” link at the top, which opens an integrated notebook in Microsoft Learn or Google Colab, respectively, with the code in a fully-hosted environment.
Locally: This option requires you to setup PyTorch and TorchVision first on your local machine (installation instructions). Download the notebook or copy the code into your favorite IDE.
How to Use this Guide¶
If you’re familiar with other deep learning frameworks, check out the 0. Quickstart first
to quickly familiarize yourself with PyTorch’s API.
If you’re new to deep learning frameworks, head right into the first section of our step-by-step guide: 1. Tensors.
Total running time of the script: ( 0 minutes  0.000 seconds)
Download Python source code: intro.py
Download Jupyter notebook: intro.ipynb
Gallery generated by Sphinx-Gallery 

source: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html 
content: 

Deep Learning with PyTorch: A 60 Minute Blitz¶
Author: Soumith Chintala
What is PyTorch?¶
PyTorch is a Python-based scientific computing package serving two broad purposes:
A replacement for NumPy to use the power of GPUs and other accelerators.
An automatic differentiation library that is useful to implement neural networks.
Goal of this tutorial:¶
Understand PyTorch’s Tensor library and neural networks at a high level.
Train a small neural network to classify images
To run the tutorials below, make sure you have the torch, torchvision,
and matplotlib packages installed.
In this tutorial, you will learn the basics of PyTorch tensors.
 Code
Learn about autograd.
 Code
This tutorial demonstrates how you can train neural networks in PyTorch.
 Code
Learn how to train an image classifier in PyTorch by using the
CIFAR10 dataset.
 Code 

source: https://pytorch.org/tutorials/beginner/nn_tutorial.html 
content: 

Note
Click here
to download the full example code
What is torch.nn really?¶
Authors: Jeremy Howard, fast.ai. Thanks to Rachel Thomas and Francisco Ingham.
We recommend running this tutorial as a notebook, not a script. To download the notebook (.ipynb) file,
click the link at the top of the page.
PyTorch provides the elegantly designed modules and classes torch.nn ,
torch.optim ,
Dataset ,
and DataLoader
to help you create and train neural networks.
In order to fully utilize their power and customize
them for your problem, you need to really understand exactly what they’re
doing. To develop this understanding, we will first train basic neural net
on the MNIST data set without using any features from these models; we will
initially only use the most basic PyTorch tensor functionality. Then, we will
incrementally add one feature from torch.nn, torch.optim, Dataset, or
DataLoader at a time, showing exactly what each piece does, and how it
works to make the code either more concise, or more flexible.
This tutorial assumes you already have PyTorch installed, and are familiar
with the basics of tensor operations. (If you’re familiar with Numpy array
operations, you’ll find the PyTorch tensor operations used here nearly identical).
MNIST data setup¶
We will use the classic MNIST dataset,
which consists of black-and-white images of hand-drawn digits (between 0 and 9).
We will use pathlib
for dealing with paths (part of the Python 3 standard library), and will
download the dataset using
requests. We will only
import modules when we use them, so you can see exactly what’s being
used at each point.
from pathlib import Path
import requests

DATA_PATH = Path("data")
PATH = DATA_PATH / "mnist"

PATH.mkdir(parents=True, exist_ok=True)

URL = "https://github.com/pytorch/tutorials/raw/main/_static/"
FILENAME = "mnist.pkl.gz"

if not (PATH / FILENAME).exists():
        content = requests.get(URL + FILENAME).content
        (PATH / FILENAME).open("wb").write(content)

This dataset is in numpy array format, and has been stored using pickle,
a python-specific format for serializing data.
import pickle
import gzip

with gzip.open((PATH / FILENAME).as_posix(), "rb") as f:
        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding="latin-1")

Each image is 28 x 28, and is being stored as a flattened row of length
784 (=28x28). Let’s take a look at one; we need to reshape it to 2d
first.
from matplotlib import pyplot
import numpy as np

pyplot.imshow(x_train[0].reshape((28, 28)), cmap="gray")
# ``pyplot.show()`` only if not on Colab
try:
    import google.colab
except ImportError:
    pyplot.show()
print(x_train.shape)

(50000, 784)

PyTorch uses torch.tensor, rather than numpy arrays, so we need to
convert our data.
import torch

x_train, y_train, x_valid, y_valid = map(
    torch.tensor, (x_train, y_train, x_valid, y_valid)
)
n, c = x_train.shape
print(x_train, y_train)
print(x_train.shape)
print(y_train.min(), y_train.max())

tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])
torch.Size([50000, 784])
tensor(0) tensor(9)

Neural net from scratch (without torch.nn)¶
Let’s first create a model using nothing but PyTorch tensor operations. We’re assuming
you’re already familiar with the basics of neural networks. (If you’re not, you can
learn them at course.fast.ai).
PyTorch provides methods to create random or zero-filled tensors, which we will
use to create our weights and bias for a simple linear model. These are just regular
tensors, with one very special addition: we tell PyTorch that they require a
gradient. This causes PyTorch to record all of the operations done on the tensor,
so that it can calculate the gradient during back-propagation automatically!
For the weights, we set requires_grad after the initialization, since we
don’t want that step included in the gradient. (Note that a trailing _ in
PyTorch signifies that the operation is performed in-place.)
Note
We are initializing the weights here with
Xavier initialisation
(by multiplying with 1/sqrt(n)).
import math

weights = torch.randn(784, 10) / math.sqrt(784)
weights.requires_grad_()
bias = torch.zeros(10, requires_grad=True)

Thanks to PyTorch’s ability to calculate gradients automatically, we can
use any standard Python function (or callable object) as a model! So
let’s just write a plain matrix multiplication and broadcasted addition
to create a simple linear model. We also need an activation function, so
we’ll write log_softmax and use it. Remember: although PyTorch
provides lots of prewritten loss functions, activation functions, and
so forth, you can easily write your own using plain python. PyTorch will
even create fast GPU or vectorized CPU code for your function
automatically.
def log_softmax(x):
    return x - x.exp().sum(-1).log().unsqueeze(-1)

def model(xb):
    return log_softmax(xb @ weights + bias)

In the above, the @ stands for the matrix multiplication operation. We will call
our function on one batch of data (in this case, 64 images).  This is
one forward pass.  Note that our predictions won’t be any better than
random at this stage, since we start with random weights.
bs = 64  # batch size

xb = x_train[0:bs]  # a mini-batch from x
preds = model(xb)  # predictions
preds[0], preds.shape
print(preds[0], preds.shape)

tensor([-2.5452, -2.0790, -2.1832, -2.6221, -2.3670, -2.3854, -2.9432, -2.4391,
        -1.8657, -2.0355], grad_fn=<SelectBackward0>) torch.Size([64, 10])

As you see, the preds tensor contains not only the tensor values, but also a
gradient function. We’ll use this later to do backprop.
Let’s implement negative log-likelihood to use as the loss function
(again, we can just use standard Python):
def nll(input, target):
    return -input[range(target.shape[0]), target].mean()

loss_func = nll

Let’s check our loss with our random model, so we can see if we improve
after a backprop pass later.
yb = y_train[0:bs]
print(loss_func(preds, yb))

tensor(2.4020, grad_fn=<NegBackward0>)

Let’s also implement a function to calculate the accuracy of our model.
For each prediction, if the index with the largest value matches the
target value, then the prediction was correct.
def accuracy(out, yb):
    preds = torch.argmax(out, dim=1)
    return (preds == yb).float().mean()

Let’s check the accuracy of our random model, so we can see if our
accuracy improves as our loss improves.
print(accuracy(preds, yb))

tensor(0.0938)

We can now run a training loop.  For each iteration, we will:
select a mini-batch of data (of size bs)
use the model to make predictions
calculate the loss
loss.backward() updates the gradients of the model, in this case, weights
and bias.
We now use these gradients to update the weights and bias.  We do this
within the torch.no_grad() context manager, because we do not want these
actions to be recorded for our next calculation of the gradient.  You can read
more about how PyTorch’s Autograd records operations
here.
We then set the
gradients to zero, so that we are ready for the next loop.
Otherwise, our gradients would record a running tally of all the operations
that had happened (i.e. loss.backward() adds the gradients to whatever is
already stored, rather than replacing them).
Tip
You can use the standard python debugger to step through PyTorch
code, allowing you to check the various variable values at each step.
Uncomment set_trace() below to try it out.
from IPython.core.debugger import set_trace

lr = 0.5  # learning rate
epochs = 2  # how many epochs to train for

for epoch in range(epochs):
    for i in range((n - 1) // bs + 1):
        #         set_trace()
        start_i = i * bs
        end_i = start_i + bs
        xb = x_train[start_i:end_i]
        yb = y_train[start_i:end_i]
        pred = model(xb)
        loss = loss_func(pred, yb)

        loss.backward()
        with torch.no_grad():
            weights -= weights.grad * lr
            bias -= bias.grad * lr
            weights.grad.zero_()
            bias.grad.zero_()

That’s it: we’ve created and trained a minimal neural network (in this case, a
logistic regression, since we have no hidden layers) entirely from scratch!
Let’s check the loss and accuracy and compare those to what we got
earlier. We expect that the loss will have decreased and accuracy to
have increased, and they have.
print(loss_func(model(xb), yb), accuracy(model(xb), yb))

tensor(0.0813, grad_fn=<NegBackward0>) tensor(1.)

Using torch.nn.functional¶
We will now refactor our code, so that it does the same thing as before, only
we’ll start taking advantage of PyTorch’s nn classes to make it more concise
and flexible. At each step from here, we should be making our code one or more
of: shorter, more understandable, and/or more flexible.
The first and easiest step is to make our code shorter by replacing our
hand-written activation and loss functions with those from torch.nn.functional
(which is generally imported into the namespace F by convention). This module
contains all the functions in the torch.nn library (whereas other parts of the
library contain classes). As well as a wide range of loss and activation
functions, you’ll also find here some convenient functions for creating neural
nets, such as pooling functions. (There are also functions for doing convolutions,
linear layers, etc, but as we’ll see, these are usually better handled using
other parts of the library.)
If you’re using negative log likelihood loss and log softmax activation,
then Pytorch provides a single function F.cross_entropy that combines
the two. So we can even remove the activation function from our model.
import torch.nn.functional as F

loss_func = F.cross_entropy

def model(xb):
    return xb @ weights + bias

Note that we no longer call log_softmax in the model function. Let’s
confirm that our loss and accuracy are the same as before:
print(loss_func(model(xb), yb), accuracy(model(xb), yb))

tensor(0.0813, grad_fn=<NllLossBackward0>) tensor(1.)

Refactor using nn.Module¶
Next up, we’ll use nn.Module and nn.Parameter, for a clearer and more
concise training loop. We subclass nn.Module (which itself is a class and
able to keep track of state).  In this case, we want to create a class that
holds our weights, bias, and method for the forward step.  nn.Module has a
number of attributes and methods (such as .parameters() and .zero_grad())
which we will be using.
Note
nn.Module (uppercase M) is a PyTorch specific concept, and is a
class we’ll be using a lot. nn.Module is not to be confused with the Python
concept of a (lowercase m) module,
which is a file of Python code that can be imported.
from torch import nn

class Mnist_Logistic(nn.Module):
    def __init__(self):
        super().__init__()
        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))
        self.bias = nn.Parameter(torch.zeros(10))

    def forward(self, xb):
        return xb @ self.weights + self.bias

Since we’re now using an object instead of just using a function, we
first have to instantiate our model:
model = Mnist_Logistic()

Now we can calculate the loss in the same way as before. Note that
nn.Module objects are used as if they are functions (i.e they are
callable), but behind the scenes Pytorch will call our forward
method automatically.
print(loss_func(model(xb), yb))

tensor(2.3096, grad_fn=<NllLossBackward0>)

Previously for our training loop we had to update the values for each parameter
by name, and manually zero out the grads for each parameter separately, like this:
with torch.no_grad():
    weights -= weights.grad * lr
    bias -= bias.grad * lr
    weights.grad.zero_()
    bias.grad.zero_()

Now we can take advantage of model.parameters() and model.zero_grad() (which
are both defined by PyTorch for nn.Module) to make those steps more concise
and less prone to the error of forgetting some of our parameters, particularly
if we had a more complicated model:
with torch.no_grad():
    for p in model.parameters(): p -= p.grad * lr
    model.zero_grad()

We’ll wrap our little training loop in a fit function so we can run it
again later.
def fit():
    for epoch in range(epochs):
        for i in range((n - 1) // bs + 1):
            start_i = i * bs
            end_i = start_i + bs
            xb = x_train[start_i:end_i]
            yb = y_train[start_i:end_i]
            pred = model(xb)
            loss = loss_func(pred, yb)

            loss.backward()
            with torch.no_grad():
                for p in model.parameters():
                    p -= p.grad * lr
                model.zero_grad()

fit()

Let’s double-check that our loss has gone down:
print(loss_func(model(xb), yb))

tensor(0.0821, grad_fn=<NllLossBackward0>)

Refactor using nn.Linear¶
We continue to refactor our code.  Instead of manually defining and
initializing self.weights and self.bias, and calculating xb  @
self.weights + self.bias, we will instead use the Pytorch class
nn.Linear for a
linear layer, which does all that for us. Pytorch has many types of
predefined layers that can greatly simplify our code, and often makes it
faster too.
class Mnist_Logistic(nn.Module):
    def __init__(self):
        super().__init__()
        self.lin = nn.Linear(784, 10)

    def forward(self, xb):
        return self.lin(xb)

We instantiate our model and calculate the loss in the same way as before:
model = Mnist_Logistic()
print(loss_func(model(xb), yb))

tensor(2.3313, grad_fn=<NllLossBackward0>)

We are still able to use our same fit method as before.
fit()

print(loss_func(model(xb), yb))

tensor(0.0819, grad_fn=<NllLossBackward0>)

Refactor using torch.optim¶
Pytorch also has a package with various optimization algorithms, torch.optim.
We can use the step method from our optimizer to take a forward step, instead
of manually updating each parameter.
This will let us replace our previous manually coded optimization step:
with torch.no_grad():
    for p in model.parameters(): p -= p.grad * lr
    model.zero_grad()

and instead use just:
opt.step()
opt.zero_grad()

(optim.zero_grad() resets the gradient to 0 and we need to call it before
computing the gradient for the next minibatch.)
from torch import optim

We’ll define a little function to create our model and optimizer so we
can reuse it in the future.
def get_model():
    model = Mnist_Logistic()
    return model, optim.SGD(model.parameters(), lr=lr)

model, opt = get_model()
print(loss_func(model(xb), yb))

for epoch in range(epochs):
    for i in range((n - 1) // bs + 1):
        start_i = i * bs
        end_i = start_i + bs
        xb = x_train[start_i:end_i]
        yb = y_train[start_i:end_i]
        pred = model(xb)
        loss = loss_func(pred, yb)

        loss.backward()
        opt.step()
        opt.zero_grad()

print(loss_func(model(xb), yb))

tensor(2.2659, grad_fn=<NllLossBackward0>)
tensor(0.0810, grad_fn=<NllLossBackward0>)

Refactor using Dataset¶
PyTorch has an abstract Dataset class.  A Dataset can be anything that has
a __len__ function (called by Python’s standard len function) and
a __getitem__ function as a way of indexing into it.
This tutorial
walks through a nice example of creating a custom FacialLandmarkDataset class
as a subclass of Dataset.
PyTorch’s TensorDataset
is a Dataset wrapping tensors. By defining a length and way of indexing,
this also gives us a way to iterate, index, and slice along the first
dimension of a tensor. This will make it easier to access both the
independent and dependent variables in the same line as we train.
from torch.utils.data import TensorDataset

Both x_train and y_train can be combined in a single TensorDataset,
which will be easier to iterate over and slice.
train_ds = TensorDataset(x_train, y_train)

Previously, we had to iterate through minibatches of x and y values separately:
xb = x_train[start_i:end_i]
yb = y_train[start_i:end_i]

Now, we can do these two steps together:
xb,yb = train_ds[i*bs : i*bs+bs]

model, opt = get_model()

for epoch in range(epochs):
    for i in range((n - 1) // bs + 1):
        xb, yb = train_ds[i * bs: i * bs + bs]
        pred = model(xb)
        loss = loss_func(pred, yb)

        loss.backward()
        opt.step()
        opt.zero_grad()

print(loss_func(model(xb), yb))

tensor(0.0826, grad_fn=<NllLossBackward0>)

Refactor using DataLoader¶
PyTorch’s DataLoader is responsible for managing batches. You can
create a DataLoader from any Dataset. DataLoader makes it easier
to iterate over batches. Rather than having to use train_ds[i*bs : i*bs+bs],
the DataLoader gives us each minibatch automatically.
from torch.utils.data import DataLoader

train_ds = TensorDataset(x_train, y_train)
train_dl = DataLoader(train_ds, batch_size=bs)

Previously, our loop iterated over batches (xb, yb) like this:
for i in range((n-1)//bs + 1):
    xb,yb = train_ds[i*bs : i*bs+bs]
    pred = model(xb)

Now, our loop is much cleaner, as (xb, yb) are loaded automatically from the data loader:
for xb,yb in train_dl:
    pred = model(xb)

model, opt = get_model()

for epoch in range(epochs):
    for xb, yb in train_dl:
        pred = model(xb)
        loss = loss_func(pred, yb)

        loss.backward()
        opt.step()
        opt.zero_grad()

print(loss_func(model(xb), yb))

tensor(0.0818, grad_fn=<NllLossBackward0>)

Thanks to PyTorch’s nn.Module, nn.Parameter, Dataset, and DataLoader,
our training loop is now dramatically smaller and easier to understand. Let’s
now try to add the basic features necessary to create effective models in practice.
Add validation¶
In section 1, we were just trying to get a reasonable training loop set up for
use on our training data.  In reality, you always should also have
a validation set, in order
to identify if you are overfitting.
Shuffling the training data is
important
to prevent correlation between batches and overfitting. On the other hand, the
validation loss will be identical whether we shuffle the validation set or not.
Since shuffling takes extra time, it makes no sense to shuffle the validation data.
We’ll use a batch size for the validation set that is twice as large as
that for the training set. This is because the validation set does not
need backpropagation and thus takes less memory (it doesn’t need to
store the gradients). We take advantage of this to use a larger batch
size and compute the loss more quickly.
train_ds = TensorDataset(x_train, y_train)
train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)

valid_ds = TensorDataset(x_valid, y_valid)
valid_dl = DataLoader(valid_ds, batch_size=bs * 2)

We will calculate and print the validation loss at the end of each epoch.
(Note that we always call model.train() before training, and model.eval()
before inference, because these are used by layers such as nn.BatchNorm2d
and nn.Dropout to ensure appropriate behavior for these different phases.)
model, opt = get_model()

for epoch in range(epochs):
    model.train()
    for xb, yb in train_dl:
        pred = model(xb)
        loss = loss_func(pred, yb)

        loss.backward()
        opt.step()
        opt.zero_grad()

    model.eval()
    with torch.no_grad():
        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)

    print(epoch, valid_loss / len(valid_dl))

0 tensor(0.3048)
1 tensor(0.2872)

Create fit() and get_data()¶
We’ll now do a little refactoring of our own. Since we go through a similar
process twice of calculating the loss for both the training set and the
validation set, let’s make that into its own function, loss_batch, which
computes the loss for one batch.
We pass an optimizer in for the training set, and use it to perform
backprop.  For the validation set, we don’t pass an optimizer, so the
method doesn’t perform backprop.
def loss_batch(model, loss_func, xb, yb, opt=None):
    loss = loss_func(model(xb), yb)

    if opt is not None:
        loss.backward()
        opt.step()
        opt.zero_grad()

    return loss.item(), len(xb)

fit runs the necessary operations to train our model and compute the
training and validation losses for each epoch.
import numpy as np

def fit(epochs, model, loss_func, opt, train_dl, valid_dl):
    for epoch in range(epochs):
        model.train()
        for xb, yb in train_dl:
            loss_batch(model, loss_func, xb, yb, opt)

        model.eval()
        with torch.no_grad():
            losses, nums = zip(
                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]
            )
        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)

        print(epoch, val_loss)

get_data returns dataloaders for the training and validation sets.
def get_data(train_ds, valid_ds, bs):
    return (
        DataLoader(train_ds, batch_size=bs, shuffle=True),
        DataLoader(valid_ds, batch_size=bs * 2),
    )

Now, our whole process of obtaining the data loaders and fitting the
model can be run in 3 lines of code:
train_dl, valid_dl = get_data(train_ds, valid_ds, bs)
model, opt = get_model()
fit(epochs, model, loss_func, opt, train_dl, valid_dl)

0 0.2939354367017746
1 0.3258970756947994

You can use these basic 3 lines of code to train a wide variety of models.
Let’s see if we can use them to train a convolutional neural network (CNN)!
Switch to CNN¶
We are now going to build our neural network with three convolutional layers.
Because none of the functions in the previous section assume anything about
the model form, we’ll be able to use them to train a CNN without any modification.
We will use PyTorch’s predefined
Conv2d class
as our convolutional layer. We define a CNN with 3 convolutional layers.
Each convolution is followed by a ReLU.  At the end, we perform an
average pooling.  (Note that view is PyTorch’s version of Numpy’s
reshape)
class Mnist_CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)
        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)
        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)

    def forward(self, xb):
        xb = xb.view(-1, 1, 28, 28)
        xb = F.relu(self.conv1(xb))
        xb = F.relu(self.conv2(xb))
        xb = F.relu(self.conv3(xb))
        xb = F.avg_pool2d(xb, 4)
        return xb.view(-1, xb.size(1))

lr = 0.1

Momentum is a variation on
stochastic gradient descent that takes previous updates into account as well
and generally leads to faster training.
model = Mnist_CNN()
opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)

fit(epochs, model, loss_func, opt, train_dl, valid_dl)

0 0.3646130460739136
1 0.26228193019628526

Using nn.Sequential¶
torch.nn has another handy class we can use to simplify our code:
Sequential .
A Sequential object runs each of the modules contained within it, in a
sequential manner. This is a simpler way of writing our neural network.
To take advantage of this, we need to be able to easily define a
custom layer from a given function.  For instance, PyTorch doesn’t
have a view layer, and we need to create one for our network. Lambda
will create a layer that we can then use when defining a network with
Sequential.
class Lambda(nn.Module):
    def __init__(self, func):
        super().__init__()
        self.func = func

    def forward(self, x):
        return self.func(x)


def preprocess(x):
    return x.view(-1, 1, 28, 28)

The model created with Sequential is simple:
model = nn.Sequential(
    Lambda(preprocess),
    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),
    nn.ReLU(),
    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),
    nn.ReLU(),
    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),
    nn.ReLU(),
    nn.AvgPool2d(4),
    Lambda(lambda x: x.view(x.size(0), -1)),
)

opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)

fit(epochs, model, loss_func, opt, train_dl, valid_dl)

0 0.3330025281429291
1 0.22993727023601532

Wrapping DataLoader¶
Our CNN is fairly concise, but it only works with MNIST, because:

It assumes the input is a 28*28 long vector
It assumes that the final CNN grid size is 4*4 (since that’s the average pooling kernel size we used)


It assumes the input is a 28*28 long vector
It assumes that the final CNN grid size is 4*4 (since that’s the average pooling kernel size we used)
Let’s get rid of these two assumptions, so our model works with any 2d
single channel image. First, we can remove the initial Lambda layer by
moving the data preprocessing into a generator:
def preprocess(x, y):
    return x.view(-1, 1, 28, 28), y


class WrappedDataLoader:
    def __init__(self, dl, func):
        self.dl = dl
        self.func = func

    def __len__(self):
        return len(self.dl)

    def __iter__(self):
        for b in self.dl:
            yield (self.func(*b))

train_dl, valid_dl = get_data(train_ds, valid_ds, bs)
train_dl = WrappedDataLoader(train_dl, preprocess)
valid_dl = WrappedDataLoader(valid_dl, preprocess)

Next, we can replace nn.AvgPool2d with nn.AdaptiveAvgPool2d, which
allows us to define the size of the output tensor we want, rather than
the input tensor we have. As a result, our model will work with any
size input.
model = nn.Sequential(
    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),
    nn.ReLU(),
    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),
    nn.ReLU(),
    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),
    nn.ReLU(),
    nn.AdaptiveAvgPool2d(1),
    Lambda(lambda x: x.view(x.size(0), -1)),
)

opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)

Let’s try it out:
fit(epochs, model, loss_func, opt, train_dl, valid_dl)

0 0.3212135115623474
1 0.21439074140787123

Using your GPU¶
If you’re lucky enough to have access to a CUDA-capable GPU (you can
rent one for about $0.50/hour from most cloud providers) you can
use it to speed up your code. First check that your GPU is working in
Pytorch:
print(torch.cuda.is_available())

True

And then create a device object for it:
dev = torch.device(
    "cuda") if torch.cuda.is_available() else torch.device("cpu")

Let’s update preprocess to move batches to the GPU:
def preprocess(x, y):
    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)


train_dl, valid_dl = get_data(train_ds, valid_ds, bs)
train_dl = WrappedDataLoader(train_dl, preprocess)
valid_dl = WrappedDataLoader(valid_dl, preprocess)

Finally, we can move our model to the GPU.
model.to(dev)
opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)

You should find it runs faster now:
fit(epochs, model, loss_func, opt, train_dl, valid_dl)

0 0.18051417416334153
1 0.16745494151115417

Closing thoughts¶
We now have a general data pipeline and training loop which you can use for
training many types of models using Pytorch. To see how simple training a model
can now be, take a look at the mnist_sample notebook.
Of course, there are many things you’ll want to add, such as data augmentation,
hyperparameter tuning, monitoring training, transfer learning, and so forth.
These features are available in the fastai library, which has been developed
using the same design approach shown in this tutorial, providing a natural
next step for practitioners looking to take their models further.
We promised at the start of this tutorial we’d explain through example each of
torch.nn, torch.optim, Dataset, and DataLoader. So let’s summarize
what we’ve seen:
torch.nn:
Module: creates a callable which behaves like a function, but can also
contain state(such as neural net layer weights). It knows what Parameter (s) it
contains and can zero all their gradients, loop through them for weight updates, etc.
Parameter: a wrapper for a tensor that tells a Module that it has weights
that need updating during backprop. Only tensors with the requires_grad attribute set are updated
functional: a module(usually imported into the F namespace by convention)
which contains activation functions, loss functions, etc, as well as non-stateful
versions of layers such as convolutional and linear layers.
torch.optim: Contains optimizers such as SGD, which update the weights
of Parameter during the backward step
Dataset: An abstract interface of objects with a __len__ and a __getitem__,
including classes provided with Pytorch such as TensorDataset
DataLoader: Takes any Dataset and creates an iterator which returns batches of data.
Total running time of the script: ( 0 minutes  36.362 seconds)
Download Python source code: nn_tutorial.py
Download Jupyter notebook: nn_tutorial.ipynb
Gallery generated by Sphinx-Gallery 

source: https://pytorch.org/tutorials/intermediate/nlp_from_scratch_index.html 
content: 

NLP from Scratch¶
In these three-part series you will build and train
a basic character-level Recurrent Neural Network (RNN) to classify words.
You will learn:
How to construct Recurrent Neural Networks from scratch
Essential data handling techniques for NLP
How to train an RNN to identify the language origin of words.
Before you begin, we recommend that you review the following:
PyTorch Learn the Basics series
How to install PyTorch
Learn how to use an RNN to classify names into their language of origin.
 Code
Expand the RNN we created in Part 1 to generate names from languages.
 Code
Create a sequence-to-sequence model that can translate your text from French
to English.
 Code 

source: https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html 
content: 

Visualizing Models, Data, and Training with TensorBoard¶
In the 60 Minute Blitz,
we show you how to load in data,
feed it through a model we define as a subclass of nn.Module,
train this model on training data, and test it on test data.
To see what’s happening, we print out some statistics as the model
is training to get a sense for whether training is progressing.
However, we can do much better than that: PyTorch integrates with
TensorBoard, a tool designed for visualizing the results of neural
network training runs. This tutorial illustrates some of its
functionality, using the
Fashion-MNIST dataset
which can be read into PyTorch using torchvision.datasets.
In this tutorial, we’ll learn how to:
Read in data and with appropriate transforms (nearly identical to the prior tutorial).
Set up TensorBoard.
Write to TensorBoard.
Inspect a model architecture using TensorBoard.
Use TensorBoard to create interactive versions of the visualizations we created in last tutorial, with less code
Specifically, on point #5, we’ll see:
A couple of ways to inspect our training data
How to track our model’s performance as it trains
How to assess our model’s performance once it is trained.
We’ll begin with similar boilerplate code as in the CIFAR-10 tutorial:
# imports
import matplotlib.pyplot as plt
import numpy as np

import torch
import torchvision
import torchvision.transforms as transforms

import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# transforms
transform = transforms.Compose(
    [transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))])

# datasets
trainset = torchvision.datasets.FashionMNIST('./data',
    download=True,
    train=True,
    transform=transform)
testset = torchvision.datasets.FashionMNIST('./data',
    download=True,
    train=False,
    transform=transform)

# dataloaders
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                        shuffle=True, num_workers=2)


testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                        shuffle=False, num_workers=2)

# constant for classes
classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')

# helper function to show an image
# (used in the `plot_classes_preds` function below)
def matplotlib_imshow(img, one_channel=False):
    if one_channel:
        img = img.mean(dim=0)
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    if one_channel:
        plt.imshow(npimg, cmap="Greys")
    else:
        plt.imshow(np.transpose(npimg, (1, 2, 0)))

We’ll define a similar model architecture from that tutorial, making only
minor modifications to account for the fact that the images are now
one channel instead of three and 28x28 instead of 32x32:
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()

We’ll define the same optimizer and criterion from before:
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

1. TensorBoard setup¶
Now we’ll set up TensorBoard, importing tensorboard from torch.utils and defining a
SummaryWriter, our key object for writing information to TensorBoard.
from torch.utils.tensorboard import SummaryWriter

# default `log_dir` is "runs" - we'll be more specific here
writer = SummaryWriter('runs/fashion_mnist_experiment_1')

Note that this line alone creates a runs/fashion_mnist_experiment_1
folder.
2. Writing to TensorBoard¶
Now let’s write an image to our TensorBoard - specifically, a grid -
using make_grid.
# get some random training images
dataiter = iter(trainloader)
images, labels = next(dataiter)

# create grid of images
img_grid = torchvision.utils.make_grid(images)

# show images
matplotlib_imshow(img_grid, one_channel=True)

# write to tensorboard
writer.add_image('four_fashion_mnist_images', img_grid)

Now running
tensorboard --logdir=runs

from the command line and then navigating to http://localhost:6006
should show the following.
Now you know how to use TensorBoard! This example, however, could be
done in a Jupyter Notebook - where TensorBoard really excels is in
creating interactive visualizations. We’ll cover one of those next,
and several more by the end of the tutorial.
3. Inspect the model using TensorBoard¶
One of TensorBoard’s strengths is its ability to visualize complex model
structures. Let’s visualize the model we built.
writer.add_graph(net, images)
writer.close()

Now upon refreshing TensorBoard you should see a “Graphs” tab that
looks like this:
Go ahead and double click on “Net” to see it expand, seeing a
detailed view of the individual operations that make up the model.
TensorBoard has a very handy feature for visualizing high dimensional
data such as image data in a lower dimensional space; we’ll cover this
next.
4. Adding a “Projector” to TensorBoard¶
We can visualize the lower dimensional representation of higher
dimensional data via the add_embedding method
# helper function
def select_n_random(data, labels, n=100):
    '''
    Selects n random datapoints and their corresponding labels from a dataset
    '''
    assert len(data) == len(labels)

    perm = torch.randperm(len(data))
    return data[perm][:n], labels[perm][:n]

# select random images and their target indices
images, labels = select_n_random(trainset.data, trainset.targets)

# get the class labels for each image
class_labels = [classes[lab] for lab in labels]

# log embeddings
features = images.view(-1, 28 * 28)
writer.add_embedding(features,
                    metadata=class_labels,
                    label_img=images.unsqueeze(1))
writer.close()

Now in the “Projector” tab of TensorBoard, you can see these 100
images - each of which is 784 dimensional - projected down into three
dimensional space. Furthermore, this is interactive: you can click
and drag to rotate the three dimensional projection. Finally, a couple
of tips to make the visualization easier to see: select “color: label”
on the top left, as well as enabling “night mode”, which will make the
images easier to see since their background is white:
Now we’ve thoroughly inspected our data, let’s show how TensorBoard
can make tracking model training and evaluation clearer, starting with
training.
5. Tracking model training with TensorBoard¶
In the previous example, we simply printed the model’s running loss
every 2000 iterations. Now, we’ll instead log the running loss to
TensorBoard, along with a view into the predictions the model is
making via the plot_classes_preds function.
# helper functions

def images_to_probs(net, images):
    '''
    Generates predictions and corresponding probabilities from a trained
    network and a list of images
    '''
    output = net(images)
    # convert output probabilities to predicted class
    _, preds_tensor = torch.max(output, 1)
    preds = np.squeeze(preds_tensor.numpy())
    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]


def plot_classes_preds(net, images, labels):
    '''
    Generates matplotlib Figure using a trained network, along with images
    and labels from a batch, that shows the network's top prediction along
    with its probability, alongside the actual label, coloring this
    information based on whether the prediction was correct or not.
    Uses the "images_to_probs" function.
    '''
    preds, probs = images_to_probs(net, images)
    # plot the images in the batch, along with predicted and true labels
    fig = plt.figure(figsize=(12, 48))
    for idx in np.arange(4):
        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])
        matplotlib_imshow(images[idx], one_channel=True)
        ax.set_title("{0}, {1:.1f}%\n(label: {2})".format(
            classes[preds[idx]],
            probs[idx] * 100.0,
            classes[labels[idx]]),
                    color=("green" if preds[idx]==labels[idx].item() else "red"))
    return fig

Finally, let’s train the model using the same model training code from
the prior tutorial, but writing results to TensorBoard every 1000
batches instead of printing to console; this is done using the
add_scalar
function.
In addition, as we train, we’ll generate an image showing the model’s
predictions vs. the actual results on the four images included in that
batch.
running_loss = 0.0
for epoch in range(1):  # loop over the dataset multiple times

    for i, data in enumerate(trainloader, 0):

        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 1000 == 999:    # every 1000 mini-batches...

            # ...log the running loss
            writer.add_scalar('training loss',
                            running_loss / 1000,
                            epoch * len(trainloader) + i)

            # ...log a Matplotlib Figure showing the model's predictions on a
            # random mini-batch
            writer.add_figure('predictions vs. actuals',
                            plot_classes_preds(net, inputs, labels),
                            global_step=epoch * len(trainloader) + i)
            running_loss = 0.0
print('Finished Training')

You can now look at the scalars tab to see the running loss plotted
over the 15,000 iterations of training:
In addition, we can look at the predictions the model made on
arbitrary batches throughout learning. See the “Images” tab and scroll
down under the “predictions vs. actuals” visualization to see this;
this shows us that, for example, after just 3000 training iterations,
the model was already able to distinguish between visually distinct
classes such as shirts, sneakers, and coats, though it isn’t as
confident as it becomes later on in training:
In the prior tutorial, we looked at per-class accuracy once the model
had been trained; here, we’ll use TensorBoard to plot precision-recall
curves (good explanation
here)
for each class.
6. Assessing trained models with TensorBoard¶
# 1. gets the probability predictions in a test_size x num_classes Tensor
# 2. gets the preds in a test_size Tensor
# takes ~10 seconds to run
class_probs = []
class_label = []
with torch.no_grad():
    for data in testloader:
        images, labels = data
        output = net(images)
        class_probs_batch = [F.softmax(el, dim=0) for el in output]

        class_probs.append(class_probs_batch)
        class_label.append(labels)

test_probs = torch.cat([torch.stack(batch) for batch in class_probs])
test_label = torch.cat(class_label)

# helper function
def add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):
    '''
    Takes in a "class_index" from 0 to 9 and plots the corresponding
    precision-recall curve
    '''
    tensorboard_truth = test_label == class_index
    tensorboard_probs = test_probs[:, class_index]

    writer.add_pr_curve(classes[class_index],
                        tensorboard_truth,
                        tensorboard_probs,
                        global_step=global_step)
    writer.close()

# plot all the pr curves
for i in range(len(classes)):
    add_pr_curve_tensorboard(i, test_probs, test_label)

You will now see a “PR Curves” tab that contains the precision-recall
curves for each class. Go ahead and poke around; you’ll see that on
some classes the model has nearly 100% “area under the curve”,
whereas on others this area is lower:
And that’s an intro to TensorBoard and PyTorch’s integration with it.
Of course, you could do everything TensorBoard does in your Jupyter
Notebook, but with TensorBoard, you gets visuals that are interactive
by default. 

source: https://pytorch.org/tutorials/intermediate/pinmem_nonblock.html 
content: 

Note
Click here
to download the full example code
A guide on good usage of non_blocking and pin_memory() in PyTorch¶
Author: Vincent Moens
Introduction¶
Transferring data from the CPU to the GPU is fundamental in many PyTorch applications.
It’s crucial for users to understand the most effective tools and options available for moving data between devices.
This tutorial examines two key methods for device-to-device data transfer in PyTorch:
pin_memory() and to() with the non_blocking=True option.
What you will learn¶
Optimizing the transfer of tensors from the CPU to the GPU can be achieved through asynchronous transfers and memory
pinning. However, there are important considerations:
Using tensor.pin_memory().to(device, non_blocking=True) can be up to twice as slow as a straightforward tensor.to(device).
Generally, tensor.to(device, non_blocking=True) is an effective choice for enhancing transfer speed.
While cpu_tensor.to("cuda", non_blocking=True).mean() executes correctly, attempting
cuda_tensor.to("cpu", non_blocking=True).mean() will result in erroneous outputs.
Preamble¶
The performance reported in this tutorial are conditioned on the system used to build the tutorial.
Although the conclusions are applicable across different systems, the specific observations may vary slightly
depending on the hardware available, especially on older hardware.
The primary objective of this tutorial is to offer a theoretical framework for understanding CPU to GPU data transfers.
However, any design decisions should be tailored to individual cases and guided by benchmarked throughput measurements,
as well as the specific requirements of the task at hand.
import torch

assert torch.cuda.is_available(), "A cuda device is required to run this tutorial"

This tutorial requires tensordict to be installed. If you don’t have tensordict in your environment yet, install it
by running the following command in a separate cell:
# Install tensordict with the following command
!pip3 install tensordict

We start by outlining the theory surrounding these concepts, and then move to concrete test examples of the features.
Background¶
Memory management basics¶
When one creates a CPU tensor in PyTorch, the content of this tensor needs to be placed
in memory. The memory we talk about here is a rather complex concept worth looking at carefully.
We distinguish two types of memory that are handled by the Memory Management Unit: the RAM (for simplicity)
and the swap space on disk (which may or may not be the hard drive). Together, the available space in disk and RAM (physical memory)
make up the virtual memory, which is an abstraction of the total resources available.
In short, the virtual memory makes it so that the available space is larger than what can be found on RAM in isolation
and creates the illusion that the main memory is larger than it actually is.
In normal circumstances, a regular CPU tensor is pageable which means that it is divided in blocks called pages that
can live anywhere in the virtual memory (both in RAM or on disk). As mentioned earlier, this has the advantage that
the memory seems larger than what the main memory actually is.
Typically, when a program accesses a page that is not in RAM, a “page fault” occurs and the operating system (OS) then brings
back this page into RAM (“swap in” or “page in”).
In turn, the OS may have to swap out (or “page out”) another page to make room for the new page.
In contrast to pageable memory, a pinned (or page-locked or non-pageable) memory is a type of memory that cannot
be swapped out to disk.
It allows for faster and more predictable access times, but has the downside that it is more limited than the
pageable memory (aka the main memory).
CUDA and (non-)pageable memory¶
To understand how CUDA copies a tensor from CPU to CUDA, let’s consider the two scenarios above:
If the memory is page-locked, the device can access the memory directly in the main memory. The memory addresses are well
defined and functions that need to read these data can be significantly accelerated.
If the memory is pageable, all the pages will have to be brought to the main memory before being sent to the GPU.
This operation may take time and is less predictable than when executed on page-locked tensors.
More precisely, when CUDA sends pageable data from CPU to GPU, it must first create a page-locked copy of that data
before making the transfer.
Asynchronous vs. Synchronous Operations with non_blocking=True (CUDA cudaMemcpyAsync)¶
When executing a copy from a host (e.g., CPU) to a device (e.g., GPU), the CUDA toolkit offers modalities to do these
operations synchronously or asynchronously with respect to the host.
In practice, when calling to(), PyTorch always makes a call to
cudaMemcpyAsync.
If non_blocking=False (default), a cudaStreamSynchronize will be called after each and every cudaMemcpyAsync, making
the call to to() blocking in the main thread.
If non_blocking=True, no synchronization is triggered, and the main thread on the host is not blocked.
Therefore, from the host perspective, multiple tensors can be sent to the device simultaneously,
as the thread does not need to wait for one transfer to be completed to initiate the other.
Note
In general, the transfer is blocking on the device side (even if it isn’t on the host side):
the copy on the device cannot occur while another operation is being executed.
However, in some advanced scenarios, a copy and a kernel execution can be done simultaneously on the GPU side.
As the following example will show, three requirements must be met to enable this:
The device must have at least one free DMA (Direct Memory Access) engine. Modern GPU architectures such as Volterra,
Tesla, or H100 devices have more than one DMA engine.
The transfer must be done on a separate, non-default cuda stream. In PyTorch, cuda streams can be handles using
Stream.
The source data must be in pinned memory.
We demonstrate this by running profiles on the following script.
import contextlib

from torch.cuda import Stream


s = Stream()

torch.manual_seed(42)
t1_cpu_pinned = torch.randn(1024**2 * 5, pin_memory=True)
t2_cpu_paged = torch.randn(1024**2 * 5, pin_memory=False)
t3_cuda = torch.randn(1024**2 * 5, device="cuda:0")

assert torch.cuda.is_available()
device = torch.device("cuda", torch.cuda.current_device())


# The function we want to profile
def inner(pinned: bool, streamed: bool):
    with torch.cuda.stream(s) if streamed else contextlib.nullcontext():
        if pinned:
            t1_cuda = t1_cpu_pinned.to(device, non_blocking=True)
        else:
            t2_cuda = t2_cpu_paged.to(device, non_blocking=True)
        t_star_cuda_h2d_event = s.record_event()
    # This operation can be executed during the CPU to GPU copy if and only if the tensor is pinned and the copy is
    #  done in the other stream
    t3_cuda_mul = t3_cuda * t3_cuda * t3_cuda
    t3_cuda_h2d_event = torch.cuda.current_stream().record_event()
    t_star_cuda_h2d_event.synchronize()
    t3_cuda_h2d_event.synchronize()


# Our profiler: profiles the `inner` function and stores the results in a .json file
def benchmark_with_profiler(
    pinned,
    streamed,
) -> None:
    torch._C._profiler._set_cuda_sync_enabled_val(True)
    wait, warmup, active = 1, 1, 2
    num_steps = wait + warmup + active
    rank = 0
    with torch.profiler.profile(
        activities=[
            torch.profiler.ProfilerActivity.CPU,
            torch.profiler.ProfilerActivity.CUDA,
        ],
        schedule=torch.profiler.schedule(
            wait=wait, warmup=warmup, active=active, repeat=1, skip_first=1
        ),
    ) as prof:
        for step_idx in range(1, num_steps + 1):
            inner(streamed=streamed, pinned=pinned)
            if rank is None or rank == 0:
                prof.step()
    prof.export_chrome_trace(f"trace_streamed{int(streamed)}_pinned{int(pinned)}.json")

Loading these profile traces in chrome (chrome://tracing) shows the following results: first, let’s see
what happens if both the arithmetic operation on t3_cuda is executed after the pageable tensor is sent to GPU
in the main stream:
benchmark_with_profiler(streamed=False, pinned=False)

Using a pinned tensor doesn’t change the trace much, both operations are still executed consecutively:
benchmark_with_profiler(streamed=False, pinned=True)

Sending a pageable tensor to GPU on a separate stream is also a blocking operation:
benchmark_with_profiler(streamed=True, pinned=False)

Only pinned tensors copies to GPU on a separate stream overlap with another cuda kernel executed on
the main stream:
benchmark_with_profiler(streamed=True, pinned=True)

A PyTorch perspective¶
pin_memory()¶
PyTorch offers the possibility to create and send tensors to page-locked memory through the
pin_memory() method and constructor arguments.
CPU tensors on a machine where CUDA is initialized can be cast to pinned memory through the pin_memory()
method. Importantly, pin_memory is blocking on the main thread of the host: it will wait for the tensor to be copied to
page-locked memory before executing the next operation.
New tensors can be directly created in pinned memory with functions like zeros(), ones() and other
constructors.
Let us check the speed of pinning memory and sending tensors to CUDA:
import torch
import gc
from torch.utils.benchmark import Timer
import matplotlib.pyplot as plt


def timer(cmd):
    median = (
        Timer(cmd, globals=globals())
        .adaptive_autorange(min_run_time=1.0, max_run_time=20.0)
        .median
        * 1000
    )
    print(f"{cmd}: {median: 4.4f} ms")
    return median


# A tensor in pageable memory
pageable_tensor = torch.randn(1_000_000)

# A tensor in page-locked (pinned) memory
pinned_tensor = torch.randn(1_000_000, pin_memory=True)

# Runtimes:
pageable_to_device = timer("pageable_tensor.to('cuda:0')")
pinned_to_device = timer("pinned_tensor.to('cuda:0')")
pin_mem = timer("pageable_tensor.pin_memory()")
pin_mem_to_device = timer("pageable_tensor.pin_memory().to('cuda:0')")

# Ratios:
r1 = pinned_to_device / pageable_to_device
r2 = pin_mem_to_device / pageable_to_device

# Create a figure with the results
fig, ax = plt.subplots()

xlabels = [0, 1, 2]
bar_labels = [
    "pageable_tensor.to(device) (1x)",
    f"pinned_tensor.to(device) ({r1:4.2f}x)",
    f"pageable_tensor.pin_memory().to(device) ({r2:4.2f}x)"
    f"\npin_memory()={100*pin_mem/pin_mem_to_device:.2f}% of runtime.",
]
values = [pageable_to_device, pinned_to_device, pin_mem_to_device]
colors = ["tab:blue", "tab:red", "tab:orange"]
ax.bar(xlabels, values, label=bar_labels, color=colors)

ax.set_ylabel("Runtime (ms)")
ax.set_title("Device casting runtime (pin-memory)")
ax.set_xticks([])
ax.legend()

plt.show()

# Clear tensors
del pageable_tensor, pinned_tensor
_ = gc.collect()

pageable_tensor.to('cuda:0'):  0.7791 ms
pinned_tensor.to('cuda:0'):  0.6806 ms
pageable_tensor.pin_memory():  0.3651 ms
pageable_tensor.pin_memory().to('cuda:0'):  1.0569 ms

We can observe that casting a pinned-memory tensor to GPU is indeed much faster than a pageable tensor, because under
the hood, a pageable tensor must be copied to pinned memory before being sent to GPU.
However, contrary to a somewhat common belief, calling pin_memory() on a pageable tensor before
casting it to GPU should not bring any significant speed-up, on the contrary this call is usually slower than just
executing the transfer. This makes sense, since we’re actually asking Python to execute an operation that CUDA will
perform anyway before copying the data from host to device.
Note
The PyTorch implementation of
pin_memory
which relies on creating a brand new storage in pinned memory through cudaHostAlloc
could be, in rare cases, faster than transitioning data in chunks as cudaMemcpy does.
Here too, the observation may vary depending on the available hardware, the size of the tensors being sent or
the amount of available RAM.
non_blocking=True¶
As mentioned earlier, many PyTorch operations have the option of being executed asynchronously with respect to the host
through the non_blocking argument.
Here, to account accurately of the benefits of using non_blocking, we will design a slightly more complex
experiment since we want to assess how fast it is to send multiple tensors to GPU with and without calling
non_blocking.
# A simple loop that copies all tensors to cuda
def copy_to_device(*tensors):
    result = []
    for tensor in tensors:
        result.append(tensor.to("cuda:0"))
    return result


# A loop that copies all tensors to cuda asynchronously
def copy_to_device_nonblocking(*tensors):
    result = []
    for tensor in tensors:
        result.append(tensor.to("cuda:0", non_blocking=True))
    # We need to synchronize
    torch.cuda.synchronize()
    return result


# Create a list of tensors
tensors = [torch.randn(1000) for _ in range(1000)]
to_device = timer("copy_to_device(*tensors)")
to_device_nonblocking = timer("copy_to_device_nonblocking(*tensors)")

# Ratio
r1 = to_device_nonblocking / to_device

# Plot the results
fig, ax = plt.subplots()

xlabels = [0, 1]
bar_labels = [f"to(device) (1x)", f"to(device, non_blocking=True) ({r1:4.2f}x)"]
colors = ["tab:blue", "tab:red"]
values = [to_device, to_device_nonblocking]

ax.bar(xlabels, values, label=bar_labels, color=colors)

ax.set_ylabel("Runtime (ms)")
ax.set_title("Device casting runtime (non-blocking)")
ax.set_xticks([])
ax.legend()

plt.show()

copy_to_device(*tensors):  24.9430 ms
copy_to_device_nonblocking(*tensors):  18.6000 ms

To get a better sense of what is happening here, let us profile these two functions:
from torch.profiler import profile, ProfilerActivity


def profile_mem(cmd):
    with profile(activities=[ProfilerActivity.CPU]) as prof:
        exec(cmd)
    print(cmd)
    print(prof.key_averages().table(row_limit=10))

Let’s see the call stack with a regular to(device) first:
print("Call to `to(device)`", profile_mem("copy_to_device(*tensors)"))

copy_to_device(*tensors)
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                 aten::to         4.03%       1.196ms       100.00%      29.678ms      29.678us          1000
           aten::_to_copy        13.13%       3.897ms        95.97%      28.482ms      28.482us          1000
      aten::empty_strided        24.76%       7.348ms        24.76%       7.348ms       7.348us          1000
              aten::copy_        19.31%       5.729ms        58.08%      17.237ms      17.237us          1000
          cudaMemcpyAsync        18.28%       5.426ms        18.28%       5.426ms       5.426us          1000
    cudaStreamSynchronize        20.49%       6.081ms        20.49%       6.081ms       6.081us          1000
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 29.678ms

Call to `to(device)` None

and now the non_blocking version:
print(
    "Call to `to(device, non_blocking=True)`",
    profile_mem("copy_to_device_nonblocking(*tensors)"),
)

copy_to_device_nonblocking(*tensors)
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                 aten::to         4.70%       1.065ms        99.90%      22.634ms      22.634us          1000
           aten::_to_copy        16.26%       3.683ms        95.20%      21.569ms      21.569us          1000
      aten::empty_strided        31.54%       7.145ms        31.54%       7.145ms       7.145us          1000
              aten::copy_        22.93%       5.195ms        47.41%      10.741ms      10.741us          1000
          cudaMemcpyAsync        24.48%       5.546ms        24.48%       5.546ms       5.546us          1000
    cudaDeviceSynchronize         0.10%      22.474us         0.10%      22.474us      22.474us             1
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 22.657ms

Call to `to(device, non_blocking=True)` None

The results are without any doubt better when using non_blocking=True, as all transfers are initiated simultaneously
on the host side and only one synchronization is done.
The benefit will vary depending on the number and the size of the tensors as well as depending on the hardware being
used.
Note
Interestingly, the blocking to("cuda") actually performs the same asynchronous device casting operation
(cudaMemcpyAsync) as the one with non_blocking=True with a synchronization point after each copy.
Synergies¶
Now that we have made the point that data transfer of tensors already in pinned memory to GPU is faster than from
pageable memory, and that we know that doing these transfers asynchronously is also faster than synchronously, we can
benchmark combinations of these approaches. First, let’s write a couple of new functions that will call pin_memory
and to(device) on each tensor:
def pin_copy_to_device(*tensors):
    result = []
    for tensor in tensors:
        result.append(tensor.pin_memory().to("cuda:0"))
    return result


def pin_copy_to_device_nonblocking(*tensors):
    result = []
    for tensor in tensors:
        result.append(tensor.pin_memory().to("cuda:0", non_blocking=True))
    # We need to synchronize
    torch.cuda.synchronize()
    return result

The benefits of using pin_memory() are more pronounced for
somewhat large batches of large tensors:
tensors = [torch.randn(1_000_000) for _ in range(1000)]
page_copy = timer("copy_to_device(*tensors)")
page_copy_nb = timer("copy_to_device_nonblocking(*tensors)")

tensors_pinned = [torch.randn(1_000_000, pin_memory=True) for _ in range(1000)]
pinned_copy = timer("copy_to_device(*tensors_pinned)")
pinned_copy_nb = timer("copy_to_device_nonblocking(*tensors_pinned)")

pin_and_copy = timer("pin_copy_to_device(*tensors)")
pin_and_copy_nb = timer("pin_copy_to_device_nonblocking(*tensors)")

# Plot
strategies = ("pageable copy", "pinned copy", "pin and copy")
blocking = {
    "blocking": [page_copy, pinned_copy, pin_and_copy],
    "non-blocking": [page_copy_nb, pinned_copy_nb, pin_and_copy_nb],
}

x = torch.arange(3)
width = 0.25
multiplier = 0


fig, ax = plt.subplots(layout="constrained")

for attribute, runtimes in blocking.items():
    offset = width * multiplier
    rects = ax.bar(x + offset, runtimes, width, label=attribute)
    ax.bar_label(rects, padding=3, fmt="%.2f")
    multiplier += 1

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel("Runtime (ms)")
ax.set_title("Runtime (pin-mem and non-blocking)")
ax.set_xticks([0, 1, 2])
ax.set_xticklabels(strategies)
plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
ax.legend(loc="upper left", ncols=3)

plt.show()

del tensors, tensors_pinned
_ = gc.collect()

copy_to_device(*tensors):  820.7590 ms
copy_to_device_nonblocking(*tensors):  659.2608 ms
copy_to_device(*tensors_pinned):  678.4663 ms
copy_to_device_nonblocking(*tensors_pinned):  654.3293 ms
pin_copy_to_device(*tensors):  1269.7422 ms
pin_copy_to_device_nonblocking(*tensors):  655.5188 ms

Other copy directions (GPU -> CPU, CPU -> MPS)¶
Until now, we have operated under the assumption that asynchronous copies from the CPU to the GPU are safe.
This is generally true because CUDA automatically handles synchronization to ensure that the data being accessed is
valid at read time.
However, this guarantee does not extend to transfers in the opposite direction, from GPU to CPU.
Without explicit synchronization, these transfers offer no assurance that the copy will be complete at the time of
data access. Consequently, the data on the host might be incomplete or incorrect, effectively rendering it garbage:
tensor = (
    torch.arange(1, 1_000_000, dtype=torch.double, device="cuda")
    .expand(100, 999999)
    .clone()
)
torch.testing.assert_close(
    tensor.mean(), torch.tensor(500_000, dtype=torch.double, device="cuda")
), tensor.mean()
try:
    i = -1
    for i in range(100):
        cpu_tensor = tensor.to("cpu", non_blocking=True)
        torch.testing.assert_close(
            cpu_tensor.mean(), torch.tensor(500_000, dtype=torch.double)
        )
    print("No test failed with non_blocking")
except AssertionError:
    print(f"{i}th test failed with non_blocking. Skipping remaining tests")
try:
    i = -1
    for i in range(100):
        cpu_tensor = tensor.to("cpu", non_blocking=True)
        torch.cuda.synchronize()
        torch.testing.assert_close(
            cpu_tensor.mean(), torch.tensor(500_000, dtype=torch.double)
        )
    print("No test failed with synchronize")
except AssertionError:
    print(f"One test failed with synchronize: {i}th assertion!")

0th test failed with non_blocking. Skipping remaining tests
No test failed with synchronize

The same considerations apply to copies from the CPU to non-CUDA devices, such as MPS.
Generally, asynchronous copies to a device are safe without explicit synchronization only when the target is a
CUDA-enabled device.
In summary, copying data from CPU to GPU is safe when using non_blocking=True, but for any other direction,
non_blocking=True can still be used but the user must make sure that a device synchronization is executed before
the data is accessed.
Practical recommendations¶
We can now wrap up some early recommendations based on our observations:
In general, non_blocking=True will provide good throughput, regardless of whether the original tensor is or
isn’t in pinned memory.
If the tensor is already in pinned memory, the transfer can be accelerated, but sending it to
pin memory manually from python main thread is a blocking operation on the host, and hence will annihilate much of
the benefit of using non_blocking=True (as CUDA does the pin_memory transfer anyway).
One might now legitimately ask what use there is for the pin_memory() method.
In the following section, we will explore further how this can be used to accelerate the data transfer even more.
Additional considerations¶
PyTorch notoriously provides a DataLoader class whose constructor accepts a
pin_memory argument.
Considering our previous discussion on pin_memory, you might wonder how the DataLoader manages to
accelerate data transfers if memory pinning is inherently blocking.
The key lies in the DataLoader’s use of a separate thread to handle the transfer of data from pageable to pinned
memory, thus preventing any blockage in the main thread.
To illustrate this, we will use the TensorDict primitive from the homonymous library.
When invoking to(), the default behavior is to send tensors to the device asynchronously,
followed by a single call to torch.device.synchronize() afterwards.
Additionally, TensorDict.to() includes a non_blocking_pin option  which initiates multiple threads to execute
pin_memory() before proceeding with to to(device).
This approach can further accelerate data transfers, as demonstrated in the following example.
from tensordict import TensorDict
import torch
from torch.utils.benchmark import Timer
import matplotlib.pyplot as plt

# Create the dataset
td = TensorDict({str(i): torch.randn(1_000_000) for i in range(1000)})

# Runtimes
copy_blocking = timer("td.to('cuda:0', non_blocking=False)")
copy_non_blocking = timer("td.to('cuda:0')")
copy_pin_nb = timer("td.to('cuda:0', non_blocking_pin=True, num_threads=0)")
copy_pin_multithread_nb = timer("td.to('cuda:0', non_blocking_pin=True, num_threads=4)")

# Rations
r1 = copy_non_blocking / copy_blocking
r2 = copy_pin_nb / copy_blocking
r3 = copy_pin_multithread_nb / copy_blocking

# Figure
fig, ax = plt.subplots()

xlabels = [0, 1, 2, 3]
bar_labels = [
    "Blocking copy (1x)",
    f"Non-blocking copy ({r1:4.2f}x)",
    f"Blocking pin, non-blocking copy ({r2:4.2f}x)",
    f"Non-blocking pin, non-blocking copy ({r3:4.2f}x)",
]
values = [copy_blocking, copy_non_blocking, copy_pin_nb, copy_pin_multithread_nb]
colors = ["tab:blue", "tab:red", "tab:orange", "tab:green"]

ax.bar(xlabels, values, label=bar_labels, color=colors)

ax.set_ylabel("Runtime (ms)")
ax.set_title("Device casting runtime")
ax.set_xticks([])
ax.legend()

plt.show()

td.to('cuda:0', non_blocking=False):  820.9782 ms
td.to('cuda:0'):  659.1453 ms
td.to('cuda:0', non_blocking_pin=True, num_threads=0):  655.0730 ms
td.to('cuda:0', non_blocking_pin=True, num_threads=4):  657.1083 ms

In this example, we are transferring many large tensors from the CPU to the GPU.
This scenario is ideal for utilizing multithreaded pin_memory(), which can significantly enhance performance.
However, if the tensors are small, the overhead associated with multithreading may outweigh the benefits.
Similarly, if there are only a few tensors, the advantages of pinning tensors on separate threads become limited.
As an additional note, while it might seem advantageous to create permanent buffers in pinned memory to shuttle
tensors from pageable memory before transferring them to the GPU, this strategy does not necessarily expedite
computation. The inherent bottleneck caused by copying data into pinned memory remains a limiting factor.
Moreover, transferring data that resides on disk (whether in shared memory or files) to the GPU typically requires an
intermediate step of copying the data into pinned memory (located in RAM).
Utilizing non_blocking for large data transfers in this context can significantly increase RAM consumption,
potentially leading to adverse effects.
In practice, there is no one-size-fits-all solution.
The effectiveness of using multithreaded pin_memory combined with non_blocking transfers depends on a
variety of  factors, including the specific system, operating system, hardware, and the nature of the tasks
being executed.
Here is a list of factors to check when trying to speed-up data transfers between CPU and GPU, or comparing
throughput’s across scenarios:
Number of available cores
How many CPU cores are available? Is the system shared with other users or processes that might compete for
resources?
Core utilization
Are the CPU cores heavily utilized by other processes? Does the application perform other CPU-intensive tasks
concurrently with data transfers?
Memory utilization
How much pageable and page-locked memory is currently being used? Is there sufficient free memory to allocate
additional pinned memory without affecting system performance? Remember that nothing comes for free, for instance
pin_memory will consume RAM and may impact other tasks.
CUDA Device Capabilities
Does the GPU support multiple DMA engines for concurrent data transfers? What are the specific capabilities and
limitations of the CUDA device being used?
Number of tensors to be sent
How many tensors are transferred in a typical operation?
Size of the tensors to be sent
What is the size of the tensors being transferred? A few large tensors or many small tensors may not benefit from
the same transfer program.
System Architecture
How is the system’s architecture influencing data transfer speeds (for example, bus speeds, network latency)?
Additionally, allocating a large number of tensors or sizable tensors in pinned memory can monopolize a substantial
portion of RAM.
This reduces the available memory for other critical operations, such as paging, which can negatively impact the
overall performance of an algorithm.
Conclusion¶
Throughout this tutorial, we have explored several critical factors that influence transfer speeds and memory
management when sending tensors from the host to the device. We’ve learned that using non_blocking=True generally
accelerates data transfers, and that pin_memory() can also enhance performance if implemented
correctly. However, these techniques require careful design and calibration to be effective.
Remember that profiling your code and keeping an eye on the memory consumption are essential to optimize resource
usage and achieve the best possible performance.
Additional resources¶
If you are dealing with issues with memory copies when using CUDA devices or want to learn more about
what was discussed in this tutorial, check the following references:
CUDA toolkit memory management doc;
CUDA pin-memory note;
How to Optimize Data Transfers in CUDA C/C++;
tensordict doc and repo.
Total running time of the script: ( 1 minutes  31.484 seconds)
Download Python source code: pinmem_nonblock.py
Download Jupyter notebook: pinmem_nonblock.ipynb
Gallery generated by Sphinx-Gallery