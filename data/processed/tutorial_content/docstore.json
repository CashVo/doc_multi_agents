{"docstore/metadata": {"76a0bad1-36b2-4021-8ba3-6fd36e412a7f": {"doc_hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5"}, "da00c709-a1a4-4c1e-b746-47d726fed0e5": {"doc_hash": "57e072bcbd7a5fa4fc32cb894ab8c4d1cc96886313a182ef30f30ea1d8a9cd73", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "4b09eb42-e161-45a6-be04-d0879655fe5f": {"doc_hash": "4968e477d8264c1637f208d9aa026ad359c6cee74b9e48c4b119aa20b9c34a93", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "94c83065-d2e4-444c-aee2-d9176406797a": {"doc_hash": "804565797022e37ff4ca31cdf1f28cb00128eb15b90f41d68627e0652652d5c4", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "9f5f2c20-5a40-479b-8a46-170227efa56c": {"doc_hash": "8de9ff1dcc92372c0772fb93899840a6e6965271d087258a6fe7e6100cd742f7", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "27c23573-b9d9-498a-a230-757e93f4ad65": {"doc_hash": "128c4d0638791da4f434250cf460dd7ee18e799bf9ec4466c15c6b48840872d1", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "945787bc-444e-4e08-93fe-4a72f912d1c8": {"doc_hash": "085ea0eb333fb754808b617d545ffa38522ed6b3fa06a862e8828b5194a42476", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "6baec8ba-aa4b-4b43-ac4f-4450fd9a152e": {"doc_hash": "c3b08dc719c4990a7ddf718b872b0097cc02f298d96b86df3bee77f8358af72b", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "f374d1ee-e8ac-4ac2-9779-0902435d1a1c": {"doc_hash": "9bf2b92c01a5eb549873fd7422b09fd698030006aa656f28634b7411be4ffc53", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "1c0bc95d-4ffd-4a1f-b4e1-c29774c185d8": {"doc_hash": "e7a8d2aee67771d07d4735d6caeee379d2f3d8845137bc8f04fd074e59090c3e", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "ca4af97d-30a8-451c-a357-30b35e650e35": {"doc_hash": "49bf46fd8ff69e31a9c4ce72bcee20347cc3fd23b8d28a154e031a1556a0224d", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "8b4fd769-5690-43a0-96ed-7a17d7ea0d4e": {"doc_hash": "ea6840dfbe2b7c591b27eb933d563ab29edfe32731f865a260ea04d01d128f7e", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "9984ef6f-7c69-4fa8-acaf-f5904b603339": {"doc_hash": "339e6fce224e57b0cf011516b5c58232e37f45edc6d6a231d994c9cac37f8391", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "72ff2f24-dde8-4ba1-9c66-7cd48cd74ad3": {"doc_hash": "79d9beebb520f6efe8ee3e401e509a9425765a217f7d27f84729bd15f069efa0", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "82a64506-04b9-4653-93fd-59a6f6c69eb7": {"doc_hash": "de7ecaee4414a425231b77138e34abe577af8cbac9427704dcb7b63be1703c01", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "314d64ef-e47a-4f6c-b3b9-04656a1cc246": {"doc_hash": "6bf20b0ebfbf81f69253c51b9498bb6b79ffb405405304e83e991ec9db863971", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "04d552af-4b8f-46fe-bba4-976d1ea93144": {"doc_hash": "4ed054920e7c3ca2623fcf0984c9f6bf42b6e9a24d0754e7b68a433ee45fbab0", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "b7a3893f-6c5e-40e4-9814-1e6bf88fbd8a": {"doc_hash": "219904f1ce5f8dcf637dd8af409b347ae98312998d80c7586e4475d065812769", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "1f758777-5371-4f9d-b2c5-2547a58f897e": {"doc_hash": "3ba80e92d074bf629fd604d221b43791e16646378f16f506e8263d303176e729", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "f12ebdb1-c3b3-42ba-871f-a5f0cfb9c0b1": {"doc_hash": "90e9b1d22470529e24415f9105b75af962798a4fb4d6a3fb9ccf54aa40bf1f90", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "a9260e27-55a2-4846-9b09-8e26ccdc3fb4": {"doc_hash": "99e9139a9a569864070a893da1ab1e2992cbcb85017924e16fc68738a8f18b46", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "82283394-fb3e-4fbb-ba98-74d379d90e38": {"doc_hash": "52fcf3226350154175bfe197aa2840d6676c1f1ea3c503c8324adce710de6705", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "edbfae17-be4f-4f59-91ed-0180b0e414ee": {"doc_hash": "3ee355d80910ac0228794c3ddcefbba15d12c8ce1d4b413575599b2cee13a780", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "47dd6256-4647-4f38-a5e5-e49c6a9a7944": {"doc_hash": "f14785959fc34ff2ecb2d2d77d8f5005af935e151155eba60d7a7c207a15e631", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "a392b378-61b4-44b5-a9ea-92da83b583e1": {"doc_hash": "beefad6eb9eefe5e78fc2c253ac1712b0fdf7650edbe390c119dfdcd9c2923f4", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "6ee5bf1a-6eae-42ef-9bbd-2cf4d097830f": {"doc_hash": "97c7020cfc52a99c0180231dcd9c3e73c698e75715239abbd4838596063d78d7", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "4d871b0f-0168-4be8-9a9e-d70a4ccc0cfb": {"doc_hash": "51f87586f80dc042b8a426f61e860ffe5a32b2d86b2b18924ffce10c9ad15592", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "1c105476-5e5c-4004-9eea-ff9bdace056a": {"doc_hash": "a753f5a320bda115fe8219fef212a1008f1d2f821bcdec50f814d56d3f62a3fa", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "bedc3f1e-39b4-45d6-8ea0-5709999a3328": {"doc_hash": "75c862bb0b55a021025c2abf278233361a0994f420410996aafcdaa9246c7d74", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "ceba251d-952b-4e73-a6b6-6f49da2bce9e": {"doc_hash": "d8bd7b68bfe14614b13a7e59d4dfd8bfba866a4dd8ebce5aeae4c7372bcd6d71", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "9d8e219b-298e-4c23-9f35-6a911992d386": {"doc_hash": "2239d7a963c358777d29ef633517eceaa96cd96fa2c1a981cdddc91bd22d1d64", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "9c4cfd30-e960-4441-ae38-acd0ebec4c6a": {"doc_hash": "0225482585fa0da87357ac9c8000b8fee2306cabc6f1de1b6ad6f76ba3ebd402", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}, "cf1ab04c-4203-4ac1-81f6-35e0d148a3fa": {"doc_hash": "650273a78a1941387b92b08aa1ff2030dbbb05166d291b460826eb48961f6526", "ref_doc_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f"}}, "docstore/data": {"da00c709-a1a4-4c1e-b746-47d726fed0e5": {"__data__": {"id_": "da00c709-a1a4-4c1e-b746-47d726fed0e5", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b09eb42-e161-45a6-be04-d0879655fe5f", "node_type": "1", "metadata": {}, "hash": "7ce3fc9ad2fb405d54b64421b004603491003f1b6041a77559d841f64aff5a39", "class_name": "RelatedNodeInfo"}}, "text": "source: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html \r\ncontent: \r\n\r\nNote\r\nClick here\r\nto download the full example code\r\nLearn the Basics ||\r\nQuickstart ||\r\nTensors ||\r\nDatasets & DataLoaders ||\r\nTransforms ||\r\nBuild Model ||\r\nAutograd ||\r\nOptimization ||\r\nSave & Load Model\r\nQuickstart\u00b6\r\nThis section runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.\r\nWorking with data\u00b6\r\nPyTorch has two primitives to work with data:\r\ntorch.utils.data.DataLoader and torch.utils.data.Dataset.\r\nDataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around\r\nthe Dataset.\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import datasets\r\nfrom torchvision.transforms import ToTensor\r\n\r\nPyTorch offers domain-specific libraries such as TorchText,\r\nTorchVision, and TorchAudio,\r\nall of which include datasets. For this tutorial, we  will be using a TorchVision dataset.\r\nThe torchvision.datasets module contains Dataset objects for many real-world vision data like\r\nCIFAR, COCO (full list here). In this tutorial, we\r\nuse the FashionMNIST dataset. Every TorchVision Dataset includes two arguments: transform and\r\ntarget_transform to modify the samples and labels respectively.\r\n# Download training data from open datasets.\r\ntraining_data = datasets.FashionMNIST(\r\n    root=\"data\",\r\n    train=True,\r\n    download=True,\r\n    transform=ToTensor(),\r\n)\r\n\r\n# Download test data from open datasets.\r\ntest_data = datasets.FashionMNIST(\r\n    root=\"data\",\r\n    train=False,\r\n    download=True,\r\n    transform=ToTensor(),\r\n)\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\r\n\r\n  0%|          | 0.00/26.4M [00:00<?, ?B/s]\r\n  0%|          | 65.5k/26.4M [00:00<01:11, 367kB/s]\r\n  1%|          | 229k/26.4M [00:00<00:38, 688kB/s]\r\n  3%|3         | 852k/26.4M [00:00<00:10, 2.45MB/s]\r\n  7%|7         | 1.93M/26.4M [00:00<00:05, 4.16MB/s]\r\n 24%|##3       | 6.29M/26.4M [00:00<00:01, 14.7MB/s]\r\n 37%|###7      | 9.86M/26.4M [00:00<00:00, 17.2MB/s]\r\n 54%|#####3    | 14.2M/26.4M [00:00<00:00, 23.7MB/s]\r\n 68%|######7   | 17.9M/26.4M [00:01<00:00, 22.9MB/s]\r\n 87%|########7 | 23.0M/26.4M [00:01<00:00, 29.7MB/s]\r\n100%|##########| 26.4M/26.4M [00:01<00:00, 19.4MB/s]\r\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\r\n\r\n  0%|          | 0.00/29.5k [00:00<?, ?B/s]\r\n100%|##########| 29.5k/29.5k [00:00<00:00, 328kB/s]\r\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.", "mimetype": "text/plain", "start_char_idx": 5, "end_char_idx": 3124, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b09eb42-e161-45a6-be04-d0879655fe5f": {"__data__": {"id_": "4b09eb42-e161-45a6-be04-d0879655fe5f", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da00c709-a1a4-4c1e-b746-47d726fed0e5", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "57e072bcbd7a5fa4fc32cb894ab8c4d1cc96886313a182ef30f30ea1d8a9cd73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94c83065-d2e4-444c-aee2-d9176406797a", "node_type": "1", "metadata": {}, "hash": "8ecfb04221a6337d73933f69c7743767d8ce82f4a5c4eb38e83b263fabd87b1a", "class_name": "RelatedNodeInfo"}}, "text": "s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\r\n\r\n  0%|          | 0.00/29.5k [00:00<?, ?B/s]\r\n100%|##########| 29.5k/29.5k [00:00<00:00, 328kB/s]\r\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\r\n\r\n  0%|          | 0.00/4.42M [00:00<?, ?B/s]\r\n  1%|1         | 65.5k/4.42M [00:00<00:12, 359kB/s]\r\n  5%|5         | 229k/4.42M [00:00<00:06, 677kB/s]\r\n 16%|#5        | 688k/4.42M [00:00<00:02, 1.52MB/s]\r\n 27%|##7       | 1.21M/4.42M [00:00<00:01, 2.05MB/s]\r\n 40%|####      | 1.77M/4.42M [00:00<00:01, 2.41MB/s]\r\n 54%|#####4    | 2.39M/4.42M [00:01<00:00, 2.75MB/s]\r\n 70%|######9   | 3.08M/4.42M [00:01<00:00, 3.09MB/s]\r\n 87%|########6 | 3.83M/4.42M [00:01<00:00, 3.42MB/s]\r\n100%|##########| 4.42M/4.42M [00:01<00:00, 2.85MB/s]\r\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\r\n\r\n  0%|          | 0.00/5.15k [00:00<?, ?B/s]\r\n100%|##########| 5.15k/5.15k [00:00<00:00, 34.8MB/s]\r\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\r\n\r\nWe pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports\r\nautomatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element\r\nin the dataloader iterable will return a batch of 64 features and labels.\r\nbatch_size = 64\r\n\r\n# Create data loaders.\r\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\r\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\r\n\r\nfor X, y in test_dataloader:\r\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\r\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\r\n    break\r\n\r\nShape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\r\nShape of y: torch.Size([64]) torch.int64\r\n\r\nRead more about loading data in PyTorch.\r\nCreating Models\u00b6\r\nTo define a neural network in PyTorch, we create a class that inherits\r\nfrom nn.Module.", "mimetype": "text/plain", "start_char_idx": 2625, "end_char_idx": 5196, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94c83065-d2e4-444c-aee2-d9176406797a": {"__data__": {"id_": "94c83065-d2e4-444c-aee2-d9176406797a", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b09eb42-e161-45a6-be04-d0879655fe5f", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "4968e477d8264c1637f208d9aa026ad359c6cee74b9e48c4b119aa20b9c34a93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f5f2c20-5a40-479b-8a46-170227efa56c", "node_type": "1", "metadata": {}, "hash": "76d0a228be3c129eb12f1d3d02cd394b34aeb7187369472bedcfe910af5f877f", "class_name": "RelatedNodeInfo"}}, "text": "Here we define a batch size of 64, i.e. each element\r\nin the dataloader iterable will return a batch of 64 features and labels.\r\nbatch_size = 64\r\n\r\n# Create data loaders.\r\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\r\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\r\n\r\nfor X, y in test_dataloader:\r\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\r\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\r\n    break\r\n\r\nShape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\r\nShape of y: torch.Size([64]) torch.int64\r\n\r\nRead more about loading data in PyTorch.\r\nCreating Models\u00b6\r\nTo define a neural network in PyTorch, we create a class that inherits\r\nfrom nn.Module. We define the layers of the network\r\nin the __init__ function and specify how data will pass through the network in the forward function. To accelerate\r\noperations in the neural network, we move it to the GPU or MPS if available.\r\n# Get cpu, gpu or mps device for training.\r\ndevice = (\r\n    \"cuda\"\r\n    if torch.cuda.is_available()\r\n    else \"mps\"\r\n    if torch.backends.mps.is_available()\r\n    else \"cpu\"\r\n)\r\nprint(f\"Using {device} device\")\r\n\r\n# Define model\r\nclass NeuralNetwork(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.flatten = nn.Flatten()\r\n        self.linear_relu_stack = nn.Sequential(\r\n            nn.Linear(28*28, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 10)\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.flatten(x)\r\n        logits = self.linear_relu_stack(x)\r\n        return logits\r\n\r\nmodel = NeuralNetwork().to(device)\r\nprint(model)\r\n\r\nUsing cuda device\r\nNeuralNetwork(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_relu_stack): Sequential(\r\n    (0): Linear(in_features=784, out_features=512, bias=True)\r\n    (1): ReLU()\r\n    (2): Linear(in_features=512, out_features=512, bias=True)\r\n    (3): ReLU()\r\n    (4): Linear(in_features=512, out_features=10, bias=True)\r\n  )\r\n)\r\n\r\nRead more about building neural networks in PyTorch.\r\nOptimizing the Model Parameters\u00b6\r\nTo train a model, we need a loss function\r\nand an optimizer.\r\nloss_fn = nn.CrossEntropyLoss()\r\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\r\n\r\nIn a single training loop, the model makes predictions on the training dataset (fed to it in batches), and\r\nbackpropagates the prediction error to adjust the model\u2019s parameters.\r\ndef train(dataloader, model, loss_fn, optimizer):\r\n    size = len(dataloader.dataset)\r\n    model.train()\r\n    for batch, (X, y) in enumerate(dataloader):\r\n        X, y = X.to(device), y.to(device)\r\n\r\n        # Compute prediction error\r\n        pred = model(X)\r\n        loss = loss_fn(pred, y)\r\n\r\n        # Backpropagation\r\n        loss.backward()\r\n        optimizer.step()\r\n        optimizer.zero_grad()\r\n\r\n        if batch % 100 == 0:\r\n            loss, current = loss.item(), (batch + 1) * len(X)\r\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\r\n\r\nWe also check the model\u2019s performance against the test dataset to ensure it is learning.\r\ndef test(dataloader, model, loss_fn):\r\n    size = len(dataloader.dataset)\r\n    num_batches = len(dataloader)\r\n    model.eval()\r\n    test_loss, correct = 0, 0\r\n    with torch.no_grad():\r\n        for X, y in dataloader:\r\n            X, y = X.to(device), y.to(device)\r\n            pred = model(X)\r\n            test_loss += loss_fn(pred, y).item()\r\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\r\n    test_loss /= num_batches\r\n    correct /= size\r\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\r\n\r\nThe training process is conducted over several iterations (epochs). During each epoch, the model learns\r\nparameters to make better predictions. We print the model\u2019s accuracy and loss at each epoch; we\u2019d like to see the\r\naccuracy increase and the loss decrease with every epoch.", "mimetype": "text/plain", "start_char_idx": 4504, "end_char_idx": 8447, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f5f2c20-5a40-479b-8a46-170227efa56c": {"__data__": {"id_": "9f5f2c20-5a40-479b-8a46-170227efa56c", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94c83065-d2e4-444c-aee2-d9176406797a", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "804565797022e37ff4ca31cdf1f28cb00128eb15b90f41d68627e0652652d5c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27c23573-b9d9-498a-a230-757e93f4ad65", "node_type": "1", "metadata": {}, "hash": "b9476018eb4f9ffc4ff35e2e45f5be6135a0984a101b7063bf7231b59b564b3b", "class_name": "RelatedNodeInfo"}}, "text": "During each epoch, the model learns\r\nparameters to make better predictions. We print the model\u2019s accuracy and loss at each epoch; we\u2019d like to see the\r\naccuracy increase and the loss decrease with every epoch.\r\nepochs = 5\r\nfor t in range(epochs):\r\n    print(f\"Epoch {t+1}\\n-------------------------------\")\r\n    train(train_dataloader, model, loss_fn, optimizer)\r\n    test(test_dataloader, model, loss_fn)\r\nprint(\"Done!\")\r\n\r\nEpoch 1\r\n-------------------------------\r\nloss: 2.303494  [   64/60000]\r\nloss: 2.294637  [ 6464/60000]\r\nloss: 2.277102  [12864/60000]\r\nloss: 2.269977  [19264/60000]\r\nloss: 2.254235  [25664/60000]\r\nloss: 2.237146  [32064/60000]\r\nloss: 2.231055  [38464/60000]\r\nloss: 2.205037  [44864/60000]\r\nloss: 2.203240  [51264/60000]\r\nloss: 2.170889  [57664/60000]\r\nTest Error:\r\n Accuracy: 53.9%, Avg loss: 2.168588\r\n\r\nEpoch 2\r\n-------------------------------\r\nloss: 2.177787  [   64/60000]\r\nloss: 2.168083  [ 6464/60000]\r\nloss: 2.114910  [12864/60000]\r\nloss: 2.130412  [19264/60000]\r\nloss: 2.087473  [25664/60000]\r\nloss: 2.039670  [32064/60000]\r\nloss: 2.054274  [38464/60000]\r\nloss: 1.985457  [44864/60000]\r\nloss: 1.996023  [51264/60000]\r\nloss: 1.917241  [57664/60000]\r\nTest Error:\r\n Accuracy: 60.2%, Avg loss: 1.920374\r\n\r\nEpoch 3\r\n-------------------------------\r\nloss: 1.951705  [   64/60000]\r\nloss: 1.919516  [ 6464/60000]\r\nloss: 1.808730  [12864/60000]\r\nloss: 1.846550  [19264/60000]\r\nloss: 1.740618  [25664/60000]\r\nloss: 1.698733  [32064/60000]\r\nloss: 1.708889  [38464/60000]\r\nloss: 1.614436  [44864/60000]\r\nloss: 1.646475  [51264/60000]\r\nloss: 1.524308  [57664/60000]\r\nTest Error:\r\n Accuracy: 61.4%, Avg loss: 1.547092\r\n\r\nEpoch 4\r\n-------------------------------\r\nloss: 1.612695  [   64/60000]\r\nloss: 1.570870  [ 6464/60000]\r\nloss: 1.424730  [12864/60000]\r\nloss: 1.489542  [19264/60000]\r\nloss: 1.367256  [25664/60000]\r\nloss: 1.373464  [32064/60000]\r\nloss: 1.376744  [38464/60000]\r\nloss: 1.304962  [44864/60000]\r\nloss: 1.347154  [51264/60000]\r\nloss: 1.230661  [57664/60000]\r\nTest Error:\r\n Accuracy: 62.7%, Avg loss: 1.260891\r\n\r\nEpoch 5\r\n-------------------------------\r\nloss: 1.337803  [   64/60000]\r\nloss: 1.313278  [ 6464/60000]\r\nloss: 1.151837  [12864/60000]\r\nloss: 1.252142  [19264/60000]\r\nloss: 1.123048  [25664/60000]\r\nloss: 1.159531  [32064/60000]\r\nloss: 1.175011  [38464/60000]\r\nloss: 1.115554  [44864/60000]\r\nloss: 1.160974  [51264/60000]\r\nloss: 1.062730  [57664/60000]\r\nTest Error:\r\n Accuracy: 64.6%, Avg loss: 1.087374\r\n\r\nDone!\r\n\r\nRead more about Training your model.", "mimetype": "text/plain", "start_char_idx": 8238, "end_char_idx": 10733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27c23573-b9d9-498a-a230-757e93f4ad65": {"__data__": {"id_": "27c23573-b9d9-498a-a230-757e93f4ad65", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f5f2c20-5a40-479b-8a46-170227efa56c", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "8de9ff1dcc92372c0772fb93899840a6e6965271d087258a6fe7e6100cd742f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "945787bc-444e-4e08-93fe-4a72f912d1c8", "node_type": "1", "metadata": {}, "hash": "a22ac49c8078295f419b1667a27e962902c6283311dc7ef2d80c96b3a8a5b704", "class_name": "RelatedNodeInfo"}}, "text": "Read more about Training your model.\r\nSaving Models\u00b6\r\nA common way to save a model is to serialize the internal state dictionary (containing the model parameters).\r\ntorch.save(model.state_dict(), \"model.pth\")\r\nprint(\"Saved PyTorch Model State to model.pth\")\r\n\r\nSaved PyTorch Model State to model.pth\r\n\r\nLoading Models\u00b6\r\nThe process for loading a model includes re-creating the model structure and loading\r\nthe state dictionary into it.\r\nmodel = NeuralNetwork().to(device)\r\nmodel.load_state_dict(torch.load(\"model.pth\", weights_only=True))\r\n\r\n<All keys matched successfully>\r\n\r\nThis model can now be used to make predictions.\r\nclasses = [\r\n    \"T-shirt/top\",\r\n    \"Trouser\",\r\n    \"Pullover\",\r\n    \"Dress\",\r\n    \"Coat\",\r\n    \"Sandal\",\r\n    \"Shirt\",\r\n    \"Sneaker\",\r\n    \"Bag\",\r\n    \"Ankle boot\",\r\n]\r\n\r\nmodel.eval()\r\nx, y = test_data[0][0], test_data[0][1]\r\nwith torch.no_grad():\r\n    x = x.to(device)\r\n    pred = model(x)\r\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\r\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\r\n\r\nPredicted: \"Ankle boot\", Actual: \"Ankle boot\"\r\n\r\nRead more about Saving & Loading your model.\r\nTotal running time of the script: ( 1 minutes  1.923 seconds)\r\nDownload Python source code: quickstart_tutorial.py\r\nDownload Jupyter notebook: quickstart_tutorial.ipynb\r\nGallery generated by Sphinx-Gallery \r\n\r\nsource: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html \r\ncontent: \r\n\r\nNote\r\nClick here\r\nto download the full example code\r\nLearn the Basics ||\r\nQuickstart ||\r\nTensors ||\r\nDatasets & DataLoaders ||\r\nTransforms ||\r\nBuild Model ||\r\nAutograd ||\r\nOptimization ||\r\nSave & Load Model\r\nTensors\u00b6\r\nTensors are a specialized data structure that are very similar to arrays and matrices.\r\nIn PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model\u2019s parameters.\r\nTensors are similar to NumPy\u2019s ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and\r\nNumPy arrays can often share the same underlying memory, eliminating the need to copy data (see Bridge with NumPy). Tensors\r\nare also optimized for automatic differentiation (we\u2019ll see more about that later in the Autograd\r\nsection). If you\u2019re familiar with ndarrays, you\u2019ll be right at home with the Tensor API. If not, follow along!\r\nimport torch\r\nimport numpy as np\r\n\r\nInitializing a Tensor\u00b6\r\nTensors can be initialized in various ways. Take a look at the following examples:\r\nDirectly from data\r\nTensors can be created directly from data. The data type is automatically inferred.\r\ndata = [[1, 2],[3, 4]]\r\nx_data = torch.tensor(data)\r\n\r\nFrom a NumPy array\r\nTensors can be created from NumPy arrays (and vice versa - see Bridge with NumPy).\r\nnp_array = np.array(data)\r\nx_np = torch.from_numpy(np_array)\r\n\r\nFrom another tensor:\r\nThe new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.\r\nx_ones = torch.ones_like(x_data) # retains the properties of x_data\r\nprint(f\"Ones Tensor: \\n {x_ones} \\n\")\r\n\r\nx_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\r\nprint(f\"Random Tensor: \\n {x_rand} \\n\")\r\n\r\nOnes Tensor:\r\n tensor([[1, 1],\r\n        [1, 1]])\r\n\r\nRandom Tensor:\r\n tensor([[0.8823, 0.9150],\r\n        [0.3829, 0.9593]])\r\n\r\nWith random or constant values:\r\nshape is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\r\nshape = (2,3,)\r\nrand_tensor = torch.rand(shape)\r\nones_tensor = torch.ones(shape)\r\nzeros_tensor = torch.zeros(shape)\r\n\r\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\r\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\r\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")\r\n\r\nRandom Tensor:\r\n tensor([[0.3904, 0.6009, 0.2566],\r\n        [0.7936, 0.9408, 0.1332]])\r\n\r\nOnes Tensor:\r\n tensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\n\r\nZeros Tensor:\r\n tensor([[0., 0., 0.],\r\n        [0., 0., 0.]])", "mimetype": "text/plain", "start_char_idx": 10697, "end_char_idx": 14615, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "945787bc-444e-4e08-93fe-4a72f912d1c8": {"__data__": {"id_": "945787bc-444e-4e08-93fe-4a72f912d1c8", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27c23573-b9d9-498a-a230-757e93f4ad65", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "128c4d0638791da4f434250cf460dd7ee18e799bf9ec4466c15c6b48840872d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6baec8ba-aa4b-4b43-ac4f-4450fd9a152e", "node_type": "1", "metadata": {}, "hash": "647b1ce1ae3499353722a58f92f3d185e5419745d39c6643eb09d785dc9f62fd", "class_name": "RelatedNodeInfo"}}, "text": "In the functions below, it determines the dimensionality of the output tensor.\r\nshape = (2,3,)\r\nrand_tensor = torch.rand(shape)\r\nones_tensor = torch.ones(shape)\r\nzeros_tensor = torch.zeros(shape)\r\n\r\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\r\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\r\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")\r\n\r\nRandom Tensor:\r\n tensor([[0.3904, 0.6009, 0.2566],\r\n        [0.7936, 0.9408, 0.1332]])\r\n\r\nOnes Tensor:\r\n tensor([[1., 1., 1.],\r\n        [1., 1., 1.]])\r\n\r\nZeros Tensor:\r\n tensor([[0., 0., 0.],\r\n        [0., 0., 0.]])\r\n\r\nAttributes of a Tensor\u00b6\r\nTensor attributes describe their shape, datatype, and the device on which they are stored.\r\ntensor = torch.rand(3,4)\r\n\r\nprint(f\"Shape of tensor: {tensor.shape}\")\r\nprint(f\"Datatype of tensor: {tensor.dtype}\")\r\nprint(f\"Device tensor is stored on: {tensor.device}\")\r\n\r\nShape of tensor: torch.Size([3, 4])\r\nDatatype of tensor: torch.float32\r\nDevice tensor is stored on: cpu\r\n\r\nOperations on Tensors\u00b6\r\nOver 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing,\r\nindexing, slicing), sampling and more are\r\ncomprehensively described here.\r\nEach of these operations can be run on the GPU (at typically higher speeds than on a\r\nCPU). If you\u2019re using Colab, allocate a GPU by going to Runtime > Change runtime type > GPU.\r\nBy default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using\r\n.to method (after checking for GPU availability). Keep in mind that copying large tensors\r\nacross devices can be expensive in terms of time and memory!\r\n# We move our tensor to the GPU if available\r\nif torch.cuda.is_available():\r\n    tensor = tensor.to(\"cuda\")\r\n\r\nTry out some of the operations from the list.\r\nIf you\u2019re familiar with the NumPy API, you\u2019ll find the Tensor API a breeze to use.\r\nStandard numpy-like indexing and slicing:\r\ntensor = torch.ones(4, 4)\r\nprint(f\"First row: {tensor[0]}\")\r\nprint(f\"First column: {tensor[:, 0]}\")\r\nprint(f\"Last column: {tensor[..., -1]}\")\r\ntensor[:,1] = 0\r\nprint(tensor)\r\n\r\nFirst row: tensor([1., 1., 1., 1.])\r\nFirst column: tensor([1., 1., 1., 1.])\r\nLast column: tensor([1., 1., 1., 1.])\r\ntensor([[1., 0., 1., 1.],\r\n        [1., 0., 1., 1.],\r\n        [1., 0., 1., 1.],\r\n        [1., 0., 1., 1.]])\r\n\r\nJoining tensors You can use torch.cat to concatenate a sequence of tensors along a given dimension.\r\nSee also torch.stack,\r\nanother tensor joining operator that is subtly different from torch.cat.\r\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\r\nprint(t1)\r\n\r\ntensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\r\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\r\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\r\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\r\n\r\nArithmetic operations\r\n# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\r\n# ``tensor.T`` returns the transpose of a tensor\r\ny1 = tensor @ tensor.T\r\ny2 = tensor.matmul(tensor.T)\r\n\r\ny3 = torch.rand_like(y1)\r\ntorch.matmul(tensor, tensor.T, out=y3)\r\n\r\n\r\n# This computes the element-wise product. z1, z2, z3 will have the same value\r\nz1 = tensor * tensor\r\nz2 = tensor.mul(tensor)\r\n\r\nz3 = torch.rand_like(tensor)\r\ntorch.mul(tensor, tensor, out=z3)\r\n\r\ntensor([[1., 0., 1., 1.],\r\n        [1., 0., 1., 1.", "mimetype": "text/plain", "start_char_idx": 14066, "end_char_idx": 17380, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6baec8ba-aa4b-4b43-ac4f-4450fd9a152e": {"__data__": {"id_": "6baec8ba-aa4b-4b43-ac4f-4450fd9a152e", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "945787bc-444e-4e08-93fe-4a72f912d1c8", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "085ea0eb333fb754808b617d545ffa38522ed6b3fa06a862e8828b5194a42476", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f374d1ee-e8ac-4ac2-9779-0902435d1a1c", "node_type": "1", "metadata": {}, "hash": "749931b939a8767de585f87a573f6d56ab90e8ed499c867e36179f057ac29dcf", "class_name": "RelatedNodeInfo"}}, "text": "],\r\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\r\n\r\nArithmetic operations\r\n# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\r\n# ``tensor.T`` returns the transpose of a tensor\r\ny1 = tensor @ tensor.T\r\ny2 = tensor.matmul(tensor.T)\r\n\r\ny3 = torch.rand_like(y1)\r\ntorch.matmul(tensor, tensor.T, out=y3)\r\n\r\n\r\n# This computes the element-wise product. z1, z2, z3 will have the same value\r\nz1 = tensor * tensor\r\nz2 = tensor.mul(tensor)\r\n\r\nz3 = torch.rand_like(tensor)\r\ntorch.mul(tensor, tensor, out=z3)\r\n\r\ntensor([[1., 0., 1., 1.],\r\n        [1., 0., 1., 1.],\r\n        [1., 0., 1., 1.],\r\n        [1., 0., 1., 1.]])\r\n\r\nSingle-element tensors If you have a one-element tensor, for example by aggregating all\r\nvalues of a tensor into one value, you can convert it to a Python\r\nnumerical value using item():\r\nagg = tensor.sum()\r\nagg_item = agg.item()\r\nprint(agg_item, type(agg_item))\r\n\r\n12.0 <class 'float'>\r\n\r\nIn-place operations\r\nOperations that store the result into the operand are called in-place. They are denoted by a _ suffix.\r\nFor example: x.copy_(y), x.t_(), will change x.\r\nprint(f\"{tensor} \\n\")\r\ntensor.add_(5)\r\nprint(tensor)\r\n\r\ntensor([[1., 0., 1., 1.],\r\n        [1., 0., 1., 1.],\r\n        [1., 0., 1., 1.],\r\n        [1., 0., 1., 1.]])\r\n\r\ntensor([[6., 5., 6., 6.],\r\n        [6., 5., 6., 6.],\r\n        [6., 5., 6., 6.],\r\n        [6., 5., 6., 6.]])\r\n\r\nNote\r\nIn-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss\r\nof history. Hence, their use is discouraged.\r\nBridge with NumPy\u00b6\r\nTensors on the CPU and NumPy arrays can share their underlying memory\r\nlocations, and changing one will change the other.\r\nTensor to NumPy array\u00b6\r\nt = torch.ones(5)\r\nprint(f\"t: {t}\")\r\nn = t.numpy()\r\nprint(f\"n: {n}\")\r\n\r\nt: tensor([1., 1., 1., 1., 1.])\r\nn: [1. 1. 1. 1. 1.]\r\n\r\nA change in the tensor reflects in the NumPy array.\r\nt.add_(1)\r\nprint(f\"t: {t}\")\r\nprint(f\"n: {n}\")\r\n\r\nt: tensor([2., 2., 2., 2., 2.])\r\nn: [2. 2. 2. 2. 2.]\r\n\r\nNumPy array to Tensor\u00b6\r\nn = np.ones(5)\r\nt = torch.from_numpy(n)\r\n\r\nChanges in the NumPy array reflects in the tensor.\r\nnp.add(n, 1, out=n)\r\nprint(f\"t: {t}\")\r\nprint(f\"n: {n}\")\r\n\r\nt: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\r\nn: [2. 2. 2. 2. 2.]\r\n\r\nTotal running time of the script: ( 0 minutes  0.022 seconds)\r\nDownload Python source code: tensorqs_tutorial.py\r\nDownload Jupyter notebook: tensorqs_tutorial.ipynb\r\nGallery generated by Sphinx-Gallery \r\n\r\nsource: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html \r\ncontent: \r\n\r\nNote\r\nClick here\r\nto download the full example code\r\nLearn the Basics ||\r\nQuickstart ||\r\nTensors ||\r\nDatasets & DataLoaders ||\r\nTransforms ||\r\nBuild Model ||\r\nAutograd ||\r\nOptimization ||\r\nSave & Load Model\r\nDatasets & DataLoaders\u00b6\r\nCode for processing data samples can get messy and hard to maintain; we ideally want our dataset code\r\nto be decoupled from our model training code for better readability and modularity.\r\nPyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset\r\nthat allow you to use pre-loaded datasets as well as your own data.\r\nDataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around\r\nthe Dataset to enable easy access to the samples.", "mimetype": "text/plain", "start_char_idx": 16770, "end_char_idx": 20089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f374d1ee-e8ac-4ac2-9779-0902435d1a1c": {"__data__": {"id_": "f374d1ee-e8ac-4ac2-9779-0902435d1a1c", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6baec8ba-aa4b-4b43-ac4f-4450fd9a152e", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "c3b08dc719c4990a7ddf718b872b0097cc02f298d96b86df3bee77f8358af72b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c0bc95d-4ffd-4a1f-b4e1-c29774c185d8", "node_type": "1", "metadata": {}, "hash": "ae0f07294caca040c33c1adee346bdb270b186c165c12e8b2b3ad5387efcab18", "class_name": "RelatedNodeInfo"}}, "text": "PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset\r\nthat allow you to use pre-loaded datasets as well as your own data.\r\nDataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around\r\nthe Dataset to enable easy access to the samples.\r\nPyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that\r\nsubclass torch.utils.data.Dataset and implement functions specific to the particular data.\r\nThey can be used to prototype and benchmark your model. You can find them\r\nhere: Image Datasets,\r\nText Datasets, and\r\nAudio Datasets\r\nLoading a Dataset\u00b6\r\nHere is an example of how to load the Fashion-MNIST dataset from TorchVision.\r\nFashion-MNIST is a dataset of Zalando\u2019s article images consisting of 60,000 training examples and 10,000 test examples.\r\nEach example comprises a 28\u00d728 grayscale image and an associated label from one of 10 classes.\r\nWe load the FashionMNIST Dataset with the following parameters:\r\n\r\nroot is the path where the train/test data is stored,\r\ntrain specifies training or test dataset,\r\ndownload=True downloads the data from the internet if it\u2019s not available at root.\r\ntransform and target_transform specify the feature and label transformations\r\n\r\n\r\nroot is the path where the train/test data is stored,\r\ntrain specifies training or test dataset,\r\ndownload=True downloads the data from the internet if it\u2019s not available at root.\r\ntransform and target_transform specify the feature and label transformations\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\nfrom torchvision import datasets\r\nfrom torchvision.transforms import ToTensor\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\ntraining_data = datasets.FashionMNIST(\r\n    root=\"data\",\r\n    train=True,\r\n    download=True,\r\n    transform=ToTensor()\r\n)\r\n\r\ntest_data = datasets.FashionMNIST(\r\n    root=\"data\",\r\n    train=False,\r\n    download=True,\r\n    transform=ToTensor()\r\n)\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\r\n\r\n  0%|          | 0.00/26.4M [00:00<?, ?B/s]\r\n  0%|          | 65.5k/26.4M [00:00<01:13, 361kB/s]\r\n  1%|          | 229k/26.4M [00:00<00:38, 675kB/s]\r\n  2%|1         | 524k/26.4M [00:00<00:23, 1.10MB/s]\r\n  3%|3         | 852k/26.4M [00:00<00:18, 1.37MB/s]\r\n  4%|4         | 1.18M/26.4M [00:00<00:16, 1.52MB/s]\r\n  6%|5         | 1.57M/26.4M [00:01<00:14, 1.73MB/s]\r\n  8%|7         | 2.00M/26.4M [00:01<00:12, 1.92MB/s]\r\n  9%|9         | 2.46M/26.4M [00:01<00:11, 2.11MB/s]\r\n 11%|#1        | 2.98M/26.4M [00:01<00:10, 2.34MB/s]\r\n 14%|#3        | 3.57M/26.4M [00:01<00:08, 2.61MB/s]\r\n 16%|#5        | 4.19M/26.4M [00:02<00:07, 2.85MB/s]\r\n 18%|#8        | 4.88M/26.4M [00:02<00:06, 3.12MB/s]\r\n 21%|##1       | 5.64M/26.4M [00:02<00:05, 3.72MB/s]\r\n 23%|##2       | 6.06M/26.4M [00:02<00:05, 3.54MB/s]\r\n 26%|##6       | 6.95M/26.4M [00:02<00:04, 3.96MB/s]\r\n 30%|##9       | 7.", "mimetype": "text/plain", "start_char_idx": 19775, "end_char_idx": 22854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c0bc95d-4ffd-4a1f-b4e1-c29774c185d8": {"__data__": {"id_": "1c0bc95d-4ffd-4a1f-b4e1-c29774c185d8", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f374d1ee-e8ac-4ac2-9779-0902435d1a1c", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "9bf2b92c01a5eb549873fd7422b09fd698030006aa656f28634b7411be4ffc53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca4af97d-30a8-451c-a357-30b35e650e35", "node_type": "1", "metadata": {}, "hash": "8c2c2fa434a9e5b8b7fa73bf729fb450106a663d2c2d47fa2dc884feb862bbe2", "class_name": "RelatedNodeInfo"}}, "text": "4M [00:01<00:08, 2.61MB/s]\r\n 16%|#5        | 4.19M/26.4M [00:02<00:07, 2.85MB/s]\r\n 18%|#8        | 4.88M/26.4M [00:02<00:06, 3.12MB/s]\r\n 21%|##1       | 5.64M/26.4M [00:02<00:05, 3.72MB/s]\r\n 23%|##2       | 6.06M/26.4M [00:02<00:05, 3.54MB/s]\r\n 26%|##6       | 6.95M/26.4M [00:02<00:04, 3.96MB/s]\r\n 30%|##9       | 7.90M/26.4M [00:02<00:03, 4.75MB/s]\r\n 32%|###1      | 8.45M/26.4M [00:02<00:03, 4.54MB/s]\r\n 36%|###6      | 9.54M/26.4M [00:03<00:03, 5.49MB/s]\r\n 39%|###8      | 10.2M/26.4M [00:03<00:03, 5.26MB/s]\r\n 43%|####3     | 11.5M/26.4M [00:03<00:02, 5.91MB/s]\r\n 49%|####8     | 12.9M/26.4M [00:03<00:01, 7.07MB/s]\r\n 52%|#####1    | 13.7M/26.4M [00:03<00:01, 6.74MB/s]\r\n 58%|#####7    | 15.3M/26.4M [00:03<00:01, 8.17MB/s]\r\n 61%|######1   | 16.2M/26.4M [00:03<00:01, 7.77MB/s]\r\n 68%|######8   | 18.0M/26.4M [00:04<00:00, 10.1MB/s]\r\n 72%|#######2  | 19.1M/26.4M [00:04<00:00, 10.0MB/s]\r\n 77%|#######6  | 20.3M/26.4M [00:04<00:00, 10.1MB/s]\r\n 81%|########  | 21.4M/26.4M [00:04<00:00, 9.59MB/s]\r\n 89%|########8 | 23.5M/26.4M [00:04<00:00, 12.5MB/s]\r\n 95%|#########5| 25.1M/26.4M [00:04<00:00, 12.5MB/s]\r\n100%|##########| 26.4M/26.4M [00:04<00:00, 5.64MB/s]\r\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\r\n\r\n  0%|          | 0.00/29.5k [00:00<?, ?B/s]\r\n100%|##########| 29.5k/29.5k [00:00<00:00, 327kB/s]\r\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\r\n\r\n  0%|          | 0.", "mimetype": "text/plain", "start_char_idx": 22537, "end_char_idx": 24493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca4af97d-30a8-451c-a357-30b35e650e35": {"__data__": {"id_": "ca4af97d-30a8-451c-a357-30b35e650e35", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c0bc95d-4ffd-4a1f-b4e1-c29774c185d8", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "e7a8d2aee67771d07d4735d6caeee379d2f3d8845137bc8f04fd074e59090c3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b4fd769-5690-43a0-96ed-7a17d7ea0d4e", "node_type": "1", "metadata": {}, "hash": "ba7d1509896a8958719375400e42ead738f1fb206e50f1b850a868bd68955da0", "class_name": "RelatedNodeInfo"}}, "text": "gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\r\n\r\n  0%|          | 0.00/29.5k [00:00<?, ?B/s]\r\n100%|##########| 29.5k/29.5k [00:00<00:00, 327kB/s]\r\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\r\n\r\n  0%|          | 0.00/4.42M [00:00<?, ?B/s]\r\n  1%|1         | 65.5k/4.42M [00:00<00:12, 360kB/s]\r\n  5%|5         | 229k/4.42M [00:00<00:06, 678kB/s]\r\n 21%|##1       | 950k/4.42M [00:00<00:01, 2.17MB/s]\r\n 87%|########6 | 3.83M/4.42M [00:00<00:00, 7.55MB/s]\r\n100%|##########| 4.42M/4.42M [00:00<00:00, 6.05MB/s]\r\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\r\n\r\n  0%|          | 0.00/5.15k [00:00<?, ?B/s]\r\n100%|##########| 5.15k/5.15k [00:00<00:00, 32.5MB/s]\r\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\r\n\r\nIterating and Visualizing the Dataset\u00b6\r\nWe can index Datasets manually like a list: training_data[index].\r\nWe use matplotlib to visualize some samples in our training data.\r\nlabels_map = {\r\n    0: \"T-Shirt\",\r\n    1: \"Trouser\",\r\n    2: \"Pullover\",\r\n    3: \"Dress\",\r\n    4: \"Coat\",\r\n    5: \"Sandal\",\r\n    6: \"Shirt\",\r\n    7: \"Sneaker\",\r\n    8: \"Bag\",\r\n    9: \"Ankle Boot\",\r\n}\r\nfigure = plt.figure(figsize=(8, 8))\r\ncols, rows = 3, 3\r\nfor i in range(1, cols * rows + 1):\r\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\r\n    img, label = training_data[sample_idx]\r\n    figure.add_subplot(rows, cols, i)\r\n    plt.title(labels_map[label])\r\n    plt.axis(\"off\")\r\n    plt.imshow(img.squeeze(), cmap=\"gray\")\r\nplt.show()\r\n\r\nCreating a Custom Dataset for your files\u00b6\r\nA custom Dataset class must implement three functions: __init__, __len__, and __getitem__.\r\nTake a look at this implementation; the FashionMNIST images are stored\r\nin a directory img_dir, and their labels are stored separately in a CSV file annotations_file.\r\nIn the next sections, we\u2019ll break down what\u2019s happening in each of these functions.", "mimetype": "text/plain", "start_char_idx": 23981, "end_char_idx": 26427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b4fd769-5690-43a0-96ed-7a17d7ea0d4e": {"__data__": {"id_": "8b4fd769-5690-43a0-96ed-7a17d7ea0d4e", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca4af97d-30a8-451c-a357-30b35e650e35", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "49bf46fd8ff69e31a9c4ce72bcee20347cc3fd23b8d28a154e031a1556a0224d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9984ef6f-7c69-4fa8-acaf-f5904b603339", "node_type": "1", "metadata": {}, "hash": "7ec62e668289b38f3ab739c8049ff899a35bb0e0a730fbd66e5e4fe29d3509be", "class_name": "RelatedNodeInfo"}}, "text": "Take a look at this implementation; the FashionMNIST images are stored\r\nin a directory img_dir, and their labels are stored separately in a CSV file annotations_file.\r\nIn the next sections, we\u2019ll break down what\u2019s happening in each of these functions.\r\nimport os\r\nimport pandas as pd\r\nfrom torchvision.io import read_image\r\n\r\nclass CustomImageDataset(Dataset):\r\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\r\n        self.img_labels = pd.read_csv(annotations_file)\r\n        self.img_dir = img_dir\r\n        self.transform = transform\r\n        self.target_transform = target_transform\r\n\r\n    def __len__(self):\r\n        return len(self.img_labels)\r\n\r\n    def __getitem__(self, idx):\r\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\r\n        image = read_image(img_path)\r\n        label = self.img_labels.iloc[idx, 1]\r\n        if self.transform:\r\n            image = self.transform(image)\r\n        if self.target_transform:\r\n            label = self.target_transform(label)\r\n        return image, label\r\n\r\n__init__\u00b6\r\nThe __init__ function is run once when instantiating the Dataset object. We initialize\r\nthe directory containing the images, the annotations file, and both transforms (covered\r\nin more detail in the next section).\r\nThe labels.csv file looks like:\r\ntshirt1.jpg, 0\r\ntshirt2.jpg, 0\r\n......\r\nankleboot999.jpg, 9\r\n\r\ndef __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\r\n    self.img_labels = pd.read_csv(annotations_file)\r\n    self.img_dir = img_dir\r\n    self.transform = transform\r\n    self.target_transform = target_transform\r\n\r\n__len__\u00b6\r\nThe __len__ function returns the number of samples in our dataset.\r\nExample:\r\ndef __len__(self):\r\n    return len(self.img_labels)\r\n\r\n__getitem__\u00b6\r\nThe __getitem__ function loads and returns a sample from the dataset at the given index idx.\r\nBased on the index, it identifies the image\u2019s location on disk, converts that to a tensor using read_image, retrieves the\r\ncorresponding label from the csv data in self.img_labels, calls the transform functions on them (if applicable), and returns the\r\ntensor image and corresponding label in a tuple.\r\ndef __getitem__(self, idx):\r\n    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\r\n    image = read_image(img_path)\r\n    label = self.img_labels.iloc[idx, 1]\r\n    if self.transform:\r\n        image = self.transform(image)\r\n    if self.target_transform:\r\n        label = self.target_transform(label)\r\n    return image, label\r\n\r\nPreparing your data for training with DataLoaders\u00b6\r\nThe Dataset retrieves our dataset\u2019s features and labels one sample at a time. While training a model, we typically want to\r\npass samples in \u201cminibatches\u201d, reshuffle the data at every epoch to reduce model overfitting, and use Python\u2019s multiprocessing to\r\nspeed up data retrieval.\r\nDataLoader is an iterable that abstracts this complexity for us in an easy API.\r\nfrom torch.utils.data import DataLoader\r\n\r\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\r\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\r\n\r\nIterate through the DataLoader\u00b6\r\nWe have loaded that dataset into the DataLoader and can iterate through the dataset as needed.\r\nEach iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively).\r\nBecause we specified shuffle=True, after we iterate over all batches the data is shuffled (for finer-grained control over\r\nthe data loading order, take a look at Samplers).\r\n# Display image and label.", "mimetype": "text/plain", "start_char_idx": 26176, "end_char_idx": 29786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9984ef6f-7c69-4fa8-acaf-f5904b603339": {"__data__": {"id_": "9984ef6f-7c69-4fa8-acaf-f5904b603339", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b4fd769-5690-43a0-96ed-7a17d7ea0d4e", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "ea6840dfbe2b7c591b27eb933d563ab29edfe32731f865a260ea04d01d128f7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72ff2f24-dde8-4ba1-9c66-7cd48cd74ad3", "node_type": "1", "metadata": {}, "hash": "05b678c273fecc0c4673e1ecc5ef3e6e8b837079ea7a4700c98a3d7afcff8da8", "class_name": "RelatedNodeInfo"}}, "text": "While training a model, we typically want to\r\npass samples in \u201cminibatches\u201d, reshuffle the data at every epoch to reduce model overfitting, and use Python\u2019s multiprocessing to\r\nspeed up data retrieval.\r\nDataLoader is an iterable that abstracts this complexity for us in an easy API.\r\nfrom torch.utils.data import DataLoader\r\n\r\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\r\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\r\n\r\nIterate through the DataLoader\u00b6\r\nWe have loaded that dataset into the DataLoader and can iterate through the dataset as needed.\r\nEach iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively).\r\nBecause we specified shuffle=True, after we iterate over all batches the data is shuffled (for finer-grained control over\r\nthe data loading order, take a look at Samplers).\r\n# Display image and label.\r\ntrain_features, train_labels = next(iter(train_dataloader))\r\nprint(f\"Feature batch shape: {train_features.size()}\")\r\nprint(f\"Labels batch shape: {train_labels.size()}\")\r\nimg = train_features[0].squeeze()\r\nlabel = train_labels[0]\r\nplt.imshow(img, cmap=\"gray\")\r\nplt.show()\r\nprint(f\"Label: {label}\")\r\n\r\nFeature batch shape: torch.Size([64, 1, 28, 28])\r\nLabels batch shape: torch.Size([64])\r\nLabel: 5\r\n\r\nFurther Reading\u00b6\r\ntorch.utils.data API\r\nTotal running time of the script: ( 0 minutes  8.438 seconds)\r\nDownload Python source code: data_tutorial.py\r\nDownload Jupyter notebook: data_tutorial.ipynb\r\nGallery generated by Sphinx-Gallery \r\n\r\nsource: https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html \r\ncontent: \r\n\r\nNote\r\nClick here\r\nto download the full example code\r\nLearn the Basics ||\r\nQuickstart ||\r\nTensors ||\r\nDatasets & DataLoaders ||\r\nTransforms ||\r\nBuild Model ||\r\nAutograd ||\r\nOptimization ||\r\nSave & Load Model\r\nTransforms\u00b6\r\nData does not always come in its final processed form that is required for\r\ntraining machine learning algorithms. We use transforms to perform some\r\nmanipulation of the data and make it suitable for training.\r\nAll TorchVision datasets have two parameters -transform to modify the features and\r\ntarget_transform to modify the labels - that accept callables containing the transformation logic.\r\nThe torchvision.transforms module offers\r\nseveral commonly-used transforms out of the box.\r\nThe FashionMNIST features are in PIL Image format, and the labels are integers.\r\nFor training, we need the features as normalized tensors, and the labels as one-hot encoded tensors.\r\nTo make these transformations, we use ToTensor and Lambda.\r\nimport torch\r\nfrom torchvision import datasets\r\nfrom torchvision.transforms import ToTensor, Lambda\r\n\r\nds = datasets.FashionMNIST(\r\n    root=\"data\",\r\n    train=True,\r\n    download=True,\r\n    transform=ToTensor(),\r\n    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\r\n)\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\r\n\r\n  0%|          | 0.00/26.4M [00:00<?, ?B/s]\r\n  0%|          | 65.5k/26.4M [00:00<01:12, 363kB/s]\r\n  1%|          | 197k/26.4M [00:00<00:45, 576kB/s]\r\n  3%|3         | 819k/26.4M [00:00<00:13, 1.89MB/s]\r\n 13%|#2        | 3.31M/26.4M [00:00<00:03, 6.59MB/s]\r\n 35%|###5      | 9.37M/26.4M [00:00<00:01, 16.3MB/s]\r\n 58%|#####8    | 15.4M/26.4M [00:01<00:00, 22.1MB/s]\r\n 81%|########  | 21.3M/26.4M [00:01<00:00, 25.4MB/s]\r\n100%|##########| 26.4M/26.4M [00:01<00:00, 19.", "mimetype": "text/plain", "start_char_idx": 28849, "end_char_idx": 32519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72ff2f24-dde8-4ba1-9c66-7cd48cd74ad3": {"__data__": {"id_": "72ff2f24-dde8-4ba1-9c66-7cd48cd74ad3", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9984ef6f-7c69-4fa8-acaf-f5904b603339", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "339e6fce224e57b0cf011516b5c58232e37f45edc6d6a231d994c9cac37f8391", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82a64506-04b9-4653-93fd-59a6f6c69eb7", "node_type": "1", "metadata": {}, "hash": "80890e6311b85ea42192f805eddd96745d36e3ed6f7ce851bc693ed376de4874", "class_name": "RelatedNodeInfo"}}, "text": "576kB/s]\r\n  3%|3         | 819k/26.4M [00:00<00:13, 1.89MB/s]\r\n 13%|#2        | 3.31M/26.4M [00:00<00:03, 6.59MB/s]\r\n 35%|###5      | 9.37M/26.4M [00:00<00:01, 16.3MB/s]\r\n 58%|#####8    | 15.4M/26.4M [00:01<00:00, 22.1MB/s]\r\n 81%|########  | 21.3M/26.4M [00:01<00:00, 25.4MB/s]\r\n100%|##########| 26.4M/26.4M [00:01<00:00, 19.4MB/s]\r\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\r\n\r\n  0%|          | 0.00/29.5k [00:00<?, ?B/s]\r\n100%|##########| 29.5k/29.5k [00:00<00:00, 325kB/s]\r\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\r\n\r\n  0%|          | 0.00/4.42M [00:00<?, ?B/s]\r\n  1%|1         | 65.5k/4.42M [00:00<00:12, 362kB/s]\r\n  4%|4         | 197k/4.42M [00:00<00:07, 574kB/s]\r\n 11%|#1        | 492k/4.42M [00:00<00:03, 1.05MB/s]\r\n 18%|#7        | 786k/4.42M [00:00<00:02, 1.28MB/s]\r\n 26%|##5       | 1.15M/4.42M [00:00<00:02, 1.53MB/s]\r\n 35%|###4      | 1.54M/4.42M [00:01<00:01, 1.74MB/s]\r\n 44%|####3     | 1.93M/4.42M [00:01<00:01, 1.88MB/s]\r\n 54%|#####4    | 2.39M/4.42M [00:01<00:00, 2.27MB/s]\r\n 60%|######    | 2.65M/4.42M [00:01<00:00, 2.17MB/s]\r\n 72%|#######1  | 3.18M/4.42M [00:01<00:00, 2.42MB/s]\r\n 85%|########5 | 3.77M/4.42M [00:01<00:00, 2.69MB/s]\r\n100%|##########| 4.42M/4.42M [00:02<00:00, 2.98MB/s]\r\n100%|##########| 4.42M/4.42M [00:02<00:00, 2.12MB/s]\r\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.", "mimetype": "text/plain", "start_char_idx": 32194, "end_char_idx": 34296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82a64506-04b9-4653-93fd-59a6f6c69eb7": {"__data__": {"id_": "82a64506-04b9-4653-93fd-59a6f6c69eb7", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72ff2f24-dde8-4ba1-9c66-7cd48cd74ad3", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "79d9beebb520f6efe8ee3e401e509a9425765a217f7d27f84729bd15f069efa0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "314d64ef-e47a-4f6c-b3b9-04656a1cc246", "node_type": "1", "metadata": {}, "hash": "dac9eafa5d6fb5366a3072d9de66b947e22d15d902d71a95220497c216b67c93", "class_name": "RelatedNodeInfo"}}, "text": "42M [00:01<00:00, 2.42MB/s]\r\n 85%|########5 | 3.77M/4.42M [00:01<00:00, 2.69MB/s]\r\n100%|##########| 4.42M/4.42M [00:02<00:00, 2.98MB/s]\r\n100%|##########| 4.42M/4.42M [00:02<00:00, 2.12MB/s]\r\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\r\n\r\n  0%|          | 0.00/5.15k [00:00<?, ?B/s]\r\n100%|##########| 5.15k/5.15k [00:00<00:00, 35.8MB/s]\r\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\r\n\r\nToTensor()\u00b6\r\nToTensor\r\nconverts a PIL image or NumPy ndarray into a FloatTensor. and scales\r\nthe image\u2019s pixel intensity values in the range [0., 1.]\r\nLambda Transforms\u00b6\r\nLambda transforms apply any user-defined lambda function. Here, we define a function\r\nto turn the integer into a one-hot encoded tensor.\r\nIt first creates a zero tensor of size 10 (the number of labels in our dataset) and calls\r\nscatter_ which assigns a\r\nvalue=1 on the index as given by the label y.\r\ntarget_transform = Lambda(lambda y: torch.zeros(\r\n    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\r\n\r\nFurther Reading\u00b6\r\ntorchvision.transforms API\r\nTotal running time of the script: ( 0 minutes  5.750 seconds)\r\nDownload Python source code: transforms_tutorial.py\r\nDownload Jupyter notebook: transforms_tutorial.ipynb\r\nGallery generated by Sphinx-Gallery \r\n\r\nsource: https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html \r\ncontent: \r\n\r\nNote\r\nClick here\r\nto download the full example code\r\nLearn the Basics ||\r\nQuickstart ||\r\nTensors ||\r\nDatasets & DataLoaders ||\r\nTransforms ||\r\nBuild Model ||\r\nAutograd ||\r\nOptimization ||\r\nSave & Load Model\r\nBuild the Neural Network\u00b6\r\nNeural networks comprise of layers/modules that perform operations on data.\r\nThe torch.nn namespace provides all the building blocks you need to\r\nbuild your own neural network. Every module in PyTorch subclasses the nn.Module.\r\nA neural network is a module itself that consists of other modules (layers). This nested structure allows for\r\nbuilding and managing complex architectures easily.\r\nIn the following sections, we\u2019ll build a neural network to classify images in the FashionMNIST dataset.\r\nimport os\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import datasets, transforms\r\n\r\nGet Device for Training\u00b6\r\nWe want to be able to train our model on a hardware accelerator like the GPU or MPS,\r\nif available. Let\u2019s check to see if torch.cuda\r\nor torch.backends.mps are available, otherwise we use the CPU.\r\ndevice = (\r\n    \"cuda\"\r\n    if torch.cuda.is_available()\r\n    else \"mps\"\r\n    if torch.backends.mps.is_available()\r\n    else \"cpu\"\r\n)\r\nprint(f\"Using {device} device\")\r\n\r\nUsing cuda device\r\n\r\nDefine the Class\u00b6\r\nWe define our neural network by subclassing nn.Module, and\r\ninitialize the neural network layers in __init__. Every nn.Module subclass implements\r\nthe operations on input data in the forward method.\r\nclass NeuralNetwork(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.flatten = nn.Flatten()\r\n        self.linear_relu_stack = nn.Sequential(\r\n            nn.Linear(28*28, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 10),\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.flatten(x)\r\n        logits = self.linear_relu_stack(x)\r\n        return logits\r\n\r\nWe create an instance of NeuralNetwork, and move it to the device, and print\r\nits structure.", "mimetype": "text/plain", "start_char_idx": 33853, "end_char_idx": 37573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "314d64ef-e47a-4f6c-b3b9-04656a1cc246": {"__data__": {"id_": "314d64ef-e47a-4f6c-b3b9-04656a1cc246", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82a64506-04b9-4653-93fd-59a6f6c69eb7", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "de7ecaee4414a425231b77138e34abe577af8cbac9427704dcb7b63be1703c01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04d552af-4b8f-46fe-bba4-976d1ea93144", "node_type": "1", "metadata": {}, "hash": "ad10dc3c90788df0a5b87153c5fc3609f1ec04a97cddcebcfe56dc8ab9490ef5", "class_name": "RelatedNodeInfo"}}, "text": "Every nn.Module subclass implements\r\nthe operations on input data in the forward method.\r\nclass NeuralNetwork(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.flatten = nn.Flatten()\r\n        self.linear_relu_stack = nn.Sequential(\r\n            nn.Linear(28*28, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 10),\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.flatten(x)\r\n        logits = self.linear_relu_stack(x)\r\n        return logits\r\n\r\nWe create an instance of NeuralNetwork, and move it to the device, and print\r\nits structure.\r\nmodel = NeuralNetwork().to(device)\r\nprint(model)\r\n\r\nNeuralNetwork(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_relu_stack): Sequential(\r\n    (0): Linear(in_features=784, out_features=512, bias=True)\r\n    (1): ReLU()\r\n    (2): Linear(in_features=512, out_features=512, bias=True)\r\n    (3): ReLU()\r\n    (4): Linear(in_features=512, out_features=10, bias=True)\r\n  )\r\n)\r\n\r\nTo use the model, we pass it the input data. This executes the model\u2019s forward,\r\nalong with some background operations.\r\nDo not call model.forward() directly!\r\nCalling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output.\r\nWe get the prediction probabilities by passing it through an instance of the nn.Softmax module.\r\nX = torch.rand(1, 28, 28, device=device)\r\nlogits = model(X)\r\npred_probab = nn.Softmax(dim=1)(logits)\r\ny_pred = pred_probab.argmax(1)\r\nprint(f\"Predicted class: {y_pred}\")\r\n\r\nPredicted class: tensor([7], device='cuda:0')\r\n\r\nModel Layers\u00b6\r\nLet\u2019s break down the layers in the FashionMNIST model. To illustrate it, we\r\nwill take a sample minibatch of 3 images of size 28x28 and see what happens to it as\r\nwe pass it through the network.\r\ninput_image = torch.rand(3,28,28)\r\nprint(input_image.size())\r\n\r\ntorch.Size([3, 28, 28])\r\n\r\nnn.Flatten\u00b6\r\nWe initialize the nn.Flatten\r\nlayer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (\r\nthe minibatch dimension (at dim=0) is maintained).\r\nflatten = nn.Flatten()\r\nflat_image = flatten(input_image)\r\nprint(flat_image.size())\r\n\r\ntorch.Size([3, 784])\r\n\r\nnn.Linear\u00b6\r\nThe linear layer\r\nis a module that applies a linear transformation on the input using its stored weights and biases.\r\nlayer1 = nn.Linear(in_features=28*28, out_features=20)\r\nhidden1 = layer1(flat_image)\r\nprint(hidden1.size())\r\n\r\ntorch.Size([3, 20])\r\n\r\nnn.ReLU\u00b6\r\nNon-linear activations are what create the complex mappings between the model\u2019s inputs and outputs.\r\nThey are applied after linear transformations to introduce nonlinearity, helping neural networks\r\nlearn a wide variety of phenomena.\r\nIn this model, we use nn.ReLU between our\r\nlinear layers, but there\u2019s other activations to introduce non-linearity in your model.", "mimetype": "text/plain", "start_char_idx": 36930, "end_char_idx": 39878, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04d552af-4b8f-46fe-bba4-976d1ea93144": {"__data__": {"id_": "04d552af-4b8f-46fe-bba4-976d1ea93144", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "314d64ef-e47a-4f6c-b3b9-04656a1cc246", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "6bf20b0ebfbf81f69253c51b9498bb6b79ffb405405304e83e991ec9db863971", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7a3893f-6c5e-40e4-9814-1e6bf88fbd8a", "node_type": "1", "metadata": {}, "hash": "24144741fd88463fa46912377a37e60fefce93f6a3604ac92815824d070e8e12", "class_name": "RelatedNodeInfo"}}, "text": "flatten = nn.Flatten()\r\nflat_image = flatten(input_image)\r\nprint(flat_image.size())\r\n\r\ntorch.Size([3, 784])\r\n\r\nnn.Linear\u00b6\r\nThe linear layer\r\nis a module that applies a linear transformation on the input using its stored weights and biases.\r\nlayer1 = nn.Linear(in_features=28*28, out_features=20)\r\nhidden1 = layer1(flat_image)\r\nprint(hidden1.size())\r\n\r\ntorch.Size([3, 20])\r\n\r\nnn.ReLU\u00b6\r\nNon-linear activations are what create the complex mappings between the model\u2019s inputs and outputs.\r\nThey are applied after linear transformations to introduce nonlinearity, helping neural networks\r\nlearn a wide variety of phenomena.\r\nIn this model, we use nn.ReLU between our\r\nlinear layers, but there\u2019s other activations to introduce non-linearity in your model.\r\nprint(f\"Before ReLU: {hidden1}\\n\\n\")\r\nhidden1 = nn.ReLU()(hidden1)\r\nprint(f\"After ReLU: {hidden1}\")\r\n\r\nBefore ReLU: tensor([[ 0.4158, -0.0130, -0.1144,  0.3960,  0.1476, -0.0690, -0.0269,  0.2690,\r\n          0.1353,  0.1975,  0.4484,  0.0753,  0.4455,  0.5321, -0.1692,  0.4504,\r\n          0.2476, -0.1787, -0.2754,  0.2462],\r\n        [ 0.2326,  0.0623, -0.2984,  0.2878,  0.2767, -0.5434, -0.5051,  0.4339,\r\n          0.0302,  0.1634,  0.5649, -0.0055,  0.2025,  0.4473, -0.2333,  0.6611,\r\n          0.1883, -0.1250,  0.0820,  0.2778],\r\n        [ 0.3325,  0.2654,  0.1091,  0.0651,  0.3425, -0.3880, -0.0152,  0.2298,\r\n          0.3872,  0.0342,  0.8503,  0.0937,  0.1796,  0.5007, -0.1897,  0.4030,\r\n          0.1189, -0.3237,  0.2048,  0.4343]], grad_fn=<AddmmBackward0>)\r\n\r\n\r\nAfter ReLU: tensor([[0.4158, 0.0000, 0.0000, 0.3960, 0.1476, 0.0000, 0.0000, 0.2690, 0.1353,\r\n         0.1975, 0.4484, 0.0753, 0.4455, 0.5321, 0.0000, 0.4504, 0.2476, 0.0000,\r\n         0.0000, 0.2462],\r\n        [0.2326, 0.0623, 0.0000, 0.2878, 0.2767, 0.0000, 0.0000, 0.4339, 0.0302,\r\n         0.1634, 0.5649, 0.0000, 0.2025, 0.4473, 0.0000, 0.6611, 0.1883, 0.0000,\r\n         0.0820, 0.2778],\r\n        [0.3325, 0.2654, 0.1091, 0.0651, 0.3425, 0.0000, 0.0000, 0.2298, 0.3872,\r\n         0.0342, 0.8503, 0.0937, 0.1796, 0.5007, 0.0000, 0.4030, 0.1189, 0.0000,\r\n         0.2048, 0.4343]], grad_fn=<ReluBackward0>)\r\n\r\nnn.Sequential\u00b6\r\nnn.Sequential is an ordered\r\ncontainer of modules.", "mimetype": "text/plain", "start_char_idx": 39129, "end_char_idx": 41339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7a3893f-6c5e-40e4-9814-1e6bf88fbd8a": {"__data__": {"id_": "b7a3893f-6c5e-40e4-9814-1e6bf88fbd8a", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04d552af-4b8f-46fe-bba4-976d1ea93144", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "4ed054920e7c3ca2623fcf0984c9f6bf42b6e9a24d0754e7b68a433ee45fbab0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f758777-5371-4f9d-b2c5-2547a58f897e", "node_type": "1", "metadata": {}, "hash": "7198f32b71a8a3451b48a9cfef3473494df3fdf42a8185e3e3871da9be6597bf", "class_name": "RelatedNodeInfo"}}, "text": "The data is passed through all the modules in the same order as defined. You can use\r\nsequential containers to put together a quick network like seq_modules.\r\nseq_modules = nn.Sequential(\r\n    flatten,\r\n    layer1,\r\n    nn.ReLU(),\r\n    nn.Linear(20, 10)\r\n)\r\ninput_image = torch.rand(3,28,28)\r\nlogits = seq_modules(input_image)\r\n\r\nnn.Softmax\u00b6\r\nThe last linear layer of the neural network returns logits - raw values in [-infty, infty] - which are passed to the\r\nnn.Softmax module. The logits are scaled to values\r\n[0, 1] representing the model\u2019s predicted probabilities for each class. dim parameter indicates the dimension along\r\nwhich the values must sum to 1.\r\nsoftmax = nn.Softmax(dim=1)\r\npred_probab = softmax(logits)\r\n\r\nModel Parameters\u00b6\r\nMany layers inside a neural network are parameterized, i.e. have associated weights\r\nand biases that are optimized during training. Subclassing nn.Module automatically\r\ntracks all fields defined inside your model object, and makes all parameters\r\naccessible using your model\u2019s parameters() or named_parameters() methods.\r\nIn this example, we iterate over each parameter, and print its size and a preview of its values.", "mimetype": "text/plain", "start_char_idx": 41340, "end_char_idx": 42502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f758777-5371-4f9d-b2c5-2547a58f897e": {"__data__": {"id_": "1f758777-5371-4f9d-b2c5-2547a58f897e", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7a3893f-6c5e-40e4-9814-1e6bf88fbd8a", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "219904f1ce5f8dcf637dd8af409b347ae98312998d80c7586e4475d065812769", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f12ebdb1-c3b3-42ba-871f-a5f0cfb9c0b1", "node_type": "1", "metadata": {}, "hash": "8a51feaa37e041a70f60de59b669c5b3ecbbaa591cac512c2ae7b2d28a85d907", "class_name": "RelatedNodeInfo"}}, "text": "The logits are scaled to values\r\n[0, 1] representing the model\u2019s predicted probabilities for each class. dim parameter indicates the dimension along\r\nwhich the values must sum to 1.\r\nsoftmax = nn.Softmax(dim=1)\r\npred_probab = softmax(logits)\r\n\r\nModel Parameters\u00b6\r\nMany layers inside a neural network are parameterized, i.e. have associated weights\r\nand biases that are optimized during training. Subclassing nn.Module automatically\r\ntracks all fields defined inside your model object, and makes all parameters\r\naccessible using your model\u2019s parameters() or named_parameters() methods.\r\nIn this example, we iterate over each parameter, and print its size and a preview of its values.\r\nprint(f\"Model structure: {model}\\n\\n\")\r\n\r\nfor name, param in model.named_parameters():\r\n    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\r\n\r\nModel structure: NeuralNetwork(\r\n  (flatten): Flatten(start_dim=1, end_dim=-1)\r\n  (linear_relu_stack): Sequential(\r\n    (0): Linear(in_features=784, out_features=512, bias=True)\r\n    (1): ReLU()\r\n    (2): Linear(in_features=512, out_features=512, bias=True)\r\n    (3): ReLU()\r\n    (4): Linear(in_features=512, out_features=10, bias=True)\r\n  )\r\n)\r\n\r\n\r\nLayer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],\r\n        [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115]],\r\n       device='cuda:0', grad_fn=<SliceBackward0>)\r\n\r\nLayer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0155, -0.0327], device='cuda:0', grad_fn=<SliceBackward0>)\r\n\r\nLayer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0116,  0.0293, -0.0280,  ...,  0.0334, -0.0078,  0.0298],\r\n        [ 0.0095,  0.0038,  0.0009,  ..., -0.0365, -0.0011, -0.0221]],\r\n       device='cuda:0', grad_fn=<SliceBackward0>)\r\n\r\nLayer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0148, -0.0256], device='cuda:0', grad_fn=<SliceBackward0>)\r\n\r\nLayer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0147, -0.0229,  0.0180,  ..., -0.0013,  0.0177,  0.0070],\r\n        [-0.0202, -0.0417, -0.0279,  ..., -0.0441,  0.0185, -0.0268]],\r\n       device='cuda:0', grad_fn=<SliceBackward0>)\r\n\r\nLayer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0070, -0.0411], device='cuda:0', grad_fn=<SliceBackward0>)\r\n\r\nFurther Reading\u00b6\r\ntorch.nn API\r\nTotal running time of the script: ( 0 minutes  0.369 seconds)\r\nDownload Python source code: buildmodel_tutorial.py\r\nDownload Jupyter notebook: buildmodel_tutorial.ipynb\r\nGallery generated by Sphinx-Gallery \r\n\r\nsource: https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html \r\ncontent: \r\n\r\nNote\r\nClick here\r\nto download the full example code\r\nLearn the Basics ||\r\nQuickstart ||\r\nTensors ||\r\nDatasets & DataLoaders ||\r\nTransforms ||\r\nBuild Model ||\r\nAutograd ||\r\nOptimization ||\r\nSave & Load Model\r\nAutomatic Differentiation with torch.autograd\u00b6\r\nWhen training neural networks, the most frequently used algorithm is\r\nback propagation. In this algorithm, parameters (model weights) are\r\nadjusted according to the gradient of the loss function with respect\r\nto the given parameter.\r\nTo compute those gradients, PyTorch has a built-in differentiation engine\r\ncalled torch.autograd.", "mimetype": "text/plain", "start_char_idx": 41820, "end_char_idx": 45185, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f12ebdb1-c3b3-42ba-871f-a5f0cfb9c0b1": {"__data__": {"id_": "f12ebdb1-c3b3-42ba-871f-a5f0cfb9c0b1", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f758777-5371-4f9d-b2c5-2547a58f897e", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "3ba80e92d074bf629fd604d221b43791e16646378f16f506e8263d303176e729", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9260e27-55a2-4846-9b09-8e26ccdc3fb4", "node_type": "1", "metadata": {}, "hash": "e257969dbd198b8bbfea36287f27a207d6f2852561d780c45b120c26a48664f8", "class_name": "RelatedNodeInfo"}}, "text": "In this algorithm, parameters (model weights) are\r\nadjusted according to the gradient of the loss function with respect\r\nto the given parameter.\r\nTo compute those gradients, PyTorch has a built-in differentiation engine\r\ncalled torch.autograd. It supports automatic computation of gradient for any\r\ncomputational graph.\r\nConsider the simplest one-layer neural network, with input x,\r\nparameters w and b, and some loss function. It can be defined in\r\nPyTorch in the following manner:\r\nimport torch\r\n\r\nx = torch.ones(5)  # input tensor\r\ny = torch.zeros(3)  # expected output\r\nw = torch.randn(5, 3, requires_grad=True)\r\nb = torch.randn(3, requires_grad=True)\r\nz = torch.matmul(x, w)+b\r\nloss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\r\n\r\nTensors, Functions and Computational graph\u00b6\r\nThis code defines the following computational graph:\r\nIn this network, w and b are parameters, which we need to\r\noptimize. Thus, we need to be able to compute the gradients of loss\r\nfunction with respect to those variables. In order to do that, we set\r\nthe requires_grad property of those tensors.\r\nNote\r\nYou can set the value of requires_grad when creating a\r\ntensor, or later by using x.requires_grad_(True) method.\r\nA function that we apply to tensors to construct computational graph is\r\nin fact an object of class Function. This object knows how to\r\ncompute the function in the forward direction, and also how to compute\r\nits derivative during the backward propagation step. A reference to\r\nthe backward propagation function is stored in grad_fn property of a\r\ntensor. You can find more information of Function in the\r\ndocumentation.\r\nprint(f\"Gradient function for z = {z.grad_fn}\")\r\nprint(f\"Gradient function for loss = {loss.grad_fn}\")\r\n\r\nGradient function for z = <AddBackward0 object at 0x7f55737a4d90>\r\nGradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f55737a4ee0>\r\n\r\nComputing Gradients\u00b6\r\nTo optimize weights of parameters in the neural network, we need to\r\ncompute the derivatives of our loss function with respect to parameters,\r\nnamely, we need \\(\\frac{\\partial loss}{\\partial w}\\) and\r\n\\(\\frac{\\partial loss}{\\partial b}\\) under some fixed values of\r\nx and y. To compute those derivatives, we call\r\nloss.backward(), and then retrieve the values from w.grad and\r\nb.grad:\r\nloss.backward()\r\nprint(w.grad)\r\nprint(b.grad)\r\n\r\ntensor([[0.3313, 0.0626, 0.2530],\r\n        [0.3313, 0.0626, 0.2530],\r\n        [0.3313, 0.0626, 0.2530],\r\n        [0.3313, 0.0626, 0.2530],\r\n        [0.3313, 0.0626, 0.2530]])\r\ntensor([0.3313, 0.0626, 0.2530])\r\n\r\nNote\r\nWe can only obtain the grad properties for the leaf\r\nnodes of the computational graph, which have requires_grad property\r\nset to True. For all other nodes in our graph, gradients will not be\r\navailable.\r\nWe can only perform gradient calculations using\r\nbackward once on a given graph, for performance reasons. If we need\r\nto do several backward calls on the same graph, we need to pass\r\nretain_graph=True to the backward call.\r\nDisabling Gradient Tracking\u00b6\r\nBy default, all tensors with requires_grad=True are tracking their\r\ncomputational history and support gradient computation. However, there\r\nare some cases when we do not need to do that, for example, when we have\r\ntrained the model and just want to apply it to some input data, i.e. we\r\nonly want to do forward computations through the network. We can stop\r\ntracking computations by surrounding our computation code with\r\ntorch.no_grad() block:\r\nz = torch.matmul(x, w)+b\r\nprint(z.requires_grad)\r\n\r\nwith torch.no_grad():\r\n    z = torch.matmul(x, w)+b\r\nprint(z.requires_grad)\r\n\r\nTrue\r\nFalse\r\n\r\nAnother way to achieve the same result is to use the detach() method\r\non the tensor:\r\nz = torch.matmul(x, w)+b\r\nz_det = z.detach()\r\nprint(z_det.requires_grad)\r\n\r\nFalse\r\n\r\nThere are reasons you might want to disable gradient tracking:\r\n\r\nTo mark some parameters in your neural network as frozen parameters.\r\nTo speed up computations when you are only doing forward pass, because computations on tensors that do\r\nnot track gradients would be more efficient.\r\n\r\n\r\nTo mark some parameters in your neural network as frozen parameters.", "mimetype": "text/plain", "start_char_idx": 44942, "end_char_idx": 49103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9260e27-55a2-4846-9b09-8e26ccdc3fb4": {"__data__": {"id_": "a9260e27-55a2-4846-9b09-8e26ccdc3fb4", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f12ebdb1-c3b3-42ba-871f-a5f0cfb9c0b1", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "90e9b1d22470529e24415f9105b75af962798a4fb4d6a3fb9ccf54aa40bf1f90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82283394-fb3e-4fbb-ba98-74d379d90e38", "node_type": "1", "metadata": {}, "hash": "ad2024511c0eb55d1df737541226531044f1288e56fda2d470e40259aa3f9da9", "class_name": "RelatedNodeInfo"}}, "text": "we\r\nonly want to do forward computations through the network. We can stop\r\ntracking computations by surrounding our computation code with\r\ntorch.no_grad() block:\r\nz = torch.matmul(x, w)+b\r\nprint(z.requires_grad)\r\n\r\nwith torch.no_grad():\r\n    z = torch.matmul(x, w)+b\r\nprint(z.requires_grad)\r\n\r\nTrue\r\nFalse\r\n\r\nAnother way to achieve the same result is to use the detach() method\r\non the tensor:\r\nz = torch.matmul(x, w)+b\r\nz_det = z.detach()\r\nprint(z_det.requires_grad)\r\n\r\nFalse\r\n\r\nThere are reasons you might want to disable gradient tracking:\r\n\r\nTo mark some parameters in your neural network as frozen parameters.\r\nTo speed up computations when you are only doing forward pass, because computations on tensors that do\r\nnot track gradients would be more efficient.\r\n\r\n\r\nTo mark some parameters in your neural network as frozen parameters.\r\nTo speed up computations when you are only doing forward pass, because computations on tensors that do\r\nnot track gradients would be more efficient.\r\nMore on Computational Graphs\u00b6\r\nConceptually, autograd keeps a record of data (tensors) and all executed\r\noperations (along with the resulting new tensors) in a directed acyclic\r\ngraph (DAG) consisting of\r\nFunction\r\nobjects. In this DAG, leaves are the input tensors, roots are the output\r\ntensors. By tracing this graph from roots to leaves, you can\r\nautomatically compute the gradients using the chain rule.\r\nIn a forward pass, autograd does two things simultaneously:\r\nrun the requested operation to compute a resulting tensor\r\nmaintain the operation\u2019s gradient function in the DAG.\r\nThe backward pass kicks off when .backward() is called on the DAG\r\nroot. autograd then:\r\ncomputes the gradients from each .grad_fn,\r\naccumulates them in the respective tensor\u2019s .grad attribute\r\nusing the chain rule, propagates all the way to the leaf tensors.\r\nNote\r\nDAGs are dynamic in PyTorch\r\nAn important thing to note is that the graph is recreated from scratch; after each\r\n.backward() call, autograd starts populating a new graph. This is\r\nexactly what allows you to use control flow statements in your model;\r\nyou can change the shape, size and operations at every iteration if\r\nneeded.\r\nOptional Reading: Tensor Gradients and Jacobian Products\u00b6\r\nIn many cases, we have a scalar loss function, and we need to compute\r\nthe gradient with respect to some parameters. However, there are cases\r\nwhen the output function is an arbitrary tensor. In this case, PyTorch\r\nallows you to compute so-called Jacobian product, and not the actual\r\ngradient.\r\nFor a vector function \\(\\vec{y}=f(\\vec{x})\\), where\r\n\\(\\vec{x}=\\langle x_1,\\dots,x_n\\rangle\\) and\r\n\\(\\vec{y}=\\langle y_1,\\dots,y_m\\rangle\\), a gradient of\r\n\\(\\vec{y}\\) with respect to \\(\\vec{x}\\) is given by Jacobian\r\nmatrix:\r\nInstead of computing the Jacobian matrix itself, PyTorch allows you to\r\ncompute Jacobian Product \\(v^T\\cdot J\\) for a given input vector\r\n\\(v=(v_1 \\dots v_m)\\). This is achieved by calling backward with\r\n\\(v\\) as an argument. The size of \\(v\\) should be the same as\r\nthe size of the original tensor, with respect to which we want to\r\ncompute the product:\r\ninp = torch.eye(4, 5, requires_grad=True)\r\nout = (inp+1).pow(2).t()\r\nout.backward(torch.ones_like(out), retain_graph=True)\r\nprint(f\"First call\\n{inp.grad}\")\r\nout.backward(torch.ones_like(out), retain_graph=True)\r\nprint(f\"\\nSecond call\\n{inp.grad}\")\r\ninp.grad.zero_()\r\nout.backward(torch.ones_like(out), retain_graph=True)\r\nprint(f\"\\nCall after zeroing gradients\\n{inp.grad}\")\r\n\r\nFirst call\r\ntensor([[4., 2., 2., 2., 2.],\r\n        [2., 4., 2., 2., 2.],\r\n        [2., 2., 4., 2., 2.],\r\n        [2., 2., 2., 4., 2.]])\r\n\r\nSecond call\r\ntensor([[8., 4., 4., 4., 4.],\r\n        [4., 8., 4., 4., 4.],\r\n        [4., 4., 8., 4., 4.],\r\n        [4., 4., 4., 8., 4.]])\r\n\r\nCall after zeroing gradients\r\ntensor([[4., 2., 2., 2., 2.],\r\n        [2., 4., 2., 2., 2.", "mimetype": "text/plain", "start_char_idx": 48265, "end_char_idx": 52121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82283394-fb3e-4fbb-ba98-74d379d90e38": {"__data__": {"id_": "82283394-fb3e-4fbb-ba98-74d379d90e38", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9260e27-55a2-4846-9b09-8e26ccdc3fb4", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "99e9139a9a569864070a893da1ab1e2992cbcb85017924e16fc68738a8f18b46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edbfae17-be4f-4f59-91ed-0180b0e414ee", "node_type": "1", "metadata": {}, "hash": "9c0364fdcc563ed1192b0530b6f8c222e703aea6be2d9d65c0326f3ef033059d", "class_name": "RelatedNodeInfo"}}, "text": "],\r\n        [2., 4., 2., 2., 2.],\r\n        [2., 2., 4., 2., 2.],\r\n        [2., 2., 2., 4., 2.]])\r\n\r\nSecond call\r\ntensor([[8., 4., 4., 4., 4.],\r\n        [4., 8., 4., 4., 4.],\r\n        [4., 4., 8., 4., 4.],\r\n        [4., 4., 4., 8., 4.]])\r\n\r\nCall after zeroing gradients\r\ntensor([[4., 2., 2., 2., 2.],\r\n        [2., 4., 2., 2., 2.],\r\n        [2., 2., 4., 2., 2.],\r\n        [2., 2., 2., 4., 2.]])\r\n\r\nNotice that when we call backward for the second time with the same\r\nargument, the value of the gradient is different. This happens because\r\nwhen doing backward propagation, PyTorch accumulates the\r\ngradients, i.e. the value of computed gradients is added to the\r\ngrad property of all leaf nodes of computational graph. If you want\r\nto compute the proper gradients, you need to zero out the grad\r\nproperty before. In real-life training an optimizer helps us to do\r\nthis.\r\nNote\r\nPreviously we were calling backward() function without\r\nparameters. This is essentially equivalent to calling\r\nbackward(torch.tensor(1.0)), which is a useful way to compute the\r\ngradients in case of a scalar-valued function, such as loss during\r\nneural network training.\r\nFurther Reading\u00b6\r\nAutograd Mechanics\r\nTotal running time of the script: ( 0 minutes  0.014 seconds)\r\nDownload Python source code: autogradqs_tutorial.py\r\nDownload Jupyter notebook: autogradqs_tutorial.ipynb\r\nGallery generated by Sphinx-Gallery \r\n\r\nsource: https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html \r\ncontent: \r\n\r\nNote\r\nClick here\r\nto download the full example code\r\nLearn the Basics ||\r\nQuickstart ||\r\nTensors ||\r\nDatasets & DataLoaders ||\r\nTransforms ||\r\nBuild Model ||\r\nAutograd ||\r\nOptimization ||\r\nSave & Load Model\r\nOptimizing Model Parameters\u00b6\r\nNow that we have a model and data it\u2019s time to train, validate and test our model by optimizing its parameters on\r\nour data. Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates\r\nthe error in its guess (loss), collects the derivatives of the error with respect to its parameters (as we saw in\r\nthe previous section), and optimizes these parameters using gradient descent. For a more\r\ndetailed walkthrough of this process, check out this video on backpropagation from 3Blue1Brown.\r\nPrerequisite Code\u00b6\r\nWe load the code from the previous sections on Datasets & DataLoaders\r\nand Build Model.\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import datasets\r\nfrom torchvision.transforms import ToTensor\r\n\r\ntraining_data = datasets.FashionMNIST(\r\n    root=\"data\",\r\n    train=True,\r\n    download=True,\r\n    transform=ToTensor()\r\n)\r\n\r\ntest_data = datasets.FashionMNIST(\r\n    root=\"data\",\r\n    train=False,\r\n    download=True,\r\n    transform=ToTensor()\r\n)\r\n\r\ntrain_dataloader = DataLoader(training_data, batch_size=64)\r\ntest_dataloader = DataLoader(test_data, batch_size=64)\r\n\r\nclass NeuralNetwork(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.flatten = nn.Flatten()\r\n        self.linear_relu_stack = nn.Sequential(\r\n            nn.Linear(28*28, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 10),\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.flatten(x)\r\n        logits = self.linear_relu_stack(x)\r\n        return logits\r\n\r\nmodel = NeuralNetwork()\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\r\n\r\n  0%|          | 0.00/26.4M [00:00<?, ?B/s]\r\n  0%|          | 65.5k/26.4M [00:00<01:12,", "mimetype": "text/plain", "start_char_idx": 51793, "end_char_idx": 55525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edbfae17-be4f-4f59-91ed-0180b0e414ee": {"__data__": {"id_": "edbfae17-be4f-4f59-91ed-0180b0e414ee", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82283394-fb3e-4fbb-ba98-74d379d90e38", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "52fcf3226350154175bfe197aa2840d6676c1f1ea3c503c8324adce710de6705", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47dd6256-4647-4f38-a5e5-e49c6a9a7944", "node_type": "1", "metadata": {}, "hash": "649c15cb8399ee99e153b230aea4ae00a3b86a7da29ff8ea48c9b1fca52ae85a", "class_name": "RelatedNodeInfo"}}, "text": "ReLU(),\r\n            nn.Linear(512, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 10),\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.flatten(x)\r\n        logits = self.linear_relu_stack(x)\r\n        return logits\r\n\r\nmodel = NeuralNetwork()\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\r\n\r\n  0%|          | 0.00/26.4M [00:00<?, ?B/s]\r\n  0%|          | 65.5k/26.4M [00:00<01:12, 362kB/s]\r\n  0%|          | 131k/26.4M [00:00<01:12, 362kB/s]\r\n  1%|          | 229k/26.4M [00:00<00:59, 444kB/s]\r\n  1%|1         | 328k/26.4M [00:00<00:54, 483kB/s]\r\n  2%|1         | 426k/26.4M [00:00<00:51, 505kB/s]\r\n  2%|2         | 557k/26.4M [00:01<00:44, 578kB/s]\r\n  3%|2         | 688k/26.4M [00:01<00:41, 626kB/s]\r\n  3%|3         | 852k/26.4M [00:01<00:35, 713kB/s]\r\n  4%|3         | 1.02M/26.4M [00:01<00:32, 772kB/s]\r\n  4%|4         | 1.18M/26.4M [00:01<00:31, 814kB/s]\r\n  5%|5         | 1.38M/26.4M [00:01<00:27, 896kB/s]\r\n  6%|6         | 1.61M/26.4M [00:02<00:24, 1.01MB/s]\r\n  7%|6         | 1.84M/26.4M [00:02<00:20, 1.18MB/s]\r\n  8%|7         | 2.00M/26.4M [00:02<00:20, 1.18MB/s]\r\n  9%|8         | 2.26M/26.4M [00:02<00:19, 1.27MB/s]\r\n 10%|9         | 2.56M/26.4M [00:02<00:17, 1.38MB/s]\r\n 11%|#         | 2.88M/26.4M [00:02<00:14, 1.65MB/s]\r\n 12%|#1        | 3.08M/26.4M [00:03<00:14, 1.59MB/s]\r\n 13%|#3        | 3.47M/26.4M [00:03<00:12, 1.79MB/s]\r\n 15%|#4        | 3.90M/26.4M [00:03<00:10, 2.14MB/s]\r\n 16%|#5        | 4.16M/26.4M [00:03<00:10, 2.08MB/s]\r\n 17%|#7        | 4.62M/26.4M [00:03<00:08, 2.45MB/s]\r\n 19%|#8        | 4.92M/26.4M [00:03<00:09, 2.37MB/s]\r\n 21%|##        | 5.47M/26.4M [00:03<00:07, 2.87MB/s]\r\n 22%|##1       | 5.80M/26.4M [00:04<00:07, 2.73MB/s]\r\n 24%|##4       | 6.46M/26.4M [00:04<00:05, 3.", "mimetype": "text/plain", "start_char_idx": 54921, "end_char_idx": 56860, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47dd6256-4647-4f38-a5e5-e49c6a9a7944": {"__data__": {"id_": "47dd6256-4647-4f38-a5e5-e49c6a9a7944", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edbfae17-be4f-4f59-91ed-0180b0e414ee", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "3ee355d80910ac0228794c3ddcefbba15d12c8ce1d4b413575599b2cee13a780", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a392b378-61b4-44b5-a9ea-92da83b583e1", "node_type": "1", "metadata": {}, "hash": "74a5d305b5a46128e5f4de4cab13d7123dcc38007d85ddeabbb688859c163c90", "class_name": "RelatedNodeInfo"}}, "text": "14MB/s]\r\n 16%|#5        | 4.16M/26.4M [00:03<00:10, 2.08MB/s]\r\n 17%|#7        | 4.62M/26.4M [00:03<00:08, 2.45MB/s]\r\n 19%|#8        | 4.92M/26.4M [00:03<00:09, 2.37MB/s]\r\n 21%|##        | 5.47M/26.4M [00:03<00:07, 2.87MB/s]\r\n 22%|##1       | 5.80M/26.4M [00:04<00:07, 2.73MB/s]\r\n 24%|##4       | 6.46M/26.4M [00:04<00:05, 3.35MB/s]\r\n 26%|##5       | 6.85M/26.4M [00:04<00:06, 3.21MB/s]\r\n 29%|##8       | 7.60M/26.4M [00:04<00:04, 3.91MB/s]\r\n 30%|###       | 8.03M/26.4M [00:04<00:05, 3.67MB/s]\r\n 34%|###3      | 8.91M/26.4M [00:04<00:03, 4.52MB/s]\r\n 36%|###5      | 9.44M/26.4M [00:04<00:03, 4.31MB/s]\r\n 39%|###9      | 10.4M/26.4M [00:05<00:03, 5.19MB/s]\r\n 42%|####1     | 11.0M/26.4M [00:05<00:03, 5.00MB/s]\r\n 46%|####6     | 12.2M/26.4M [00:05<00:02, 5.96MB/s]\r\n 49%|####8     | 12.9M/26.4M [00:05<00:02, 5.76MB/s]\r\n 54%|#####3    | 14.2M/26.4M [00:05<00:01, 6.93MB/s]\r\n 57%|#####6    | 15.0M/26.4M [00:05<00:01, 6.65MB/s]\r\n 63%|######2   | 16.5M/26.4M [00:05<00:01, 8.05MB/s]\r\n 66%|######6   | 17.5M/26.4M [00:05<00:01, 7.73MB/s]\r\n 73%|#######2  | 19.3M/26.4M [00:06<00:00, 9.31MB/s]\r\n 77%|#######7  | 20.3M/26.4M [00:06<00:00, 8.90MB/s]\r\n 85%|########4 | 22.4M/26.4M [00:06<00:00, 10.7MB/s]\r\n 89%|########9 | 23.6M/26.4M [00:06<00:00, 10.2MB/s]\r\n 98%|#########8| 26.0M/26.4M [00:06<00:00, 12.6MB/s]\r\n100%|##########| 26.4M/26.4M [00:06<00:00, 3.93MB/s]\r\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\r\n\r\n  0%|          | 0.00/29.5k [00:00<?, ?B/s]\r\n100%|##########| 29.5k/29.5k [00:00<00:00,", "mimetype": "text/plain", "start_char_idx": 56536, "end_char_idx": 58322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a392b378-61b4-44b5-a9ea-92da83b583e1": {"__data__": {"id_": "a392b378-61b4-44b5-a9ea-92da83b583e1", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47dd6256-4647-4f38-a5e5-e49c6a9a7944", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "f14785959fc34ff2ecb2d2d77d8f5005af935e151155eba60d7a7c207a15e631", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ee5bf1a-6eae-42ef-9bbd-2cf4d097830f", "node_type": "1", "metadata": {}, "hash": "7db1effa6af3eb8a7590d98d1b4f0f24e02463ebd9e517fdcaee0def7169b921", "class_name": "RelatedNodeInfo"}}, "text": "12.6MB/s]\r\n100%|##########| 26.4M/26.4M [00:06<00:00, 3.93MB/s]\r\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\r\n\r\n  0%|          | 0.00/29.5k [00:00<?, ?B/s]\r\n100%|##########| 29.5k/29.5k [00:00<00:00, 328kB/s]\r\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\r\n\r\n  0%|          | 0.00/4.42M [00:00<?, ?B/s]\r\n  1%|1         | 65.5k/4.42M [00:00<00:12, 361kB/s]\r\n  3%|2         | 131k/4.42M [00:00<00:11, 362kB/s]\r\n  4%|4         | 197k/4.42M [00:00<00:11, 362kB/s]\r\n  7%|6         | 295k/4.42M [00:00<00:09, 433kB/s]\r\n  9%|8         | 393k/4.42M [00:00<00:08, 472kB/s]\r\n 11%|#1        | 492k/4.42M [00:01<00:07, 496kB/s]\r\n 14%|#4        | 623k/4.42M [00:01<00:06, 569kB/s]\r\n 17%|#7        | 754k/4.42M [00:01<00:05, 618kB/s]\r\n 20%|##        | 885k/4.42M [00:01<00:05, 651kB/s]\r\n 24%|##3       | 1.05M/4.42M [00:01<00:04, 728kB/s]\r\n 27%|##7       | 1.21M/4.42M [00:01<00:04, 782kB/s]\r\n 32%|###1      | 1.41M/4.42M [00:02<00:03, 874kB/s]\r\n 36%|###6      | 1.61M/4.42M [00:02<00:03, 938kB/s]\r\n 41%|####1     | 1.84M/4.42M [00:02<00:02, 1.04MB/s]\r\n 47%|####7     | 2.10M/4.42M [00:02<00:02, 1.16MB/s]\r\n 53%|#####3    | 2.36M/4.42M [00:02<00:01, 1.25MB/s]\r\n 60%|######    | 2.65M/4.42M [00:03<00:01, 1.36MB/s]\r\n 68%|######8   | 3.01M/4.42M [00:03<00:00, 1.55MB/s]\r\n 76%|#######6  | 3.38M/4.42M [00:03<00:00, 1.68MB/s]\r\n 85%|########5 | 3.77M/4.42M [00:03<00:00, 1.97MB/s]\r\n 90%|######### | 4.00M/4.42M [00:03<00:00, 1.90MB/s]\r\n100%|##########| 4.", "mimetype": "text/plain", "start_char_idx": 57830, "end_char_idx": 59847, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ee5bf1a-6eae-42ef-9bbd-2cf4d097830f": {"__data__": {"id_": "6ee5bf1a-6eae-42ef-9bbd-2cf4d097830f", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a392b378-61b4-44b5-a9ea-92da83b583e1", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "beefad6eb9eefe5e78fc2c253ac1712b0fdf7650edbe390c119dfdcd9c2923f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d871b0f-0168-4be8-9a9e-d70a4ccc0cfb", "node_type": "1", "metadata": {}, "hash": "6c399317261e9a23c3a721a30f4bbbb93fe529e3d76743d59f87f4bc25e76148", "class_name": "RelatedNodeInfo"}}, "text": "36M/4.42M [00:02<00:01, 1.25MB/s]\r\n 60%|######    | 2.65M/4.42M [00:03<00:01, 1.36MB/s]\r\n 68%|######8   | 3.01M/4.42M [00:03<00:00, 1.55MB/s]\r\n 76%|#######6  | 3.38M/4.42M [00:03<00:00, 1.68MB/s]\r\n 85%|########5 | 3.77M/4.42M [00:03<00:00, 1.97MB/s]\r\n 90%|######### | 4.00M/4.42M [00:03<00:00, 1.90MB/s]\r\n100%|##########| 4.42M/4.42M [00:03<00:00, 1.16MB/s]\r\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\r\n\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\r\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\r\n\r\n  0%|          | 0.00/5.15k [00:00<?, ?B/s]\r\n100%|##########| 5.15k/5.15k [00:00<00:00, 35.5MB/s]\r\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\r\n\r\nHyperparameters\u00b6\r\nHyperparameters are adjustable parameters that let you control the model optimization process.\r\nDifferent hyperparameter values can impact model training and convergence rates\r\n(read more about hyperparameter tuning)\r\nWe define the following hyperparameters for training:\r\n\r\nNumber of Epochs - the number times to iterate over the dataset\r\nBatch Size - the number of data samples propagated through the network before the parameters are updated\r\nLearning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.\r\n\r\n\r\nNumber of Epochs - the number times to iterate over the dataset\r\nBatch Size - the number of data samples propagated through the network before the parameters are updated\r\nLearning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.\r\nlearning_rate = 1e-3\r\nbatch_size = 64\r\nepochs = 5\r\n\r\nOptimization Loop\u00b6\r\nOnce we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each\r\niteration of the optimization loop is called an epoch.\r\nEach epoch consists of two main parts:\r\n\r\nThe Train Loop - iterate over the training dataset and try to converge to optimal parameters.\r\nThe Validation/Test Loop - iterate over the test dataset to check if model performance is improving.\r\n\r\n\r\nThe Train Loop - iterate over the training dataset and try to converge to optimal parameters.\r\nThe Validation/Test Loop - iterate over the test dataset to check if model performance is improving.\r\nLet\u2019s briefly familiarize ourselves with some of the concepts used in the training loop. Jump ahead to\r\nsee the Full Implementation of the optimization loop.\r\nLoss Function\u00b6\r\nWhen presented with some training data, our untrained network is likely not to give the correct\r\nanswer. Loss function measures the degree of dissimilarity of obtained result to the target value,\r\nand it is the loss function that we want to minimize during training. To calculate the loss we make a\r\nprediction using the inputs of our given data sample and compare it against the true data label value.\r\nCommon loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and\r\nnn.NLLLoss (Negative Log Likelihood) for classification.\r\nnn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss.\r\nWe pass our model\u2019s output logits to nn.CrossEntropyLoss, which will normalize the logits and compute the prediction error.\r\n# Initialize the loss function\r\nloss_fn = nn.CrossEntropyLoss()\r\n\r\nOptimizer\u00b6\r\nOptimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed (in this example we use Stochastic Gradient Descent).\r\nAll optimization logic is encapsulated in  the optimizer object.", "mimetype": "text/plain", "start_char_idx": 59523, "end_char_idx": 63371, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d871b0f-0168-4be8-9a9e-d70a4ccc0cfb": {"__data__": {"id_": "4d871b0f-0168-4be8-9a9e-d70a4ccc0cfb", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ee5bf1a-6eae-42ef-9bbd-2cf4d097830f", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "97c7020cfc52a99c0180231dcd9c3e73c698e75715239abbd4838596063d78d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c105476-5e5c-4004-9eea-ff9bdace056a", "node_type": "1", "metadata": {}, "hash": "0930edb31c24d327317161cd37b21a7c5ad88992ce26569ca40afb1c00600eaa", "class_name": "RelatedNodeInfo"}}, "text": "Loss function measures the degree of dissimilarity of obtained result to the target value,\r\nand it is the loss function that we want to minimize during training. To calculate the loss we make a\r\nprediction using the inputs of our given data sample and compare it against the true data label value.\r\nCommon loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and\r\nnn.NLLLoss (Negative Log Likelihood) for classification.\r\nnn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss.\r\nWe pass our model\u2019s output logits to nn.CrossEntropyLoss, which will normalize the logits and compute the prediction error.\r\n# Initialize the loss function\r\nloss_fn = nn.CrossEntropyLoss()\r\n\r\nOptimizer\u00b6\r\nOptimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed (in this example we use Stochastic Gradient Descent).\r\nAll optimization logic is encapsulated in  the optimizer object. Here, we use the SGD optimizer; additionally, there are many different optimizers\r\navailable in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.\r\nWe initialize the optimizer by registering the model\u2019s parameters that need to be trained, and passing in the learning rate hyperparameter.\r\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\r\n\r\nInside the training loop, optimization happens in three steps:\r\n\r\nCall optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\r\nBackpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\r\nOnce we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass.\r\n\r\n\r\nCall optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\r\nBackpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\r\nOnce we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass.\r\nFull Implementation\u00b6\r\nWe define train_loop that loops over our optimization code, and test_loop that\r\nevaluates the model\u2019s performance against our test data.\r\ndef train_loop(dataloader, model, loss_fn, optimizer):\r\n    size = len(dataloader.dataset)\r\n    # Set the model to training mode - important for batch normalization and dropout layers\r\n    # Unnecessary in this situation but added for best practices\r\n    model.train()\r\n    for batch, (X, y) in enumerate(dataloader):\r\n        # Compute prediction and loss\r\n        pred = model(X)\r\n        loss = loss_fn(pred, y)\r\n\r\n        # Backpropagation\r\n        loss.backward()\r\n        optimizer.step()\r\n        optimizer.zero_grad()\r\n\r\n        if batch % 100 == 0:\r\n            loss, current = loss.item(), batch * batch_size + len(X)\r\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\r\n\r\n\r\ndef test_loop(dataloader, model, loss_fn):\r\n    # Set the model to evaluation mode - important for batch normalization and dropout layers\r\n    # Unnecessary in this situation but added for best practices\r\n    model.eval()\r\n    size = len(dataloader.dataset)\r\n    num_batches = len(dataloader)\r\n    test_loss, correct = 0, 0\r\n\r\n    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\r\n    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\r\n    with torch.no_grad():\r\n        for X, y in dataloader:\r\n            pred = model(X)\r\n            test_loss += loss_fn(pred, y).item()\r\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\r\n\r\n    test_loss /= num_batches\r\n    correct /= size\r\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\r\n\r\nWe initialize the loss function and optimizer, and pass it to train_loop and test_loop.\r\nFeel free to increase the number of epochs to track the model\u2019s improving performance.\r\nloss_fn = nn.CrossEntropyLoss()\r\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\r\n\r\nepochs = 10\r\nfor t in range(epochs):\r\n    print(f\"Epoch {t+1}\\n-------------------------------\")\r\n    train_loop(train_dataloader, model, loss_fn, optimizer)\r\n    test_loop(test_dataloader, model, loss_fn)\r\nprint(\"Done!\")\r\n\r\nEpoch 1\r\n-------------------------------\r\nloss: 2.", "mimetype": "text/plain", "start_char_idx": 62379, "end_char_idx": 67033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c105476-5e5c-4004-9eea-ff9bdace056a": {"__data__": {"id_": "1c105476-5e5c-4004-9eea-ff9bdace056a", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d871b0f-0168-4be8-9a9e-d70a4ccc0cfb", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "51f87586f80dc042b8a426f61e860ffe5a32b2d86b2b18924ffce10c9ad15592", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bedc3f1e-39b4-45d6-8ea0-5709999a3328", "node_type": "1", "metadata": {}, "hash": "66ae9c704dfc7d8bb86e7efd05fce7ae8dd279f736ac43165043f474a0f92a2e", "class_name": "RelatedNodeInfo"}}, "text": "Feel free to increase the number of epochs to track the model\u2019s improving performance.\r\nloss_fn = nn.CrossEntropyLoss()\r\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\r\n\r\nepochs = 10\r\nfor t in range(epochs):\r\n    print(f\"Epoch {t+1}\\n-------------------------------\")\r\n    train_loop(train_dataloader, model, loss_fn, optimizer)\r\n    test_loop(test_dataloader, model, loss_fn)\r\nprint(\"Done!\")\r\n\r\nEpoch 1\r\n-------------------------------\r\nloss: 2.298730  [   64/60000]\r\nloss: 2.289123  [ 6464/60000]\r\nloss: 2.273286  [12864/60000]\r\nloss: 2.269406  [19264/60000]\r\nloss: 2.249603  [25664/60000]\r\nloss: 2.229407  [32064/60000]\r\nloss: 2.227368  [38464/60000]\r\nloss: 2.204261  [44864/60000]\r\nloss: 2.206193  [51264/60000]\r\nloss: 2.166651  [57664/60000]\r\nTest Error:\r\n Accuracy: 50.9%, Avg loss: 2.166725\r\n\r\nEpoch 2\r\n-------------------------------\r\nloss: 2.176750  [   64/60000]\r\nloss: 2.169595  [ 6464/60000]\r\nloss: 2.117500  [12864/60000]\r\nloss: 2.129272  [19264/60000]\r\nloss: 2.079674  [25664/60000]\r\nloss: 2.032928  [32064/60000]\r\nloss: 2.050115  [38464/60000]\r\nloss: 1.985236  [44864/60000]\r\nloss: 1.987887  [51264/60000]\r\nloss: 1.907162  [57664/60000]\r\nTest Error:\r\n Accuracy: 55.9%, Avg loss: 1.915486\r\n\r\nEpoch 3\r\n-------------------------------\r\nloss: 1.951612  [   64/60000]\r\nloss: 1.928685  [ 6464/60000]\r\nloss: 1.815709  [12864/60000]\r\nloss: 1.841552  [19264/60000]\r\nloss: 1.732467  [25664/60000]\r\nloss: 1.692914  [32064/60000]\r\nloss: 1.701714  [38464/60000]\r\nloss: 1.610632  [44864/60000]\r\nloss: 1.632870  [51264/60000]\r\nloss: 1.514263  [57664/60000]\r\nTest Error:\r\n Accuracy: 58.8%, Avg loss: 1.541525\r\n\r\nEpoch 4\r\n-------------------------------\r\nloss: 1.616448  [   64/60000]\r\nloss: 1.582892  [ 6464/60000]\r\nloss: 1.427595  [12864/60000]\r\nloss: 1.487950  [19264/60000]\r\nloss: 1.359332  [25664/60000]\r\nloss: 1.364817  [32064/60000]\r\nloss: 1.371491  [38464/60000]\r\nloss: 1.298706  [44864/60000]\r\nloss: 1.336201  [51264/60000]\r\nloss: 1.232145  [57664/60000]\r\nTest Error:\r\n Accuracy: 62.2%, Avg loss: 1.260237\r\n\r\nEpoch 5\r\n-------------------------------\r\nloss: 1.345538  [   64/60000]\r\nloss: 1.327798  [ 6464/60000]\r\nloss: 1.153802  [12864/60000]\r\nloss: 1.254829  [19264/60000]\r\nloss: 1.117322  [25664/60000]\r\nloss: 1.153248  [32064/60000]\r\nloss: 1.171765  [38464/60000]\r\nloss: 1.110263  [44864/60000]\r\nloss: 1.154467  [51264/60000]\r\nloss: 1.070921  [57664/60000]\r\nTest Error:\r\n Accuracy: 64.1%, Avg loss: 1.089831\r\n\r\nEpoch 6\r\n-------------------------------\r\nloss: 1.166889  [   64/60000]\r\nloss: 1.", "mimetype": "text/plain", "start_char_idx": 66568, "end_char_idx": 69089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bedc3f1e-39b4-45d6-8ea0-5709999a3328": {"__data__": {"id_": "bedc3f1e-39b4-45d6-8ea0-5709999a3328", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c105476-5e5c-4004-9eea-ff9bdace056a", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "a753f5a320bda115fe8219fef212a1008f1d2f821bcdec50f814d56d3f62a3fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ceba251d-952b-4e73-a6b6-6f49da2bce9e", "node_type": "1", "metadata": {}, "hash": "3d5f9e8242f1d84512ebc8a312591091fc7127cbfbef84ec6d3787eb45798ed4", "class_name": "RelatedNodeInfo"}}, "text": "345538  [   64/60000]\r\nloss: 1.327798  [ 6464/60000]\r\nloss: 1.153802  [12864/60000]\r\nloss: 1.254829  [19264/60000]\r\nloss: 1.117322  [25664/60000]\r\nloss: 1.153248  [32064/60000]\r\nloss: 1.171765  [38464/60000]\r\nloss: 1.110263  [44864/60000]\r\nloss: 1.154467  [51264/60000]\r\nloss: 1.070921  [57664/60000]\r\nTest Error:\r\n Accuracy: 64.1%, Avg loss: 1.089831\r\n\r\nEpoch 6\r\n-------------------------------\r\nloss: 1.166889  [   64/60000]\r\nloss: 1.170514  [ 6464/60000]\r\nloss: 0.979435  [12864/60000]\r\nloss: 1.113774  [19264/60000]\r\nloss: 0.973411  [25664/60000]\r\nloss: 1.015192  [32064/60000]\r\nloss: 1.051113  [38464/60000]\r\nloss: 0.993591  [44864/60000]\r\nloss: 1.039709  [51264/60000]\r\nloss: 0.971077  [57664/60000]\r\nTest Error:\r\n Accuracy: 65.8%, Avg loss: 0.982440\r\n\r\nEpoch 7\r\n-------------------------------\r\nloss: 1.045165  [   64/60000]\r\nloss: 1.070583  [ 6464/60000]\r\nloss: 0.862304  [12864/60000]\r\nloss: 1.022265  [19264/60000]\r\nloss: 0.885213  [25664/60000]\r\nloss: 0.919528  [32064/60000]\r\nloss: 0.972762  [38464/60000]\r\nloss: 0.918728  [44864/60000]\r\nloss: 0.961629  [51264/60000]\r\nloss: 0.904379  [57664/60000]\r\nTest Error:\r\n Accuracy: 66.9%, Avg loss: 0.910167\r\n\r\nEpoch 8\r\n-------------------------------\r\nloss: 0.956964  [   64/60000]\r\nloss: 1.002171  [ 6464/60000]\r\nloss: 0.779057  [12864/60000]\r\nloss: 0.958409  [19264/60000]\r\nloss: 0.827240  [25664/60000]\r\nloss: 0.850262  [32064/60000]\r\nloss: 0.917320  [38464/60000]\r\nloss: 0.868384  [44864/60000]\r\nloss: 0.905506  [51264/60000]\r\nloss: 0.856353  [57664/60000]\r\nTest Error:\r\n Accuracy: 68.3%, Avg loss: 0.858248\r\n\r\nEpoch 9\r\n-------------------------------\r\nloss: 0.889765  [   64/60000]\r\nloss: 0.951220  [ 6464/60000]\r\nloss: 0.717035  [12864/60000]\r\nloss: 0.911042  [19264/60000]\r\nloss: 0.786085  [25664/60000]\r\nloss: 0.798370  [32064/60000]\r\nloss: 0.874939  [38464/60000]\r\nloss: 0.832796  [44864/60000]\r\nloss: 0.863254  [51264/60000]\r\nloss: 0.819742  [57664/60000]\r\nTest Error:\r\n Accuracy: 69.5%, Avg loss: 0.818780\r\n\r\nEpoch 10\r\n-------------------------------\r\nloss: 0.836395  [   64/60000]\r\nloss: 0.910220  [ 6464/60000]\r\nloss: 0.668506  [12864/60000]\r\nloss: 0.874338  [19264/60000]\r\nloss: 0.754805  [25664/60000]\r\nloss: 0.758453  [32064/60000]\r\nloss: 0.840451  [38464/60000]\r\nloss: 0.806153  [44864/60000]\r\nloss: 0.", "mimetype": "text/plain", "start_char_idx": 68653, "end_char_idx": 70927, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ceba251d-952b-4e73-a6b6-6f49da2bce9e": {"__data__": {"id_": "ceba251d-952b-4e73-a6b6-6f49da2bce9e", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bedc3f1e-39b4-45d6-8ea0-5709999a3328", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "75c862bb0b55a021025c2abf278233361a0994f420410996aafcdaa9246c7d74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d8e219b-298e-4c23-9f35-6a911992d386", "node_type": "1", "metadata": {}, "hash": "8a6ad89fd5a9f85e28059599bfba58076abb7f7b0c5a127e0470ef734d6592d6", "class_name": "RelatedNodeInfo"}}, "text": "832796  [44864/60000]\r\nloss: 0.863254  [51264/60000]\r\nloss: 0.819742  [57664/60000]\r\nTest Error:\r\n Accuracy: 69.5%, Avg loss: 0.818780\r\n\r\nEpoch 10\r\n-------------------------------\r\nloss: 0.836395  [   64/60000]\r\nloss: 0.910220  [ 6464/60000]\r\nloss: 0.668506  [12864/60000]\r\nloss: 0.874338  [19264/60000]\r\nloss: 0.754805  [25664/60000]\r\nloss: 0.758453  [32064/60000]\r\nloss: 0.840451  [38464/60000]\r\nloss: 0.806153  [44864/60000]\r\nloss: 0.830360  [51264/60000]\r\nloss: 0.790281  [57664/60000]\r\nTest Error:\r\n Accuracy: 71.0%, Avg loss: 0.787271\r\n\r\nDone!\r\n\r\nFurther Reading\u00b6\r\nLoss Functions\r\ntorch.optim\r\nWarmstart Training a Model\r\nTotal running time of the script: ( 2 minutes  16.268 seconds)\r\nDownload Python source code: optimization_tutorial.py\r\nDownload Jupyter notebook: optimization_tutorial.ipynb\r\nGallery generated by Sphinx-Gallery \r\n\r\nsource: https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html \r\ncontent: \r\n\r\nNote\r\nClick here\r\nto download the full example code\r\nLearn the Basics ||\r\nQuickstart ||\r\nTensors ||\r\nDatasets & DataLoaders ||\r\nTransforms ||\r\nBuild Model ||\r\nAutograd ||\r\nOptimization ||\r\nSave & Load Model\r\nSave and Load the Model\u00b6\r\nIn this section we will look at how to persist model state with saving, loading and running model predictions.\r\nimport torch\r\nimport torchvision.models as models\r\n\r\nSaving and Loading Model Weights\u00b6\r\nPyTorch models store the learned parameters in an internal\r\nstate dictionary, called state_dict.", "mimetype": "text/plain", "start_char_idx": 70490, "end_char_idx": 71958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d8e219b-298e-4c23-9f35-6a911992d386": {"__data__": {"id_": "9d8e219b-298e-4c23-9f35-6a911992d386", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ceba251d-952b-4e73-a6b6-6f49da2bce9e", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "d8bd7b68bfe14614b13a7e59d4dfd8bfba866a4dd8ebce5aeae4c7372bcd6d71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c4cfd30-e960-4441-ae38-acd0ebec4c6a", "node_type": "1", "metadata": {}, "hash": "64b308c0a71cd37c483e4940773d621a523be14345c65d1b7f5570ecd410d0b7", "class_name": "RelatedNodeInfo"}}, "text": "Further Reading\u00b6\r\nLoss Functions\r\ntorch.optim\r\nWarmstart Training a Model\r\nTotal running time of the script: ( 2 minutes  16.268 seconds)\r\nDownload Python source code: optimization_tutorial.py\r\nDownload Jupyter notebook: optimization_tutorial.ipynb\r\nGallery generated by Sphinx-Gallery \r\n\r\nsource: https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html \r\ncontent: \r\n\r\nNote\r\nClick here\r\nto download the full example code\r\nLearn the Basics ||\r\nQuickstart ||\r\nTensors ||\r\nDatasets & DataLoaders ||\r\nTransforms ||\r\nBuild Model ||\r\nAutograd ||\r\nOptimization ||\r\nSave & Load Model\r\nSave and Load the Model\u00b6\r\nIn this section we will look at how to persist model state with saving, loading and running model predictions.\r\nimport torch\r\nimport torchvision.models as models\r\n\r\nSaving and Loading Model Weights\u00b6\r\nPyTorch models store the learned parameters in an internal\r\nstate dictionary, called state_dict. These can be persisted via the torch.save\r\nmethod:\r\nmodel = models.vgg16(weights='IMAGENET1K_V1')\r\ntorch.save(model.state_dict(), 'model_weights.pth')\r\n\r\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/vgg16-397923af.pth\r\n\r\n  0%|          | 0.00/528M [00:00<?, ?B/s]\r\n  3%|3         | 16.8M/528M [00:00<00:03, 175MB/s]\r\n  7%|6         | 34.6M/528M [00:00<00:02, 182MB/s]\r\n 10%|9         | 52.4M/528M [00:00<00:02, 184MB/s]\r\n 13%|#3        | 70.2M/528M [00:00<00:02, 185MB/s]\r\n 17%|#6        | 88.1M/528M [00:00<00:02, 186MB/s]\r\n 20%|##        | 106M/528M [00:00<00:02, 186MB/s]\r\n 23%|##3       | 124M/528M [00:00<00:02, 186MB/s]\r\n 27%|##6       | 142M/528M [00:00<00:02, 186MB/s]\r\n 30%|###       | 160M/528M [00:00<00:02, 186MB/s]\r\n 34%|###3      | 177M/528M [00:01<00:01, 186MB/s]\r\n 37%|###6      | 195M/528M [00:01<00:01, 186MB/s]\r\n 40%|####      | 213M/528M [00:01<00:01, 186MB/s]\r\n 44%|####3     | 231M/528M [00:01<00:01, 187MB/s]\r\n 47%|####7     | 249M/528M [00:01<00:01, 186MB/s]\r\n 51%|#####     | 267M/528M [00:01<00:01, 186MB/s]\r\n 54%|#####3    | 285M/528M [00:01<00:01, 187MB/s]\r\n 57%|#####7    | 303M/528M [00:01<00:01, 187MB/s]\r\n 61%|######    | 320M/528M [00:01<00:01, 187MB/s]\r\n 64%|######4   | 338M/528M [00:01<00:01, 187MB/s]\r\n 67%|######7   | 356M/528M [00:02<00:00, 187MB/s]\r\n 71%|#######   | 374M/528M [00:02<00:00, 187MB/s]\r\n 74%|#######4  | 392M/528M [00:02<00:00, 187MB/s]\r\n 78%|#######7  | 410M/528M [00:02<00:00, 187MB/s]\r\n 81%|########1 | 428M/528M [00:02<00:00, 187MB/s]\r\n 84%|########4 | 446M/528M [00:02<00:00, 187MB/s]\r\n 88%|########7 | 464M/528M [00:02<00:00, 187MB/s]\r\n 91%|#########1| 482M/528M [00:02<00:00, 187MB/s]\r\n 95%|#########4| 500M/528M [00:02<00:00, 187MB/s]\r\n 98%|#########8| 518M/528M [00:02<00:00, 187MB/s]\r\n100%|##########| 528M/528M [00:02<00:00, 186MB/s]\r\n\r\nTo load model weights, you need to create an instance of the same model first, and then load the parameters\r\nusing load_state_dict() method.", "mimetype": "text/plain", "start_char_idx": 71043, "end_char_idx": 73975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c4cfd30-e960-4441-ae38-acd0ebec4c6a": {"__data__": {"id_": "9c4cfd30-e960-4441-ae38-acd0ebec4c6a", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d8e219b-298e-4c23-9f35-6a911992d386", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "2239d7a963c358777d29ef633517eceaa96cd96fa2c1a981cdddc91bd22d1d64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf1ab04c-4203-4ac1-81f6-35e0d148a3fa", "node_type": "1", "metadata": {}, "hash": "fb17442ca562d5e47a642749fba423726c36a75555a92debb20acd2d5db8879a", "class_name": "RelatedNodeInfo"}}, "text": "In the code below, we set weights_only=True to limit the\r\nfunctions executed during unpickling to only those necessary for\r\nloading weights. Using weights_only=True is considered\r\na best practice when loading weights.\r\nmodel = models.vgg16() # we do not specify ``weights``, i.e. create untrained model\r\nmodel.load_state_dict(torch.load('model_weights.pth', weights_only=True))\r\nmodel.eval()\r\n\r\nVGG(\r\n  (features): Sequential(\r\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n    (1): ReLU(inplace=True)\r\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n    (3): ReLU(inplace=True)\r\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\r\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n    (6): ReLU(inplace=True)\r\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n    (8): ReLU(inplace=True)\r\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\r\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n    (11): ReLU(inplace=True)\r\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n    (13): ReLU(inplace=True)\r\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n    (15): ReLU(inplace=True)\r\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\r\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n    (18): ReLU(inplace=True)\r\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n    (20): ReLU(inplace=True)\r\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n    (22): ReLU(inplace=True)\r\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\r\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n    (25): ReLU(inplace=True)\r\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n    (27): ReLU(inplace=True)\r\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n    (29): ReLU(inplace=True)\r\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\r\n  )\r\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\r\n  (classifier): Sequential(\r\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\r\n    (1): ReLU(inplace=True)\r\n    (2): Dropout(p=0.5, inplace=False)\r\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\r\n    (4): ReLU(inplace=True)\r\n    (5): Dropout(p=0.5, inplace=False)\r\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\r\n  )\r\n)\r\n\r\nNote\r\nbe sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results.", "mimetype": "text/plain", "start_char_idx": 73977, "end_char_idx": 76858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf1ab04c-4203-4ac1-81f6-35e0d148a3fa": {"__data__": {"id_": "cf1ab04c-4203-4ac1-81f6-35e0d148a3fa", "embedding": null, "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76a0bad1-36b2-4021-8ba3-6fd36e412a7f", "node_type": "4", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7a71618ca0f05bd63ef5f169f2fb2bdfa7da4565c4cfcedc2e4e823c9cd5d5e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c4cfd30-e960-4441-ae38-acd0ebec4c6a", "node_type": "1", "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "0225482585fa0da87357ac9c8000b8fee2306cabc6f1de1b6ad6f76ba3ebd402", "class_name": "RelatedNodeInfo"}}, "text": "Failing to do this will yield inconsistent inference results.\r\nSaving and Loading Models with Shapes\u00b6\r\nWhen loading model weights, we needed to instantiate the model class first, because the class\r\ndefines the structure of a network. We might want to save the structure of this class together with\r\nthe model, in which case we can pass model (and not model.state_dict()) to the saving function:\r\ntorch.save(model, 'model.pth')\r\n\r\nWe can then load the model as demonstrated below.\r\nAs described in Saving and loading torch.nn.Modules,\r\nsaving state_dict is considered the best practice. However,\r\nbelow we use weights_only=False because this involves loading the\r\nmodel, which is a legacy use case for torch.save.\r\nmodel = torch.load('model.pth', weights_only=False),\r\n\r\nNote\r\nThis approach uses Python pickle module when serializing the model, thus it relies on the actual class definition to be available when loading the model.\r\nRelated Tutorials\u00b6\r\nSaving and Loading a General Checkpoint in PyTorch\r\nTips for loading an nn.Module from a checkpoint\r\nTotal running time of the script: ( 0 minutes  8.632 seconds)\r\nDownload Python source code: saveloadrun_tutorial.py\r\nDownload Jupyter notebook: saveloadrun_tutorial.ipynb\r\nGallery generated by Sphinx-Gallery \r\n\r\nsource: https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html \r\ncontent: \r\n\r\nPyTorch Custom Operators\u00b6\r\nPyTorch offers a large library of operators that work on Tensors (e.g. torch.add,\r\ntorch.sum, etc). However, you may wish to bring a new custom operation to PyTorch\r\nand get it to work with subsystems like torch.compile, autograd, and torch.vmap.\r\nIn order to do so, you must register the custom operation with PyTorch via the Python\r\ntorch.library docs or C++ TORCH_LIBRARY\r\nAPIs.\r\nAuthoring a custom operator from Python\u00b6\r\nPlease see Python Custom Operators.\r\nYou may wish to author a custom operator from Python (as opposed to C++) if:\r\nyou have a Python function you want PyTorch to treat as an opaque callable, especially with\r\nrespect to torch.compile and torch.export.\r\nyou have some Python bindings to C++/CUDA kernels and want those to compose with PyTorch\r\nsubsystems (like torch.compile or torch.autograd)\r\nIntegrating custom C++ and/or CUDA code with PyTorch\u00b6\r\nPlease see Custom C++ and CUDA Operators.\r\nYou may wish to author a custom operator from C++ (as opposed to Python) if:\r\nyou have custom C++ and/or CUDA code.\r\nyou plan to use this code with AOTInductor to do Python-less inference.\r\nThe Custom Operators Manual\u00b6\r\nFor information not covered in the tutorials and this page, please see\r\nThe Custom Operators Manual\r\n(we\u2019re working on moving the information to our docs site). We recommend that you\r\nfirst read one of the tutorials above and then use the Custom Operators Manual as a reference;\r\nit is not meant to be read head to toe.\r\nWhen should I create a Custom Operator?\u00b6\r\nIf your operation is expressible as a composition of built-in PyTorch operators\r\nthen please write it as a Python function and call it instead of creating a\r\ncustom operator. Use the operator registration APIs to create a custom operator if you\r\nare calling into some library that PyTorch doesn\u2019t understand (e.g. custom C/C++ code,\r\na custom CUDA kernel, or Python bindings to C/C++/CUDA extensions).\r\nWhy should I create a Custom Operator?\u00b6\r\nIt is possible to use a C/C++/CUDA kernel by grabbing a Tensor\u2019s data pointer\r\nand passing it to a pybind\u2019ed kernel. However, this approach doesn\u2019t compose with\r\nPyTorch subsystems like autograd, torch.compile, vmap, and more. In order\r\nfor an operation to compose with PyTorch subsystems, it must be registered\r\nvia the operator registration APIs.", "mimetype": "text/plain", "start_char_idx": 76797, "end_char_idx": 80470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"76a0bad1-36b2-4021-8ba3-6fd36e412a7f": {"node_ids": ["da00c709-a1a4-4c1e-b746-47d726fed0e5", "4b09eb42-e161-45a6-be04-d0879655fe5f", "94c83065-d2e4-444c-aee2-d9176406797a", "9f5f2c20-5a40-479b-8a46-170227efa56c", "27c23573-b9d9-498a-a230-757e93f4ad65", "945787bc-444e-4e08-93fe-4a72f912d1c8", "6baec8ba-aa4b-4b43-ac4f-4450fd9a152e", "f374d1ee-e8ac-4ac2-9779-0902435d1a1c", "1c0bc95d-4ffd-4a1f-b4e1-c29774c185d8", "ca4af97d-30a8-451c-a357-30b35e650e35", "8b4fd769-5690-43a0-96ed-7a17d7ea0d4e", "9984ef6f-7c69-4fa8-acaf-f5904b603339", "72ff2f24-dde8-4ba1-9c66-7cd48cd74ad3", "82a64506-04b9-4653-93fd-59a6f6c69eb7", "314d64ef-e47a-4f6c-b3b9-04656a1cc246", "04d552af-4b8f-46fe-bba4-976d1ea93144", "b7a3893f-6c5e-40e4-9814-1e6bf88fbd8a", "1f758777-5371-4f9d-b2c5-2547a58f897e", "f12ebdb1-c3b3-42ba-871f-a5f0cfb9c0b1", "a9260e27-55a2-4846-9b09-8e26ccdc3fb4", "82283394-fb3e-4fbb-ba98-74d379d90e38", "edbfae17-be4f-4f59-91ed-0180b0e414ee", "47dd6256-4647-4f38-a5e5-e49c6a9a7944", "a392b378-61b4-44b5-a9ea-92da83b583e1", "6ee5bf1a-6eae-42ef-9bbd-2cf4d097830f", "4d871b0f-0168-4be8-9a9e-d70a4ccc0cfb", "1c105476-5e5c-4004-9eea-ff9bdace056a", "bedc3f1e-39b4-45d6-8ea0-5709999a3328", "ceba251d-952b-4e73-a6b6-6f49da2bce9e", "9d8e219b-298e-4c23-9f35-6a911992d386", "9c4cfd30-e960-4441-ae38-acd0ebec4c6a", "cf1ab04c-4203-4ac1-81f6-35e0d148a3fa"], "metadata": {"file_path": "data\\raw\\tutorial_content.txt", "file_name": "tutorial_content.txt", "file_type": "text/plain", "file_size": 80622, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}}}}