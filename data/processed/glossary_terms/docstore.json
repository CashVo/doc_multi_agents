{"docstore/metadata": {"809b050f-d80a-4626-9c8a-33ac5bf3a720": {"doc_hash": "50ed7a0c37a10b9df9a0a15e97e1780682d41038ede2df586ba97362e21b2787"}, "2a1dc1bb-f9d5-45fc-9b74-6c577737edc7": {"doc_hash": "50ed7a0c37a10b9df9a0a15e97e1780682d41038ede2df586ba97362e21b2787", "ref_doc_id": "809b050f-d80a-4626-9c8a-33ac5bf3a720"}}, "docstore/data": {"2a1dc1bb-f9d5-45fc-9b74-6c577737edc7": {"__data__": {"id_": "2a1dc1bb-f9d5-45fc-9b74-6c577737edc7", "embedding": null, "metadata": {"file_path": "data\\raw\\glossary_terms.json", "file_name": "glossary_terms.json", "file_type": "application/json", "file_size": 3067, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "809b050f-d80a-4626-9c8a-33ac5bf3a720", "node_type": "4", "metadata": {"file_path": "data\\raw\\glossary_terms.json", "file_name": "glossary_terms.json", "file_type": "application/json", "file_size": 3067, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "50ed7a0c37a10b9df9a0a15e97e1780682d41038ede2df586ba97362e21b2787", "class_name": "RelatedNodeInfo"}}, "text": "{\"terms\": [{\"term\": \"Tensor\", \"definition\": \"The primary data structure in PyTorch, similar to NumPy arrays but with GPU support, for performing various numerical operations.\"}, {\"term\": \"Autograd\", \"definition\": \"A core PyTorch feature enabling automatic differentiation for building and training neural networks.\"}, {\"term\": \"Backpropagation\", \"definition\": \"An algorithm to compute gradients of loss with respect to model parameters, used in training deep learning models.\"}, {\"term\": \"Gradient Descent\", \"definition\": \"An optimization algorithm for minimizing the loss function by adjusting model parameters iteratively.\"}, {\"term\": \"Module\", \"definition\": \"A PyTorch class (`torch.nn.Module`) that serves as the base class for all neural network layers and models.\"}, {\"term\": \"Optimizer\", \"definition\": \"A PyTorch utility (`torch.optim`) that updates model parameters based on computed gradients, commonly using methods like SGD or Adam.\"}, {\"term\": \"Loss Function\", \"definition\": \"A function that calculates the difference between predicted and actual values, guiding the training process.\"}, {\"term\": \"DataLoader\", \"definition\": \"A PyTorch class (`torch.utils.data.DataLoader`) for loading and batching data during training and testing.\"}, {\"term\": \"Dataset\", \"definition\": \"An abstraction representing a dataset, often used with `DataLoader` to manage data for training or inference.\"}, {\"term\": \"Convolutional Neural Network (CNN)\", \"definition\": \"A neural network type optimized for image processing, using convolutional layers to learn spatial hierarchies.\"}, {\"term\": \"Recurrent Neural Network (RNN)\", \"definition\": \"A neural network type suited for sequential data like time series or language, using feedback loops to maintain information across inputs.\"}, {\"term\": \"Transfer Learning\", \"definition\": \"A technique to leverage a pre-trained model on a new but similar task, allowing for faster and more effective training.\"}, {\"term\": \"Batch Normalization\", \"definition\": \"A layer that normalizes inputs to a layer, improving model stability and accelerating training.\"}, {\"term\": \"Dropout\", \"definition\": \"A regularization technique that randomly 'drops' units during training to prevent overfitting.\"}, {\"term\": \"Activation Function\", \"definition\": \"A function applied to neurons to introduce non-linearity, such as ReLU, Sigmoid, or Tanh.\"}, {\"term\": \"Epoch\", \"definition\": \"One complete pass through the entire training dataset during model training.\"}, {\"term\": \"Batch\", \"definition\": \"A subset of the dataset processed at one time, allowing for more efficient computation and memory usage during training.\"}, {\"term\": \"Learning Rate\", \"definition\": \"A hyperparameter that controls the step size in gradient descent updates, affecting training speed and convergence.\"}, {\"term\": \"CUDA\", \"definition\": \"A parallel computing platform by NVIDIA, enabling PyTorch to perform computations on GPUs.\"}, {\"term\": \"Fine-Tuning\", \"definition\": \"A transfer learning method involving slight adjustments to a pre-trained model's parameters on a new dataset.\"}]}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3067, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"809b050f-d80a-4626-9c8a-33ac5bf3a720": {"node_ids": ["2a1dc1bb-f9d5-45fc-9b74-6c577737edc7"], "metadata": {"file_path": "data\\raw\\glossary_terms.json", "file_name": "glossary_terms.json", "file_type": "application/json", "file_size": 3067, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}}}}