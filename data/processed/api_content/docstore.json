{"docstore/metadata": {"ecd805b4-e6ab-42ec-90d2-1d1d05af982a": {"doc_hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd"}, "5ed5ad09-e30c-4058-8415-6f804f47e3a5": {"doc_hash": "da31b41f58baee00a9b4f4f2c35eee8124b6dab855811e6aad4821e887583d1a", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "7f5207e9-ddc6-4215-ab0f-d6c553dd9ebf": {"doc_hash": "c96350dc3f14e4ed62c2259523904b5ebae9b4606bf356589ed3a82070f8e810", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "457eab00-ae9f-4aac-a4e7-4a98408e91e1": {"doc_hash": "bb5dc4947a6f39c38df48a2700cd4084b56c9b7ca7fd34ef69d8347e316cf522", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "685ee7e0-d8c4-42c6-bb8c-19c1b3ab4395": {"doc_hash": "a545e29efa6a02c6abfef0f005cd62bebcb163ed321942fe89d8d44233918993", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "154af448-4dda-4800-8055-adcd44e2d197": {"doc_hash": "ef9f13f5ce270e3845e717e0cfec1cdc6731b7bc6b467b0f0ad24619a4287c23", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "e84af432-ebde-4b71-85f6-add7b8ae6593": {"doc_hash": "87e0705aa960b11a08278b0bdbe3723e84d51cd3e04def2d0fd32185e6db32e2", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "a0fda24d-c163-498d-987c-f1e996539c97": {"doc_hash": "2211e0c302093779956e8dac0a16c6b51ae8b644e32ac1444ac34f638244b6d7", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "47d4ec04-876d-4cf0-aa87-6b06343d8884": {"doc_hash": "9ffbd9ecaa58af24e29aeae5abbdef1e515e640050fe6c343fc7d29163e9f7bd", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "2a39ee67-88e3-4253-91e9-96233076bf73": {"doc_hash": "07a3244a707b40f822135806bf23d257663a55e465743944c968e1cc24b34ba7", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "688399e1-d7bc-48a3-8be2-881b291cd407": {"doc_hash": "d9bdbb308eb1ea930a9af502244bfe969654da97b3653fc0d142adbaa6e75566", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "a041d4ab-217e-449c-95ca-6f91b6317ff6": {"doc_hash": "ac06dd6c55c3c5bfe84b5b4f0c756f726263c2a14c06b79241927354b7253110", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "5c1f2183-5922-4fe0-804f-593e9c0d9d85": {"doc_hash": "5a910d1bd372ef6be8beba240195920fb8167ea8fec216d532efbed139cc8639", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "1d2edc3f-2dcb-4019-97c2-85b8dcb7cecd": {"doc_hash": "4f60a24ec77efb6d7db21fe37ad48f6fa13e50daef8e0d0d6197da5559423032", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "d203d24e-5c30-4c7a-82df-f3b314f76f11": {"doc_hash": "3dcd71a69145bb452d5635f030a5b2ca1e937ecd8f21a191cfd385b59990f7e8", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "32a73738-50d7-41de-a0f8-7a8cd117786e": {"doc_hash": "8efc7d61e203ce138df68c57ceec6ca7755262144df3209063fd0652075a85de", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "a294d79f-e526-4713-bfd6-e5e73f9e49b1": {"doc_hash": "05f5771ba6b83af7c280b7f7ab69e1e29b6c1496c39e6e0d4487fd84f7236ab8", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "1c5ab448-de9d-4498-b7fb-012b9d6262e5": {"doc_hash": "35686c0eadc369429517daf05a112b49c0e35e6bde8e43f15f5a2607c8197da9", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "086119a5-f3c9-41f9-a6de-d934cb12b77c": {"doc_hash": "694d31cd26b77d962b1019cb85ac6dd7774b90356d8fd5b958ed9d6fb46418ee", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "69b0579f-7229-4d74-ac02-e13e2087cd82": {"doc_hash": "8f73e6dd3d9660dab62cd996add619b474baa776c70d8b2b116cacf519a00a87", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "aac24df6-58b5-435a-9eb6-4b916c966e7d": {"doc_hash": "5262a8c60f5969e773bd30ea6421407cc25dd2942f9e5939d2606c465ae665e5", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "4c52c881-b5ce-4d79-9ea6-3f3d97441844": {"doc_hash": "36b98c48fcd0c209f46650e9ff942381d2cb32fd56d6d4e503f62f7a7e683903", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "55a675a8-2c34-4f9f-adfd-bbc8e5ad8fdd": {"doc_hash": "e022ebeab3acc5bf233a9a6e620bf7daa726f1af5a7328b3f2d83234044c01b3", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "421aaa4f-7ad1-47b4-9bff-f330df2dffd4": {"doc_hash": "ec11ecc219cb2a77ef14635d28c9ebe72eb27c944c29b2d014bbeae2a1e51f1d", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "ae9e09b0-7373-4230-9bb2-86a20ab091f1": {"doc_hash": "b8bf4c9520bf4ab5b1be0ef1c320f536eee1a15f1f3f25a25033152617097db4", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "0689df79-d968-47e4-85fc-532331bff96e": {"doc_hash": "ebc89226bdf482985423a897dde5922a47dbe9b84779c80249f0467b11424d7d", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "f5d4dfd2-32e3-4e93-8da2-97575adec23c": {"doc_hash": "20b5592b3fe23dd365d3a73684fdbd25590322db5a428857b6d71f13a4b7e0fd", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "a55c566e-ded2-44c7-b36e-061f82aad713": {"doc_hash": "978cf3b9276f4fc2780f6de6f32b9ce30b0ff5ed1cbc8ec05a0077b2663172a2", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "02cd274e-3074-4dac-a2a6-5c04d705c5ad": {"doc_hash": "e01c7b33e50c9612ee9ef690afcc7f47c58500a80e3b43a369d8bee082f0136e", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "827479cb-6872-498c-8cbf-61fa5c524d19": {"doc_hash": "e776c73de9bf9227b0a974bfcfa2da52d4fd824fdc84df65be0e67627aaa4cac", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "0de8490c-abed-4e40-9875-2542ddd06182": {"doc_hash": "275e03b928fe4ae4379225ea6231d38dbbf55817e6ab046bbf789e7bed50d017", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "ba8be7fd-b71f-4059-aa52-3370a5d560d3": {"doc_hash": "f92e3d4bf93d29872c54fcff06804cccaec96ebd0df2000d0df6c9c3b4fd8a40", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "f28292fb-4254-42b0-9560-78964ac5b5bd": {"doc_hash": "8c5b86ef75cb2ecd322d3991e6a2bb80ce74c03bf720f6b7db443ba0a1b744c8", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "e7c47131-e38c-4f01-aa72-92b29686df5c": {"doc_hash": "a2d73512eb1b39021e5dd08f1fe5d6086dc3f254324cc9e5136d2717d33f77b2", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "a55f34b5-bee4-485c-95c2-8f43a072c6f3": {"doc_hash": "81eee3a47a66b8ba79d7722df405e5aedb3ae13fd2930867913fe3dc79bd3f6f", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "d5492f8c-6c8d-4791-ada3-4cf81c8e038f": {"doc_hash": "b20fe77b3f2326c9ad8f7565637eac5bec859c29cc18addfb8cfa00a1ff77906", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "35d60436-502c-40ac-900b-3d46e992427b": {"doc_hash": "7c2ae497cc23b6e92f3eb9ed4c087d8f676bf8254432ec62b225fc61bd7483ce", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "86eb3f8e-3809-45cb-b9f0-b163198b8f72": {"doc_hash": "2cd09e50c40a8c7ef4344f0a942060abcd3d9538fde75b7e5f7a9de9b8eaa462", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "0578511b-a91a-4dac-bb2c-c2ce505cd162": {"doc_hash": "5db386b30b99516555681ba94f9f20ba238d805f7ffe47660d5940a65c9cb174", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "26c3f524-6755-477e-9cb9-4183fffdcb1b": {"doc_hash": "31a656e97d45384cd88a38efc04aa5e85e118ee11334629f4403e53baf388853", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "4792cdb9-9ff0-4132-ab23-000c4c0b0b9a": {"doc_hash": "f9949c51cfcddbfd38cc2eadeae1b65cfb9c4d841a02edc18af1a65d9a06c684", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "8abe0411-3437-46ec-9d4e-eae3466c8df5": {"doc_hash": "9ffc454f1cc64d92278c384ba7fb83a3c91a6e6e26162eba33f860bd00c51cec", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "9fa1f032-dec8-461c-8ff0-6dadce612a1f": {"doc_hash": "e8726dc17e34c04eca2cf2ed93da21b7396a5e01cedbc0a37b0845514a05236c", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "cac3f1f9-740b-42c1-a2b0-9632a8124bd0": {"doc_hash": "4f878d19a0cd03edea3901a8772d2e2c838961210fc40380bfdd3bfd1e98acd8", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "11c964a7-4218-47a2-9133-779c576bc2d7": {"doc_hash": "e46a7f43bd5bb3fe3af1c45887e8c12a54f959cdf41928c8ed6df87be9f722e3", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "94eebc7d-8f60-48dc-90c7-268222f56ee5": {"doc_hash": "be528d7de75e5cb29acb24a46331f0a9d6c1212d97f9c8b9ee4bbfba79705755", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "1410b72d-0a68-4708-814b-1e3869f653ea": {"doc_hash": "7dce296e3d6700cacb8ade55ed6640ab7d656a46a6cd043e8b1993eec05bc183", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "ec49ba9f-d67c-4366-8bc5-fc4dee8e8b31": {"doc_hash": "0f50031cc276d0b73fceff5c7cadc197be438f14eacd07d1170e106d3fe0f733", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "56471f04-f7d5-4c56-8e46-64c330e2b09c": {"doc_hash": "1524c1b46174598cd9679dc855a552d9bf5a1a13f1e521ffac8c542ad6126c2f", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "e8548960-ae21-4f44-a467-ab3a9e3485e9": {"doc_hash": "0bcf50ca3eedea4caebebccb8bbc585b110f6656a9b192f411e4bbd0b3a259b1", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "ec35cec0-59d3-4405-a9c3-34b684cd6e55": {"doc_hash": "ddb97d4f9f6d32e548c342ef6ad77367cb36888c5ef59c0a1843e9755002935c", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "ca5d608f-6a76-4cd8-931c-53a5bcdcb3d1": {"doc_hash": "d86feecb51909a26ddafd197d638a46fad4f4cc8c394438169edf920bed23275", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "f8175003-6a75-44d6-94e4-b54df2003d00": {"doc_hash": "92b58a03e372010fbd487dfc509f80eb20d27714b03ada5a6d1944202f628543", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "09181249-7d8c-4c6a-a831-97cb7b3b9f73": {"doc_hash": "0c41b2d173c84dbf774f9b602f54d7134153740420f3f54670053c738979b42e", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "914a3b82-b51f-49fd-a9a0-a1e5885b5455": {"doc_hash": "c5ce4a48e76ff062f8bc7793bb2a23e284958e1c259eb51e3b9458c532084c47", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "f9fc2bbb-8c32-4c30-bbb9-5887ac0ee6d9": {"doc_hash": "6c7fa566cfa63122310040afb6442c734a0eb55dbea20329ef307daf61aaba1a", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}, "92cb0d67-d62e-4b38-98df-eeb64760146f": {"doc_hash": "8884632f7c53e566963cdf1b8b685d5daed5f64844446f73b2a5cc1805377113", "ref_doc_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a"}}, "docstore/data": {"5ed5ad09-e30c-4058-8415-6f804f47e3a5": {"__data__": {"id_": "5ed5ad09-e30c-4058-8415-6f804f47e3a5", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f5207e9-ddc6-4215-ab0f-d6c553dd9ebf", "node_type": "1", "metadata": {}, "hash": "dd3260e702be65fec1b94fc6c35ab7f165fc241f867c26a560508cf93571af22", "class_name": "RelatedNodeInfo"}}, "text": "source: https://pytorch.org/text/stable/nn_modules.html \r\ncontent: \r\n\r\ntorchtext.nn\u00b6\r\nMultiheadAttentionContainer\u00b6\r\n\r\nclass torchtext.nn.MultiheadAttentionContainer(nhead, in_proj_container, attention_layer, out_proj, batch_first=False)[source]\u00b6\r\n\r\n\r\n__init__(nhead, in_proj_container, attention_layer, out_proj, batch_first=False) \u2192 None[source]\u00b6\r\nA multi-head attention container\r\n\r\nParameters:\r\n\r\nnhead \u2013 the number of heads in the multiheadattention model\r\nin_proj_container \u2013 A container of multi-head in-projection linear layers (a.k.a nn.Linear).\r\nattention_layer \u2013 The custom attention layer. The input sent from MHA container to the attention layer\r\nis in the shape of (\u2026, L, N * H, E / H) for query and (\u2026, S, N * H, E / H) for key/value\r\nwhile the  output shape of the attention layer is expected to be (\u2026, L, N * H, E / H).\r\nThe attention_layer needs to support broadcast if users want the overall MultiheadAttentionContainer\r\nwith broadcast.\r\nout_proj \u2013 The multi-head out-projection layer (a.k.a nn.Linear).\r\nbatch_first \u2013 If True, then the input and output tensors are provided\r\nas (\u2026, N, L, E). Default: False\r\n\r\n\r\n\r\n\r\nExamples::>>> import torch\r\n>>> from torchtext.nn import MultiheadAttentionContainer, InProjContainer, ScaledDotProduct\r\n>>> embed_dim, num_heads, bsz = 10, 5, 64\r\n>>> in_proj_container = InProjContainer(torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim))\r\n>>> MHA = MultiheadAttentionContainer(num_heads,\r\n                                      in_proj_container,\r\n                                      ScaledDotProduct(),\r\n                                      torch.nn.Linear(embed_dim, embed_dim))\r\n>>> query = torch.rand((21, bsz, embed_dim))\r\n>>> key = value = torch.rand((16, bsz, embed_dim))\r\n>>> attn_output, attn_weights = MHA(query, key, value)\r\n>>> print(attn_output.shape)\r\n>>> torch.Size([21, 64, 10])\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nforward(query: Tensor, key: Tensor, value: Tensor, attn_mask: Optional[Tensor] = None, bias_k: Optional[Tensor] = None, bias_v: Optional[Tensor] = None) \u2192 Tuple[Tensor, Tensor][source]\u00b6\r\n\r\nParameters:\r\n\r\nquery (Tensor) \u2013 The query of the attention function.\r\nSee \u201cAttention Is All You Need\u201d for more details.\r\nkey (Tensor) \u2013 The keys of the attention function.\r\nSee \u201cAttention Is All You Need\u201d for more details.\r\nvalue (Tensor) \u2013 The values of the attention function.\r\nSee \u201cAttention Is All You Need\u201d for more details.\r\nattn_mask (BoolTensor, optional) \u2013 3D mask that prevents attention to certain positions.\r\nbias_k (Tensor, optional) \u2013 one more key and value sequence to be added to keys at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should provide\r\nbias_v.\r\nbias_v (Tensor, optional) \u2013 one more key and value sequence to be added to values at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should also provide\r\nbias_k.\r\n\r\n\r\n\r\nShape:\r\n\r\n\r\nInputs:\r\n\r\n\r\nquery: \\((..., L, N, E)\\)\r\nkey: \\((..., S, N, E)\\)\r\nvalue: \\((..., S, N, E)\\)\r\nattn_mask, bias_k and bias_v: same with the shape of the corresponding args in attention layer.\r\n\r\n\r\n\r\nOutputs:\r\n\r\n\r\nattn_output: \\((..., L, N, E)\\)\r\nattn_output_weights: \\((N * H, L, S)\\)\r\n\r\n\r\n\r\n\r\nNote: It\u2019s optional to have the query/key/value inputs with more than three dimensions (for broadcast purpose).\r\nThe MultiheadAttentionContainer module will operate on the last three dimensions.\r\nwhere where L is the target length, S is the sequence length, H is the number of attention heads,\r\nN is the batch size, and E is the embedding dimension.\r\n\r\n\r\n\r\n\r\n__init__(nhead, in_proj_container, attention_layer, out_proj, batch_first=False) \u2192 None[source]\u00b6\r\nA multi-head attention container\r\n\r\nParameters:\r\n\r\nnhead \u2013 the number of heads in the multiheadattention model\r\nin_proj_container \u2013 A container of multi-head in-projection linear layers (a.k.a nn.Linear).\r\nattention_layer \u2013 The custom attention layer.", "mimetype": "text/plain", "start_char_idx": 5, "end_char_idx": 4008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f5207e9-ddc6-4215-ab0f-d6c553dd9ebf": {"__data__": {"id_": "7f5207e9-ddc6-4215-ab0f-d6c553dd9ebf", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ed5ad09-e30c-4058-8415-6f804f47e3a5", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "da31b41f58baee00a9b4f4f2c35eee8124b6dab855811e6aad4821e887583d1a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "457eab00-ae9f-4aac-a4e7-4a98408e91e1", "node_type": "1", "metadata": {}, "hash": "4e9e437e960a714c7ce01f818e3b7b58305fd9678e9bfda43df9da7748a1b9a4", "class_name": "RelatedNodeInfo"}}, "text": "Outputs:\r\n\r\n\r\nattn_output: \\((..., L, N, E)\\)\r\nattn_output_weights: \\((N * H, L, S)\\)\r\n\r\n\r\n\r\n\r\nNote: It\u2019s optional to have the query/key/value inputs with more than three dimensions (for broadcast purpose).\r\nThe MultiheadAttentionContainer module will operate on the last three dimensions.\r\nwhere where L is the target length, S is the sequence length, H is the number of attention heads,\r\nN is the batch size, and E is the embedding dimension.\r\n\r\n\r\n\r\n\r\n__init__(nhead, in_proj_container, attention_layer, out_proj, batch_first=False) \u2192 None[source]\u00b6\r\nA multi-head attention container\r\n\r\nParameters:\r\n\r\nnhead \u2013 the number of heads in the multiheadattention model\r\nin_proj_container \u2013 A container of multi-head in-projection linear layers (a.k.a nn.Linear).\r\nattention_layer \u2013 The custom attention layer. The input sent from MHA container to the attention layer\r\nis in the shape of (\u2026, L, N * H, E / H) for query and (\u2026, S, N * H, E / H) for key/value\r\nwhile the  output shape of the attention layer is expected to be (\u2026, L, N * H, E / H).\r\nThe attention_layer needs to support broadcast if users want the overall MultiheadAttentionContainer\r\nwith broadcast.\r\nout_proj \u2013 The multi-head out-projection layer (a.k.a nn.Linear).\r\nbatch_first \u2013 If True, then the input and output tensors are provided\r\nas (\u2026, N, L, E). Default: False\r\n\r\n\r\n\r\n\r\nExamples::>>> import torch\r\n>>> from torchtext.nn import MultiheadAttentionContainer, InProjContainer, ScaledDotProduct\r\n>>> embed_dim, num_heads, bsz = 10, 5, 64\r\n>>> in_proj_container = InProjContainer(torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim))\r\n>>> MHA = MultiheadAttentionContainer(num_heads,\r\n                                      in_proj_container,\r\n                                      ScaledDotProduct(),\r\n                                      torch.nn.Linear(embed_dim, embed_dim))\r\n>>> query = torch.rand((21, bsz, embed_dim))\r\n>>> key = value = torch.rand((16, bsz, embed_dim))\r\n>>> attn_output, attn_weights = MHA(query, key, value)\r\n>>> print(attn_output.shape)\r\n>>> torch.Size([21, 64, 10])\r\n\r\n\r\n\r\n\r\n\r\nA multi-head attention container\r\nParameters:\r\n\r\nnhead \u2013 the number of heads in the multiheadattention model\r\nin_proj_container \u2013 A container of multi-head in-projection linear layers (a.k.a nn.Linear).\r\nattention_layer \u2013 The custom attention layer. The input sent from MHA container to the attention layer\r\nis in the shape of (\u2026, L, N * H, E / H) for query and (\u2026, S, N * H, E / H) for key/value\r\nwhile the  output shape of the attention layer is expected to be (\u2026, L, N * H, E / H).\r\nThe attention_layer needs to support broadcast if users want the overall MultiheadAttentionContainer\r\nwith broadcast.\r\nout_proj \u2013 The multi-head out-projection layer (a.k.a nn.Linear).\r\nbatch_first \u2013 If True, then the input and output tensors are provided\r\nas (\u2026, N, L, E). Default: False\r\n\r\n\r\nnhead \u2013 the number of heads in the multiheadattention model\r\nin_proj_container \u2013 A container of multi-head in-projection linear layers (a.k.a nn.Linear).\r\nattention_layer \u2013 The custom attention layer. The input sent from MHA container to the attention layer\r\nis in the shape of (\u2026, L, N * H, E / H) for query and (\u2026, S, N * H, E / H) for key/value\r\nwhile the  output shape of the attention layer is expected to be (\u2026, L, N * H, E / H).\r\nThe attention_layer needs to support broadcast if users want the overall MultiheadAttentionContainer\r\nwith broadcast.\r\nout_proj \u2013 The multi-head out-projection layer (a.k.a nn.Linear).\r\nbatch_first \u2013 If True, then the input and output tensors are provided\r\nas (\u2026, N, L, E).", "mimetype": "text/plain", "start_char_idx": 3205, "end_char_idx": 6906, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "457eab00-ae9f-4aac-a4e7-4a98408e91e1": {"__data__": {"id_": "457eab00-ae9f-4aac-a4e7-4a98408e91e1", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f5207e9-ddc6-4215-ab0f-d6c553dd9ebf", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "c96350dc3f14e4ed62c2259523904b5ebae9b4606bf356589ed3a82070f8e810", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "685ee7e0-d8c4-42c6-bb8c-19c1b3ab4395", "node_type": "1", "metadata": {}, "hash": "9d22e067a6ced08db97e662786e74e475b13546dce12428d18f5cb2af844241f", "class_name": "RelatedNodeInfo"}}, "text": "Default: False\r\n\r\n\r\nnhead \u2013 the number of heads in the multiheadattention model\r\nin_proj_container \u2013 A container of multi-head in-projection linear layers (a.k.a nn.Linear).\r\nattention_layer \u2013 The custom attention layer. The input sent from MHA container to the attention layer\r\nis in the shape of (\u2026, L, N * H, E / H) for query and (\u2026, S, N * H, E / H) for key/value\r\nwhile the  output shape of the attention layer is expected to be (\u2026, L, N * H, E / H).\r\nThe attention_layer needs to support broadcast if users want the overall MultiheadAttentionContainer\r\nwith broadcast.\r\nout_proj \u2013 The multi-head out-projection layer (a.k.a nn.Linear).\r\nbatch_first \u2013 If True, then the input and output tensors are provided\r\nas (\u2026, N, L, E). Default: False\r\nExamples::\r\n>>> import torch\r\n>>> from torchtext.nn import MultiheadAttentionContainer, InProjContainer, ScaledDotProduct\r\n>>> embed_dim, num_heads, bsz = 10, 5, 64\r\n>>> in_proj_container = InProjContainer(torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim))\r\n>>> MHA = MultiheadAttentionContainer(num_heads,\r\n                                      in_proj_container,\r\n                                      ScaledDotProduct(),\r\n                                      torch.nn.Linear(embed_dim, embed_dim))\r\n>>> query = torch.rand((21, bsz, embed_dim))\r\n>>> key = value = torch.rand((16, bsz, embed_dim))\r\n>>> attn_output, attn_weights = MHA(query, key, value)\r\n>>> print(attn_output.shape)\r\n>>> torch.Size([21, 64, 10])\r\n\r\n\r\n\r\n>>> import torch\r\n>>> from torchtext.nn import MultiheadAttentionContainer, InProjContainer, ScaledDotProduct\r\n>>> embed_dim, num_heads, bsz = 10, 5, 64\r\n>>> in_proj_container = InProjContainer(torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim))\r\n>>> MHA = MultiheadAttentionContainer(num_heads,\r\n                                      in_proj_container,\r\n                                      ScaledDotProduct(),\r\n                                      torch.nn.Linear(embed_dim, embed_dim))\r\n>>> query = torch.rand((21, bsz, embed_dim))\r\n>>> key = value = torch.rand((16, bsz, embed_dim))\r\n>>> attn_output, attn_weights = MHA(query, key, value)\r\n>>> print(attn_output.shape)\r\n>>> torch.Size([21, 64, 10])\r\n\r\n\r\nforward(query: Tensor, key: Tensor, value: Tensor, attn_mask: Optional[Tensor] = None, bias_k: Optional[Tensor] = None, bias_v: Optional[Tensor] = None) \u2192 Tuple[Tensor, Tensor][source]\u00b6\r\n\r\nParameters:\r\n\r\nquery (Tensor) \u2013 The query of the attention function.\r\nSee \u201cAttention Is All You Need\u201d for more details.\r\nkey (Tensor) \u2013 The keys of the attention function.\r\nSee \u201cAttention Is All You Need\u201d for more details.\r\nvalue (Tensor) \u2013 The values of the attention function.\r\nSee \u201cAttention Is All You Need\u201d for more details.\r\nattn_mask (BoolTensor, optional) \u2013 3D mask that prevents attention to certain positions.\r\nbias_k (Tensor, optional) \u2013 one more key and value sequence to be added to keys at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should provide\r\nbias_v.\r\nbias_v (Tensor, optional) \u2013 one more key and value sequence to be added to values at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should also provide\r\nbias_k.\r\n\r\n\r\n\r\nShape:\r\n\r\n\r\nInputs:\r\n\r\n\r\nquery: \\((..., L, N, E)\\)\r\nkey: \\((..., S, N, E)\\)\r\nvalue: \\((..., S, N, E)\\)\r\nattn_mask, bias_k and bias_v: same with the shape of the corresponding args in attention layer.\r\n\r\n\r\n\r\nOutputs:\r\n\r\n\r\nattn_output: \\((..., L, N, E)\\)\r\nattn_output_weights: \\((N * H, L, S)\\)\r\n\r\n\r\n\r\n\r\nNote: It\u2019s optional to have the query/key/value inputs with more than three dimensions (for broadcast purpose).\r\nThe MultiheadAttentionContainer module will operate on the last three dimensions.\r\nwhere where L is the target length, S is the sequence length, H is the number of attention heads,\r\nN is the batch size, and E is the embedding dimension.\r\n\r\n\r\nParameters:\r\n\r\nquery (Tensor) \u2013 The query of the attention function.\r\nSee \u201cAttention Is All You Need\u201d for more details.\r\nkey (Tensor) \u2013 The keys of the attention function.\r\nSee \u201cAttention Is All You Need\u201d for more details.", "mimetype": "text/plain", "start_char_idx": 6176, "end_char_idx": 10511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "685ee7e0-d8c4-42c6-bb8c-19c1b3ab4395": {"__data__": {"id_": "685ee7e0-d8c4-42c6-bb8c-19c1b3ab4395", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "457eab00-ae9f-4aac-a4e7-4a98408e91e1", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "bb5dc4947a6f39c38df48a2700cd4084b56c9b7ca7fd34ef69d8347e316cf522", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "154af448-4dda-4800-8055-adcd44e2d197", "node_type": "1", "metadata": {}, "hash": "9b720adff382b2452a1500d328a8c1e2684feb707ad42fa9de117a6a5e6fda94", "class_name": "RelatedNodeInfo"}}, "text": "Outputs:\r\n\r\n\r\nattn_output: \\((..., L, N, E)\\)\r\nattn_output_weights: \\((N * H, L, S)\\)\r\n\r\n\r\n\r\n\r\nNote: It\u2019s optional to have the query/key/value inputs with more than three dimensions (for broadcast purpose).\r\nThe MultiheadAttentionContainer module will operate on the last three dimensions.\r\nwhere where L is the target length, S is the sequence length, H is the number of attention heads,\r\nN is the batch size, and E is the embedding dimension.\r\n\r\n\r\nParameters:\r\n\r\nquery (Tensor) \u2013 The query of the attention function.\r\nSee \u201cAttention Is All You Need\u201d for more details.\r\nkey (Tensor) \u2013 The keys of the attention function.\r\nSee \u201cAttention Is All You Need\u201d for more details.\r\nvalue (Tensor) \u2013 The values of the attention function.\r\nSee \u201cAttention Is All You Need\u201d for more details.\r\nattn_mask (BoolTensor, optional) \u2013 3D mask that prevents attention to certain positions.\r\nbias_k (Tensor, optional) \u2013 one more key and value sequence to be added to keys at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should provide\r\nbias_v.\r\nbias_v (Tensor, optional) \u2013 one more key and value sequence to be added to values at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should also provide\r\nbias_k.\r\n\r\n\r\nquery (Tensor) \u2013 The query of the attention function.\r\nSee \u201cAttention Is All You Need\u201d for more details.\r\nkey (Tensor) \u2013 The keys of the attention function.\r\nSee \u201cAttention Is All You Need\u201d for more details.\r\nvalue (Tensor) \u2013 The values of the attention function.\r\nSee \u201cAttention Is All You Need\u201d for more details.\r\nattn_mask (BoolTensor, optional) \u2013 3D mask that prevents attention to certain positions.\r\nbias_k (Tensor, optional) \u2013 one more key and value sequence to be added to keys at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should provide\r\nbias_v.\r\nbias_v (Tensor, optional) \u2013 one more key and value sequence to be added to values at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should also provide\r\nbias_k.\r\nShape:\r\nInputs:\r\nquery: \\((..., L, N, E)\\)\r\nkey: \\((..., S, N, E)\\)\r\nvalue: \\((..., S, N, E)\\)\r\nattn_mask, bias_k and bias_v: same with the shape of the corresponding args in attention layer.\r\nOutputs:\r\nattn_output: \\((..., L, N, E)\\)\r\nattn_output_weights: \\((N * H, L, S)\\)\r\nNote: It\u2019s optional to have the query/key/value inputs with more than three dimensions (for broadcast purpose).\r\nThe MultiheadAttentionContainer module will operate on the last three dimensions.\r\nwhere where L is the target length, S is the sequence length, H is the number of attention heads,\r\nN is the batch size, and E is the embedding dimension.\r\nInProjContainer\u00b6\r\n\r\nclass torchtext.nn.InProjContainer(query_proj, key_proj, value_proj)[source]\u00b6\r\n\r\n\r\n__init__(query_proj, key_proj, value_proj) \u2192 None[source]\u00b6\r\nA in-proj container to project query/key/value in MultiheadAttention. This module happens before reshaping\r\nthe projected query/key/value into multiple heads. See the linear layers (bottom) of Multi-head Attention in\r\nFig 2 of Attention Is All You Need paper. Also check the usage example\r\nin torchtext.nn.MultiheadAttentionContainer.\r\n\r\nParameters:\r\n\r\nquery_proj \u2013 a proj layer for query. A typical projection layer is torch.nn.Linear.\r\nkey_proj \u2013 a proj layer for key. A typical projection layer is torch.nn.Linear.\r\nvalue_proj \u2013 a proj layer for value. A typical projection layer is torch.nn.Linear.\r\n\r\n\r\n\r\n\r\n\r\n\r\nforward(query: Tensor, key: Tensor, value: Tensor) \u2192 Tuple[Tensor, Tensor, Tensor][source]\u00b6\r\nProjects the input sequences using in-proj layers. query/key/value are simply passed to\r\nthe forward func of query/key/value_proj, respectively.\r\n\r\nParameters:\r\n\r\nquery (Tensor) \u2013 The query to be projected.\r\nkey (Tensor) \u2013 The keys to be projected.\r\nvalue (Tensor) \u2013 The values to be projected.", "mimetype": "text/plain", "start_char_idx": 9839, "end_char_idx": 13643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "154af448-4dda-4800-8055-adcd44e2d197": {"__data__": {"id_": "154af448-4dda-4800-8055-adcd44e2d197", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "685ee7e0-d8c4-42c6-bb8c-19c1b3ab4395", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "a545e29efa6a02c6abfef0f005cd62bebcb163ed321942fe89d8d44233918993", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e84af432-ebde-4b71-85f6-add7b8ae6593", "node_type": "1", "metadata": {}, "hash": "f9eb741482765d8a9da6e6a28694634a6226c5b9974115730c9a981082993a6d", "class_name": "RelatedNodeInfo"}}, "text": "See the linear layers (bottom) of Multi-head Attention in\r\nFig 2 of Attention Is All You Need paper. Also check the usage example\r\nin torchtext.nn.MultiheadAttentionContainer.\r\n\r\nParameters:\r\n\r\nquery_proj \u2013 a proj layer for query. A typical projection layer is torch.nn.Linear.\r\nkey_proj \u2013 a proj layer for key. A typical projection layer is torch.nn.Linear.\r\nvalue_proj \u2013 a proj layer for value. A typical projection layer is torch.nn.Linear.\r\n\r\n\r\n\r\n\r\n\r\n\r\nforward(query: Tensor, key: Tensor, value: Tensor) \u2192 Tuple[Tensor, Tensor, Tensor][source]\u00b6\r\nProjects the input sequences using in-proj layers. query/key/value are simply passed to\r\nthe forward func of query/key/value_proj, respectively.\r\n\r\nParameters:\r\n\r\nquery (Tensor) \u2013 The query to be projected.\r\nkey (Tensor) \u2013 The keys to be projected.\r\nvalue (Tensor) \u2013 The values to be projected.\r\n\r\n\r\n\r\n\r\nExamples::>>> import torch\r\n>>> from torchtext.nn import InProjContainer\r\n>>> embed_dim, bsz = 10, 64\r\n>>> in_proj_container = InProjContainer(torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim))\r\n>>> q = torch.rand((5, bsz, embed_dim))\r\n>>> k = v = torch.rand((6, bsz, embed_dim))\r\n>>> q, k, v = in_proj_container(q, k, v)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n__init__(query_proj, key_proj, value_proj) \u2192 None[source]\u00b6\r\nA in-proj container to project query/key/value in MultiheadAttention. This module happens before reshaping\r\nthe projected query/key/value into multiple heads. See the linear layers (bottom) of Multi-head Attention in\r\nFig 2 of Attention Is All You Need paper. Also check the usage example\r\nin torchtext.nn.MultiheadAttentionContainer.\r\n\r\nParameters:\r\n\r\nquery_proj \u2013 a proj layer for query. A typical projection layer is torch.nn.Linear.\r\nkey_proj \u2013 a proj layer for key. A typical projection layer is torch.nn.Linear.\r\nvalue_proj \u2013 a proj layer for value. A typical projection layer is torch.nn.Linear.\r\n\r\n\r\n\r\n\r\nA in-proj container to project query/key/value in MultiheadAttention. This module happens before reshaping\r\nthe projected query/key/value into multiple heads. See the linear layers (bottom) of Multi-head Attention in\r\nFig 2 of Attention Is All You Need paper. Also check the usage example\r\nin torchtext.nn.MultiheadAttentionContainer.\r\nParameters:\r\n\r\nquery_proj \u2013 a proj layer for query. A typical projection layer is torch.nn.Linear.\r\nkey_proj \u2013 a proj layer for key. A typical projection layer is torch.nn.Linear.\r\nvalue_proj \u2013 a proj layer for value. A typical projection layer is torch.nn.Linear.\r\n\r\n\r\nquery_proj \u2013 a proj layer for query. A typical projection layer is torch.nn.Linear.\r\nkey_proj \u2013 a proj layer for key. A typical projection layer is torch.nn.Linear.\r\nvalue_proj \u2013 a proj layer for value. A typical projection layer is torch.nn.Linear.\r\n\r\nforward(query: Tensor, key: Tensor, value: Tensor) \u2192 Tuple[Tensor, Tensor, Tensor][source]\u00b6\r\nProjects the input sequences using in-proj layers. query/key/value are simply passed to\r\nthe forward func of query/key/value_proj, respectively.\r\n\r\nParameters:\r\n\r\nquery (Tensor) \u2013 The query to be projected.\r\nkey (Tensor) \u2013 The keys to be projected.\r\nvalue (Tensor) \u2013 The values to be projected.\r\n\r\n\r\n\r\n\r\nExamples::>>> import torch\r\n>>> from torchtext.nn import InProjContainer\r\n>>> embed_dim, bsz = 10, 64\r\n>>> in_proj_container = InProjContainer(torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim))\r\n>>> q = torch.rand((5, bsz, embed_dim))\r\n>>> k = v = torch.rand((6, bsz, embed_dim))\r\n>>> q, k, v = in_proj_container(q, k, v)\r\n\r\n\r\n\r\n\r\n\r\nProjects the input sequences using in-proj layers. query/key/value are simply passed to\r\nthe forward func of query/key/value_proj, respectively.\r\nParameters:\r\n\r\nquery (Tensor) \u2013 The query to be projected.\r\nkey (Tensor) \u2013 The keys to be projected.\r\nvalue (Tensor) \u2013 The values to be projected.\r\n\r\n\r\nquery (Tensor) \u2013 The query to be projected.\r\nkey (Tensor) \u2013 The keys to be projected.\r\nvalue (Tensor) \u2013 The values to be projected.", "mimetype": "text/plain", "start_char_idx": 12799, "end_char_idx": 16976, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e84af432-ebde-4b71-85f6-add7b8ae6593": {"__data__": {"id_": "e84af432-ebde-4b71-85f6-add7b8ae6593", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "154af448-4dda-4800-8055-adcd44e2d197", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "ef9f13f5ce270e3845e717e0cfec1cdc6731b7bc6b467b0f0ad24619a4287c23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0fda24d-c163-498d-987c-f1e996539c97", "node_type": "1", "metadata": {}, "hash": "ae48c75ed9592c5520fb2c7a5d9b6e82023aae306a3eca5aa3ad36d20e88a1f3", "class_name": "RelatedNodeInfo"}}, "text": "query/key/value are simply passed to\r\nthe forward func of query/key/value_proj, respectively.\r\nParameters:\r\n\r\nquery (Tensor) \u2013 The query to be projected.\r\nkey (Tensor) \u2013 The keys to be projected.\r\nvalue (Tensor) \u2013 The values to be projected.\r\n\r\n\r\nquery (Tensor) \u2013 The query to be projected.\r\nkey (Tensor) \u2013 The keys to be projected.\r\nvalue (Tensor) \u2013 The values to be projected.\r\nExamples::\r\n>>> import torch\r\n>>> from torchtext.nn import InProjContainer\r\n>>> embed_dim, bsz = 10, 64\r\n>>> in_proj_container = InProjContainer(torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim))\r\n>>> q = torch.rand((5, bsz, embed_dim))\r\n>>> k = v = torch.rand((6, bsz, embed_dim))\r\n>>> q, k, v = in_proj_container(q, k, v)\r\n\r\n\r\n\r\n>>> import torch\r\n>>> from torchtext.nn import InProjContainer\r\n>>> embed_dim, bsz = 10, 64\r\n>>> in_proj_container = InProjContainer(torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim),\r\n                                        torch.nn.Linear(embed_dim, embed_dim))\r\n>>> q = torch.rand((5, bsz, embed_dim))\r\n>>> k = v = torch.rand((6, bsz, embed_dim))\r\n>>> q, k, v = in_proj_container(q, k, v)\r\n\r\nScaledDotProduct\u00b6\r\n\r\nclass torchtext.nn.ScaledDotProduct(dropout=0.0, batch_first=False)[source]\u00b6\r\n\r\n\r\n__init__(dropout=0.0, batch_first=False) \u2192 None[source]\u00b6\r\nProcesses a projected query and key-value pair to apply\r\nscaled dot product attention.\r\n\r\nParameters:\r\n\r\ndropout (float) \u2013 probability of dropping an attention weight.\r\nbatch_first \u2013 If True, then the input and output tensors are provided\r\nas (batch, seq, feature). Default: False\r\n\r\n\r\n\r\n\r\nExamples::>>> import torch, torchtext\r\n>>> SDP = torchtext.nn.ScaledDotProduct(dropout=0.1)\r\n>>> q = torch.randn(21, 256, 3)\r\n>>> k = v = torch.randn(21, 256, 3)\r\n>>> attn_output, attn_weights = SDP(q, k, v)\r\n>>> print(attn_output.shape, attn_weights.shape)\r\ntorch.Size([21, 256, 3]) torch.Size([256, 21, 21])\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nforward(query: Tensor, key: Tensor, value: Tensor, attn_mask: Optional[Tensor] = None, bias_k: Optional[Tensor] = None, bias_v: Optional[Tensor] = None) \u2192 Tuple[Tensor, Tensor][source]\u00b6\r\nUses a scaled dot product with the projected key-value pair to update\r\nthe projected query.\r\n\r\nParameters:\r\n\r\nquery (Tensor) \u2013 Projected query\r\nkey (Tensor) \u2013 Projected key\r\nvalue (Tensor) \u2013 Projected value\r\nattn_mask (BoolTensor, optional) \u2013 3D mask that prevents attention to certain positions.\r\nattn_mask \u2013 3D mask that prevents attention to certain positions.\r\nbias_k (Tensor, optional) \u2013 one more key and value sequence to be added to keys at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should provide\r\nbias_v.\r\nbias_v (Tensor, optional) \u2013 one more key and value sequence to be added to values at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should also provide\r\nbias_k.\r\n\r\n\r\n\r\n\r\nShape:\r\nquery: \\((..., L, N * H, E / H)\\)\r\nkey: \\((..., S, N * H, E / H)\\)\r\nvalue: \\((..., S, N * H, E / H)\\)\r\n\r\nattn_mask: \\((N * H, L, S)\\), positions with True are not allowed to attendwhile False values will be unchanged.\r\n\r\n\r\n\r\nbias_k and bias_v:bias: \\((1, N * H, E / H)\\)\r\nOutput: \\((..., L, N * H, E / H)\\), \\((N * H, L, S)\\)\r\n\r\n\r\nNote: It\u2019s optional to have the query/key/value inputs with more than three dimensions (for broadcast purpose).The ScaledDotProduct module will operate on the last three dimensions.\r\n\r\n\r\nwhere L is the target length, S is the source length, H is the number\r\nof attention heads, N is the batch size, and E is the embedding dimension.\r\n\r\n\r\n\r\n\r\n\r\n__init__(dropout=0.0, batch_first=False) \u2192 None[source]\u00b6\r\nProcesses a projected query and key-value pair to apply\r\nscaled dot product attention.\r\n\r\nParameters:\r\n\r\ndropout (float) \u2013 probability of dropping an attention weight.", "mimetype": "text/plain", "start_char_idx": 16598, "end_char_idx": 20529, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0fda24d-c163-498d-987c-f1e996539c97": {"__data__": {"id_": "a0fda24d-c163-498d-987c-f1e996539c97", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e84af432-ebde-4b71-85f6-add7b8ae6593", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "87e0705aa960b11a08278b0bdbe3723e84d51cd3e04def2d0fd32185e6db32e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47d4ec04-876d-4cf0-aa87-6b06343d8884", "node_type": "1", "metadata": {}, "hash": "3935288e431ebe478ab578b17e398c03fdca3baa94c9cbbe348b695a560bd6fd", "class_name": "RelatedNodeInfo"}}, "text": "bias_k and bias_v:bias: \\((1, N * H, E / H)\\)\r\nOutput: \\((..., L, N * H, E / H)\\), \\((N * H, L, S)\\)\r\n\r\n\r\nNote: It\u2019s optional to have the query/key/value inputs with more than three dimensions (for broadcast purpose).The ScaledDotProduct module will operate on the last three dimensions.\r\n\r\n\r\nwhere L is the target length, S is the source length, H is the number\r\nof attention heads, N is the batch size, and E is the embedding dimension.\r\n\r\n\r\n\r\n\r\n\r\n__init__(dropout=0.0, batch_first=False) \u2192 None[source]\u00b6\r\nProcesses a projected query and key-value pair to apply\r\nscaled dot product attention.\r\n\r\nParameters:\r\n\r\ndropout (float) \u2013 probability of dropping an attention weight.\r\nbatch_first \u2013 If True, then the input and output tensors are provided\r\nas (batch, seq, feature). Default: False\r\n\r\n\r\n\r\n\r\nExamples::>>> import torch, torchtext\r\n>>> SDP = torchtext.nn.ScaledDotProduct(dropout=0.1)\r\n>>> q = torch.randn(21, 256, 3)\r\n>>> k = v = torch.randn(21, 256, 3)\r\n>>> attn_output, attn_weights = SDP(q, k, v)\r\n>>> print(attn_output.shape, attn_weights.shape)\r\ntorch.Size([21, 256, 3]) torch.Size([256, 21, 21])\r\n\r\n\r\n\r\n\r\n\r\nProcesses a projected query and key-value pair to apply\r\nscaled dot product attention.\r\nParameters:\r\n\r\ndropout (float) \u2013 probability of dropping an attention weight.\r\nbatch_first \u2013 If True, then the input and output tensors are provided\r\nas (batch, seq, feature). Default: False\r\n\r\n\r\ndropout (float) \u2013 probability of dropping an attention weight.\r\nbatch_first \u2013 If True, then the input and output tensors are provided\r\nas (batch, seq, feature). Default: False\r\nExamples::\r\n>>> import torch, torchtext\r\n>>> SDP = torchtext.nn.ScaledDotProduct(dropout=0.1)\r\n>>> q = torch.randn(21, 256, 3)\r\n>>> k = v = torch.randn(21, 256, 3)\r\n>>> attn_output, attn_weights = SDP(q, k, v)\r\n>>> print(attn_output.shape, attn_weights.shape)\r\ntorch.Size([21, 256, 3]) torch.Size([256, 21, 21])\r\n\r\n\r\n\r\n>>> import torch, torchtext\r\n>>> SDP = torchtext.nn.ScaledDotProduct(dropout=0.1)\r\n>>> q = torch.randn(21, 256, 3)\r\n>>> k = v = torch.randn(21, 256, 3)\r\n>>> attn_output, attn_weights = SDP(q, k, v)\r\n>>> print(attn_output.shape, attn_weights.shape)\r\ntorch.Size([21, 256, 3]) torch.Size([256, 21, 21])\r\n\r\n\r\nforward(query: Tensor, key: Tensor, value: Tensor, attn_mask: Optional[Tensor] = None, bias_k: Optional[Tensor] = None, bias_v: Optional[Tensor] = None) \u2192 Tuple[Tensor, Tensor][source]\u00b6\r\nUses a scaled dot product with the projected key-value pair to update\r\nthe projected query.\r\n\r\nParameters:\r\n\r\nquery (Tensor) \u2013 Projected query\r\nkey (Tensor) \u2013 Projected key\r\nvalue (Tensor) \u2013 Projected value\r\nattn_mask (BoolTensor, optional) \u2013 3D mask that prevents attention to certain positions.\r\nattn_mask \u2013 3D mask that prevents attention to certain positions.\r\nbias_k (Tensor, optional) \u2013 one more key and value sequence to be added to keys at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should provide\r\nbias_v.\r\nbias_v (Tensor, optional) \u2013 one more key and value sequence to be added to values at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should also provide\r\nbias_k.\r\n\r\n\r\n\r\n\r\nShape:\r\nquery: \\((..., L, N * H, E / H)\\)\r\nkey: \\((..., S, N * H, E / H)\\)\r\nvalue: \\((..., S, N * H, E / H)\\)\r\n\r\nattn_mask: \\((N * H, L, S)\\), positions with True are not allowed to attendwhile False values will be unchanged.\r\n\r\n\r\n\r\nbias_k and bias_v:bias: \\((1, N * H, E / H)\\)\r\nOutput: \\((..., L, N * H, E / H)\\), \\((N * H, L, S)\\)\r\n\r\n\r\nNote: It\u2019s optional to have the query/key/value inputs with more than three dimensions (for broadcast purpose).The ScaledDotProduct module will operate on the last three dimensions.", "mimetype": "text/plain", "start_char_idx": 19854, "end_char_idx": 23503, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47d4ec04-876d-4cf0-aa87-6b06343d8884": {"__data__": {"id_": "47d4ec04-876d-4cf0-aa87-6b06343d8884", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0fda24d-c163-498d-987c-f1e996539c97", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "2211e0c302093779956e8dac0a16c6b51ae8b644e32ac1444ac34f638244b6d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a39ee67-88e3-4253-91e9-96233076bf73", "node_type": "1", "metadata": {}, "hash": "8f834f1f802666ff0d18ddfe9d8cc5820aaed9029684f0b89cd79ddde17bb4c1", "class_name": "RelatedNodeInfo"}}, "text": "Those are used for incremental decoding. Users should also provide\r\nbias_k.\r\n\r\n\r\n\r\n\r\nShape:\r\nquery: \\((..., L, N * H, E / H)\\)\r\nkey: \\((..., S, N * H, E / H)\\)\r\nvalue: \\((..., S, N * H, E / H)\\)\r\n\r\nattn_mask: \\((N * H, L, S)\\), positions with True are not allowed to attendwhile False values will be unchanged.\r\n\r\n\r\n\r\nbias_k and bias_v:bias: \\((1, N * H, E / H)\\)\r\nOutput: \\((..., L, N * H, E / H)\\), \\((N * H, L, S)\\)\r\n\r\n\r\nNote: It\u2019s optional to have the query/key/value inputs with more than three dimensions (for broadcast purpose).The ScaledDotProduct module will operate on the last three dimensions.\r\n\r\n\r\nwhere L is the target length, S is the source length, H is the number\r\nof attention heads, N is the batch size, and E is the embedding dimension.\r\n\r\n\r\n\r\nUses a scaled dot product with the projected key-value pair to update\r\nthe projected query.\r\nParameters:\r\n\r\nquery (Tensor) \u2013 Projected query\r\nkey (Tensor) \u2013 Projected key\r\nvalue (Tensor) \u2013 Projected value\r\nattn_mask (BoolTensor, optional) \u2013 3D mask that prevents attention to certain positions.\r\nattn_mask \u2013 3D mask that prevents attention to certain positions.\r\nbias_k (Tensor, optional) \u2013 one more key and value sequence to be added to keys at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should provide\r\nbias_v.\r\nbias_v (Tensor, optional) \u2013 one more key and value sequence to be added to values at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should also provide\r\nbias_k.\r\n\r\n\r\nquery (Tensor) \u2013 Projected query\r\nkey (Tensor) \u2013 Projected key\r\nvalue (Tensor) \u2013 Projected value\r\nattn_mask (BoolTensor, optional) \u2013 3D mask that prevents attention to certain positions.\r\nattn_mask \u2013 3D mask that prevents attention to certain positions.\r\nbias_k (Tensor, optional) \u2013 one more key and value sequence to be added to keys at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should provide\r\nbias_v.\r\nbias_v (Tensor, optional) \u2013 one more key and value sequence to be added to values at\r\nsequence dim (dim=-3). Those are used for incremental decoding. Users should also provide\r\nbias_k.\r\nShape:\r\n\r\nquery: \\((..., L, N * H, E / H)\\)\r\nkey: \\((..., S, N * H, E / H)\\)\r\nvalue: \\((..., S, N * H, E / H)\\)\r\n\r\nattn_mask: \\((N * H, L, S)\\), positions with True are not allowed to attendwhile False values will be unchanged.\r\n\r\n\r\n\r\nbias_k and bias_v:bias: \\((1, N * H, E / H)\\)\r\nOutput: \\((..., L, N * H, E / H)\\), \\((N * H, L, S)\\)\r\n\r\n\r\nNote: It\u2019s optional to have the query/key/value inputs with more than three dimensions (for broadcast purpose).The ScaledDotProduct module will operate on the last three dimensions.\r\n\r\n\r\nwhere L is the target length, S is the source length, H is the number\r\nof attention heads, N is the batch size, and E is the embedding dimension.\r\n\r\nquery: \\((..., L, N * H, E / H)\\)\r\nkey: \\((..., S, N * H, E / H)\\)\r\nvalue: \\((..., S, N * H, E / H)\\)\r\nattn_mask: \\((N * H, L, S)\\), positions with True are not allowed to attend\r\nwhile False values will be unchanged.\r\n\r\nwhile False values will be unchanged.\r\nbias_k and bias_v:bias: \\((1, N * H, E / H)\\)\r\nOutput: \\((..., L, N * H, E / H)\\), \\((N * H, L, S)\\)\r\nNote: It\u2019s optional to have the query/key/value inputs with more than three dimensions (for broadcast purpose).\r\nThe ScaledDotProduct module will operate on the last three dimensions.\r\n\r\nThe ScaledDotProduct module will operate on the last three dimensions.\r\nwhere L is the target length, S is the source length, H is the number\r\nof attention heads, N is the batch size, and E is the embedding dimension.", "mimetype": "text/plain", "start_char_idx": 22898, "end_char_idx": 26457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a39ee67-88e3-4253-91e9-96233076bf73": {"__data__": {"id_": "2a39ee67-88e3-4253-91e9-96233076bf73", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47d4ec04-876d-4cf0-aa87-6b06343d8884", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "9ffbd9ecaa58af24e29aeae5abbdef1e515e640050fe6c343fc7d29163e9f7bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "688399e1-d7bc-48a3-8be2-881b291cd407", "node_type": "1", "metadata": {}, "hash": "ebd6cb35183bc6abe770fa76fc9277990db4e0f2b2ffdb9465fbab9e4f04bd25", "class_name": "RelatedNodeInfo"}}, "text": "while False values will be unchanged.\r\nbias_k and bias_v:bias: \\((1, N * H, E / H)\\)\r\nOutput: \\((..., L, N * H, E / H)\\), \\((N * H, L, S)\\)\r\nNote: It\u2019s optional to have the query/key/value inputs with more than three dimensions (for broadcast purpose).\r\nThe ScaledDotProduct module will operate on the last three dimensions.\r\n\r\nThe ScaledDotProduct module will operate on the last three dimensions.\r\nwhere L is the target length, S is the source length, H is the number\r\nof attention heads, N is the batch size, and E is the embedding dimension. \r\n\r\nsource: https://pytorch.org/text/stable/data_functional.html \r\ncontent: \r\n\r\ntorchtext.data.functional\u00b6\r\ngenerate_sp_model\u00b6\r\n\r\ntorchtext.data.functional.generate_sp_model(filename, vocab_size=20000, model_type='unigram', model_prefix='m_user')[source]\u00b6\r\nTrain a SentencePiece tokenizer.\r\n\r\nParameters:\r\n\r\nfilename \u2013 the data file for training SentencePiece model.\r\nvocab_size \u2013 the size of vocabulary (Default: 20,000).\r\nmodel_type \u2013 the type of SentencePiece model, including unigram,\r\nbpe, char, word.\r\nmodel_prefix \u2013 the prefix of the files saving model and vocab.\r\n\r\n\r\n\r\n\r\nOutputs:\r\nThe model and vocab are saved in two separate files withmodel_prefix.\r\n\r\n\r\n\r\n\r\nExamples\r\n>>> from torchtext.data.functional import generate_sp_model\r\n>>> generate_sp_model('test.csv', vocab_size=23456, model_prefix='spm_user')\r\n\r\n\r\n\r\nTrain a SentencePiece tokenizer.\r\nParameters:\r\n\r\nfilename \u2013 the data file for training SentencePiece model.\r\nvocab_size \u2013 the size of vocabulary (Default: 20,000).\r\nmodel_type \u2013 the type of SentencePiece model, including unigram,\r\nbpe, char, word.\r\nmodel_prefix \u2013 the prefix of the files saving model and vocab.\r\n\r\n\r\nfilename \u2013 the data file for training SentencePiece model.\r\nvocab_size \u2013 the size of vocabulary (Default: 20,000).\r\nmodel_type \u2013 the type of SentencePiece model, including unigram,\r\nbpe, char, word.\r\nmodel_prefix \u2013 the prefix of the files saving model and vocab.\r\nOutputs:\r\n\r\nThe model and vocab are saved in two separate files withmodel_prefix.\r\n\r\n\r\n\r\nThe model and vocab are saved in two separate files with\r\nmodel_prefix.\r\n\r\nmodel_prefix.\r\nExamples\r\n>>> from torchtext.data.functional import generate_sp_model\r\n>>> generate_sp_model('test.csv', vocab_size=23456, model_prefix='spm_user')\r\n\r\nload_sp_model\u00b6\r\n\r\ntorchtext.data.functional.load_sp_model(spm)[source]\u00b6\r\nLoad a  sentencepiece model for file.\r\n\r\nParameters:\r\nspm \u2013 the file path or a file object saving the sentencepiece model.\r\n\r\n\r\n\r\nOutputs:output: a SentencePiece model.\r\n\r\n\r\nExamples\r\n>>> from torchtext.data.functional import load_sp_model\r\n>>> sp_model = load_sp_model(\"m_user.model\")\r\n>>> sp_model = load_sp_model(open(\"m_user.model\", 'rb'))\r\n\r\n\r\n\r\nLoad a  sentencepiece model for file.\r\nParameters:\r\nspm \u2013 the file path or a file object saving the sentencepiece model.\r\n\r\nspm \u2013 the file path or a file object saving the sentencepiece model.\r\nOutputs:\r\noutput: a SentencePiece model.\r\n\r\noutput: a SentencePiece model.\r\nExamples\r\n>>> from torchtext.data.functional import load_sp_model\r\n>>> sp_model = load_sp_model(\"m_user.model\")\r\n>>> sp_model = load_sp_model(open(\"m_user.model\", 'rb'))\r\n\r\nsentencepiece_numericalizer\u00b6\r\n\r\ntorchtext.data.functional.sentencepiece_numericalizer(sp_model)[source]\u00b6\r\n\r\nA sentencepiece model to numericalize a text sentence intoa generator over the ids.\r\n\r\n\r\n\r\nParameters:\r\nsp_model \u2013 a SentencePiece model.\r\n\r\n\r\n\r\nOutputs:\r\noutput: a generator with the input of text sentence and the output of thecorresponding ids based on SentencePiece model.\r\n\r\n\r\n\r\n\r\nExamples\r\n>>> from torchtext.data.functional import sentencepiece_numericalizer\r\n>>> sp_id_generator = sentencepiece_numericalizer(sp_model)\r\n>>> list_a = [\"sentencepiece encode as pieces\", \"examples to   try!\"]\r\n>>> list(sp_id_generator(list_a))\r\n    [[9858, 9249, 1629, 1305, 1809, 53, 842],\r\n     [2347, 13, 9, 150, 37]]\r\n\r\n\r\n\r\nA sentencepiece model to numericalize a text sentence into\r\na generator over the ids.\r\n\r\na generator over the ids.\r\nParameters:\r\nsp_model \u2013 a SentencePiece model.\r\n\r\nsp_model \u2013 a SentencePiece model.\r\nOutputs:\r\n\r\noutput: a generator with the input of text sentence and the output of thecorresponding ids based on SentencePiece model.\r\n\r\n\r\n\r\noutput: a generator with the input of text sentence and the output of the\r\ncorresponding ids based on SentencePiece model.\r\n\r\ncorresponding ids based on SentencePiece model.", "mimetype": "text/plain", "start_char_idx": 25912, "end_char_idx": 30298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "688399e1-d7bc-48a3-8be2-881b291cd407": {"__data__": {"id_": "688399e1-d7bc-48a3-8be2-881b291cd407", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a39ee67-88e3-4253-91e9-96233076bf73", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "07a3244a707b40f822135806bf23d257663a55e465743944c968e1cc24b34ba7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a041d4ab-217e-449c-95ca-6f91b6317ff6", "node_type": "1", "metadata": {}, "hash": "32905b55437c9cb1cd6620bdd1cd29b36f36aabf5878bfef6f1e9ffdbeb8ebcc", "class_name": "RelatedNodeInfo"}}, "text": ">>> list(sp_id_generator(list_a))\r\n    [[9858, 9249, 1629, 1305, 1809, 53, 842],\r\n     [2347, 13, 9, 150, 37]]\r\n\r\n\r\n\r\nA sentencepiece model to numericalize a text sentence into\r\na generator over the ids.\r\n\r\na generator over the ids.\r\nParameters:\r\nsp_model \u2013 a SentencePiece model.\r\n\r\nsp_model \u2013 a SentencePiece model.\r\nOutputs:\r\n\r\noutput: a generator with the input of text sentence and the output of thecorresponding ids based on SentencePiece model.\r\n\r\n\r\n\r\noutput: a generator with the input of text sentence and the output of the\r\ncorresponding ids based on SentencePiece model.\r\n\r\ncorresponding ids based on SentencePiece model.\r\nExamples\r\n>>> from torchtext.data.functional import sentencepiece_numericalizer\r\n>>> sp_id_generator = sentencepiece_numericalizer(sp_model)\r\n>>> list_a = [\"sentencepiece encode as pieces\", \"examples to   try!\"]\r\n>>> list(sp_id_generator(list_a))\r\n    [[9858, 9249, 1629, 1305, 1809, 53, 842],\r\n     [2347, 13, 9, 150, 37]]\r\n\r\nsentencepiece_tokenizer\u00b6\r\n\r\ntorchtext.data.functional.sentencepiece_tokenizer(sp_model)[source]\u00b6\r\n\r\nA sentencepiece model to tokenize a text sentence intoa generator over the tokens.\r\n\r\n\r\n\r\nParameters:\r\nsp_model \u2013 a SentencePiece model.\r\n\r\n\r\n\r\nOutputs:\r\noutput: a generator with the input of text sentence and the output of thecorresponding tokens based on SentencePiece model.\r\n\r\n\r\n\r\n\r\nExamples\r\n>>> from torchtext.data.functional import sentencepiece_tokenizer\r\n>>> sp_tokens_generator = sentencepiece_tokenizer(sp_model)\r\n>>> list_a = [\"sentencepiece encode as pieces\", \"examples to   try!\"]\r\n>>> list(sp_tokens_generator(list_a))\r\n    [['_sentence', 'piece', '_en', 'co', 'de', '_as', '_pieces'],\r\n     ['_example', 's', '_to', '_try', '!']]\r\n\r\n\r\n\r\nA sentencepiece model to tokenize a text sentence into\r\na generator over the tokens.\r\n\r\na generator over the tokens.\r\nParameters:\r\nsp_model \u2013 a SentencePiece model.\r\n\r\nsp_model \u2013 a SentencePiece model.\r\nOutputs:\r\n\r\noutput: a generator with the input of text sentence and the output of thecorresponding tokens based on SentencePiece model.\r\n\r\n\r\n\r\noutput: a generator with the input of text sentence and the output of the\r\ncorresponding tokens based on SentencePiece model.\r\n\r\ncorresponding tokens based on SentencePiece model.\r\nExamples\r\n>>> from torchtext.data.functional import sentencepiece_tokenizer\r\n>>> sp_tokens_generator = sentencepiece_tokenizer(sp_model)\r\n>>> list_a = [\"sentencepiece encode as pieces\", \"examples to   try!\"]\r\n>>> list(sp_tokens_generator(list_a))\r\n    [['_sentence', 'piece', '_en', 'co', 'de', '_as', '_pieces'],\r\n     ['_example', 's', '_to', '_try', '!']]\r\n\r\ncustom_replace\u00b6\r\n\r\ntorchtext.data.functional.custom_replace(replace_pattern)[source]\u00b6\r\nA transform to convert text string.\r\nExamples\r\n>>> from torchtext.data.functional import custom_replace\r\n>>> custom_replace_transform = custom_replace([(r'S', 's'), (r'\\s+', ' ')])\r\n>>> list_a = [\"Sentencepiece encode  aS  pieces\", \"exampleS to   try!\"]\r\n>>> list(custom_replace_transform(list_a))\r\n    ['sentencepiece encode as pieces', 'examples to try!']\r\n\r\n\r\n\r\nA transform to convert text string.\r\nExamples\r\n>>> from torchtext.data.functional import custom_replace\r\n>>> custom_replace_transform = custom_replace([(r'S', 's'), (r'\\s+', ' ')])\r\n>>> list_a = [\"Sentencepiece encode  aS  pieces\", \"exampleS to   try!\"]\r\n>>> list(custom_replace_transform(list_a))\r\n    ['sentencepiece encode as pieces', 'examples to try!']\r\n\r\nsimple_space_split\u00b6\r\n\r\ntorchtext.data.functional.simple_space_split(iterator)[source]\u00b6\r\nA transform to split text string by spaces.\r\nExamples\r\n>>> from torchtext.data.functional import simple_space_split\r\n>>> list_a = [\"Sentencepiece encode as pieces\", \"example to try!\"]\r\n>>> list(simple_space_split(list_a))\r\n    [['Sentencepiece', 'encode', 'as', 'pieces'], ['example', 'to', 'try!']]\r\n\r\n\r\n\r\nA transform to split text string by spaces.\r\nExamples\r\n>>> from torchtext.data.functional import simple_space_split\r\n>>> list_a = [\"Sentencepiece encode as pieces\", \"example to try!\"]\r\n>>> list(simple_space_split(list_a))\r\n    [['Sentencepiece', 'encode', 'as', 'pieces'], ['example', 'to', 'try!']]", "mimetype": "text/plain", "start_char_idx": 29666, "end_char_idx": 33776, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a041d4ab-217e-449c-95ca-6f91b6317ff6": {"__data__": {"id_": "a041d4ab-217e-449c-95ca-6f91b6317ff6", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "688399e1-d7bc-48a3-8be2-881b291cd407", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "d9bdbb308eb1ea930a9af502244bfe969654da97b3653fc0d142adbaa6e75566", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c1f2183-5922-4fe0-804f-593e9c0d9d85", "node_type": "1", "metadata": {}, "hash": "2e7748076824fa1751c37bc7e9559e0b7cf94263e57f7766a9455caeec21f262", "class_name": "RelatedNodeInfo"}}, "text": ">>> list(custom_replace_transform(list_a))\r\n    ['sentencepiece encode as pieces', 'examples to try!']\r\n\r\nsimple_space_split\u00b6\r\n\r\ntorchtext.data.functional.simple_space_split(iterator)[source]\u00b6\r\nA transform to split text string by spaces.\r\nExamples\r\n>>> from torchtext.data.functional import simple_space_split\r\n>>> list_a = [\"Sentencepiece encode as pieces\", \"example to try!\"]\r\n>>> list(simple_space_split(list_a))\r\n    [['Sentencepiece', 'encode', 'as', 'pieces'], ['example', 'to', 'try!']]\r\n\r\n\r\n\r\nA transform to split text string by spaces.\r\nExamples\r\n>>> from torchtext.data.functional import simple_space_split\r\n>>> list_a = [\"Sentencepiece encode as pieces\", \"example to try!\"]\r\n>>> list(simple_space_split(list_a))\r\n    [['Sentencepiece', 'encode', 'as', 'pieces'], ['example', 'to', 'try!']]\r\n\r\nnumericalize_tokens_from_iterator\u00b6\r\n\r\ntorchtext.data.functional.numericalize_tokens_from_iterator(vocab, iterator, removed_tokens=None)[source]\u00b6\r\nYield a list of ids from an token iterator with a vocab.\r\n\r\nParameters:\r\n\r\nvocab \u2013 the vocabulary convert token into id.\r\niterator \u2013 the iterator yield a list of tokens.\r\nremoved_tokens \u2013 removed tokens from output dataset (Default: None)\r\n\r\n\r\n\r\nExamples\r\n>>> from torchtext.data.functional import simple_space_split\r\n>>> from torchtext.data.functional import numericalize_tokens_from_iterator\r\n>>> vocab = {'Sentencepiece' : 0, 'encode' : 1, 'as' : 2, 'pieces' : 3}\r\n>>> ids_iter = numericalize_tokens_from_iterator(vocab,\r\n>>>                               simple_space_split([\"Sentencepiece as pieces\",\r\n>>>                                                   \"as pieces\"]))\r\n>>> for ids in ids_iter:\r\n>>>     print([num for num in ids])\r\n>>> [0, 2, 3]\r\n>>> [2, 3]\r\n\r\n\r\n\r\nYield a list of ids from an token iterator with a vocab.\r\nParameters:\r\n\r\nvocab \u2013 the vocabulary convert token into id.\r\niterator \u2013 the iterator yield a list of tokens.\r\nremoved_tokens \u2013 removed tokens from output dataset (Default: None)\r\n\r\n\r\nvocab \u2013 the vocabulary convert token into id.\r\niterator \u2013 the iterator yield a list of tokens.\r\nremoved_tokens \u2013 removed tokens from output dataset (Default: None)\r\nExamples\r\n>>> from torchtext.data.functional import simple_space_split\r\n>>> from torchtext.data.functional import numericalize_tokens_from_iterator\r\n>>> vocab = {'Sentencepiece' : 0, 'encode' : 1, 'as' : 2, 'pieces' : 3}\r\n>>> ids_iter = numericalize_tokens_from_iterator(vocab,\r\n>>>                               simple_space_split([\"Sentencepiece as pieces\",\r\n>>>                                                   \"as pieces\"]))\r\n>>> for ids in ids_iter:\r\n>>>     print([num for num in ids])\r\n>>> [0, 2, 3]\r\n>>> [2, 3]\r\n\r\nfilter_wikipedia_xml\u00b6\r\n\r\ntorchtext.data.functional.filter_wikipedia_xml(text_iterator)[source]\u00b6\r\nFilter wikipedia xml lines according to https://github.com/facebookresearch/fastText/blob/master/wikifil.pl\r\n\r\nParameters:\r\ntext_iterator \u2013 An iterator type object that yields strings. Examples include string list, text io, generators etc.\r\n\r\n\r\nExamples\r\n>>> from torchtext.data.functional import filter_wikipedia_xml\r\n>>> from torchtext.datasets import EnWik9\r\n>>> data_iter = EnWik9(split='train')\r\n>>> filter_data_iter = filter_wikipedia_xml(data_iter)\r\n>>> file_name = '.data/EnWik9/enwik9'\r\n>>> filter_data_iter = filter_wikipedia_xml(open(file_name,'r'))\r\n\r\n\r\n\r\nFilter wikipedia xml lines according to https://github.com/facebookresearch/fastText/blob/master/wikifil.pl\r\nParameters:\r\ntext_iterator \u2013 An iterator type object that yields strings. Examples include string list, text io, generators etc.\r\n\r\ntext_iterator \u2013 An iterator type object that yields strings. Examples include string list, text io, generators etc.\r\nExamples\r\n>>> from torchtext.data.functional import filter_wikipedia_xml\r\n>>> from torchtext.datasets import EnWik9\r\n>>> data_iter = EnWik9(split='train')\r\n>>> filter_data_iter = filter_wikipedia_xml(data_iter)\r\n>>> file_name = '.data/EnWik9/enwik9'\r\n>>> filter_data_iter = filter_wikipedia_xml(open(file_name,'r'))\r\n\r\nto_map_style_dataset\u00b6\r\n\r\ntorchtext.data.functional.to_map_style_dataset(iter_data)[source]\u00b6\r\nConvert iterable-style dataset to map-style dataset.\r\n\r\nParameters:\r\niter_data \u2013 An iterator type object. Examples include Iterable datasets, string list, text io, generators etc.", "mimetype": "text/plain", "start_char_idx": 32976, "end_char_idx": 37243, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c1f2183-5922-4fe0-804f-593e9c0d9d85": {"__data__": {"id_": "5c1f2183-5922-4fe0-804f-593e9c0d9d85", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a041d4ab-217e-449c-95ca-6f91b6317ff6", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "ac06dd6c55c3c5bfe84b5b4f0c756f726263c2a14c06b79241927354b7253110", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d2edc3f-2dcb-4019-97c2-85b8dcb7cecd", "node_type": "1", "metadata": {}, "hash": "6e0f4991a8482a2b58179044d01fa646497711b7828d1dd047b6fb0cafb3a4ab", "class_name": "RelatedNodeInfo"}}, "text": "Examples include string list, text io, generators etc.\r\n\r\ntext_iterator \u2013 An iterator type object that yields strings. Examples include string list, text io, generators etc.\r\nExamples\r\n>>> from torchtext.data.functional import filter_wikipedia_xml\r\n>>> from torchtext.datasets import EnWik9\r\n>>> data_iter = EnWik9(split='train')\r\n>>> filter_data_iter = filter_wikipedia_xml(data_iter)\r\n>>> file_name = '.data/EnWik9/enwik9'\r\n>>> filter_data_iter = filter_wikipedia_xml(open(file_name,'r'))\r\n\r\nto_map_style_dataset\u00b6\r\n\r\ntorchtext.data.functional.to_map_style_dataset(iter_data)[source]\u00b6\r\nConvert iterable-style dataset to map-style dataset.\r\n\r\nParameters:\r\niter_data \u2013 An iterator type object. Examples include Iterable datasets, string list, text io, generators etc.\r\n\r\n\r\nExamples\r\n>>> from torchtext.datasets import IMDB\r\n>>> from torchtext.data import to_map_style_dataset\r\n>>> train_iter = IMDB(split='train')\r\n>>> train_dataset = to_map_style_dataset(train_iter)\r\n>>> file_name = '.data/EnWik9/enwik9'\r\n>>> data_iter = to_map_style_dataset(open(file_name,'r'))\r\n\r\n\r\n\r\nConvert iterable-style dataset to map-style dataset.\r\nParameters:\r\niter_data \u2013 An iterator type object. Examples include Iterable datasets, string list, text io, generators etc.\r\n\r\niter_data \u2013 An iterator type object. Examples include Iterable datasets, string list, text io, generators etc.\r\nExamples\r\n>>> from torchtext.datasets import IMDB\r\n>>> from torchtext.data import to_map_style_dataset\r\n>>> train_iter = IMDB(split='train')\r\n>>> train_dataset = to_map_style_dataset(train_iter)\r\n>>> file_name = '.data/EnWik9/enwik9'\r\n>>> data_iter = to_map_style_dataset(open(file_name,'r'))\r\n \r\n\r\nsource: https://pytorch.org/text/stable/data_metrics.html \r\ncontent: \r\n\r\ntorchtext.data.metrics\u00b6\r\nbleu_score\u00b6\r\n\r\ntorchtext.data.metrics.bleu_score(candidate_corpus, references_corpus, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])[source]\u00b6\r\nComputes the BLEU score between a candidate translation corpus and a references\r\ntranslation corpus. Based on https://www.aclweb.org/anthology/P02-1040.pdf\r\n\r\nParameters:\r\n\r\ncandidate_corpus \u2013 an iterable of candidate translations. Each translation is an\r\niterable of tokens\r\nreferences_corpus \u2013 an iterable of iterables of reference translations. Each\r\ntranslation is an iterable of tokens\r\nmax_n \u2013 the maximum n-gram we want to use. E.g. if max_n=3, we will use unigrams,\r\nbigrams and trigrams\r\nweights \u2013 a list of weights used for each n-gram category (uniform by default)\r\n\r\n\r\n\r\nExamples\r\n>>> from torchtext.data.metrics import bleu_score\r\n>>> candidate_corpus = [['My', 'full', 'pytorch', 'test'], ['Another', 'Sentence']]\r\n>>> references_corpus = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']]]\r\n>>> bleu_score(candidate_corpus, references_corpus)\r\n    0.8408964276313782\r\n\r\n\r\n\r\nComputes the BLEU score between a candidate translation corpus and a references\r\ntranslation corpus. Based on https://www.aclweb.org/anthology/P02-1040.pdf\r\nParameters:\r\n\r\ncandidate_corpus \u2013 an iterable of candidate translations. Each translation is an\r\niterable of tokens\r\nreferences_corpus \u2013 an iterable of iterables of reference translations. Each\r\ntranslation is an iterable of tokens\r\nmax_n \u2013 the maximum n-gram we want to use. E.g. if max_n=3, we will use unigrams,\r\nbigrams and trigrams\r\nweights \u2013 a list of weights used for each n-gram category (uniform by default)\r\n\r\n\r\ncandidate_corpus \u2013 an iterable of candidate translations. Each translation is an\r\niterable of tokens\r\nreferences_corpus \u2013 an iterable of iterables of reference translations. Each\r\ntranslation is an iterable of tokens\r\nmax_n \u2013 the maximum n-gram we want to use. E.g.", "mimetype": "text/plain", "start_char_idx": 36477, "end_char_idx": 40145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d2edc3f-2dcb-4019-97c2-85b8dcb7cecd": {"__data__": {"id_": "1d2edc3f-2dcb-4019-97c2-85b8dcb7cecd", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c1f2183-5922-4fe0-804f-593e9c0d9d85", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "5a910d1bd372ef6be8beba240195920fb8167ea8fec216d532efbed139cc8639", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d203d24e-5c30-4c7a-82df-f3b314f76f11", "node_type": "1", "metadata": {}, "hash": "e543fa509c6b4d6dc63c151c41d9681dc06f9df9ec17339fbca1b9f77895b672", "class_name": "RelatedNodeInfo"}}, "text": "Based on https://www.aclweb.org/anthology/P02-1040.pdf\r\nParameters:\r\n\r\ncandidate_corpus \u2013 an iterable of candidate translations. Each translation is an\r\niterable of tokens\r\nreferences_corpus \u2013 an iterable of iterables of reference translations. Each\r\ntranslation is an iterable of tokens\r\nmax_n \u2013 the maximum n-gram we want to use. E.g. if max_n=3, we will use unigrams,\r\nbigrams and trigrams\r\nweights \u2013 a list of weights used for each n-gram category (uniform by default)\r\n\r\n\r\ncandidate_corpus \u2013 an iterable of candidate translations. Each translation is an\r\niterable of tokens\r\nreferences_corpus \u2013 an iterable of iterables of reference translations. Each\r\ntranslation is an iterable of tokens\r\nmax_n \u2013 the maximum n-gram we want to use. E.g. if max_n=3, we will use unigrams,\r\nbigrams and trigrams\r\nweights \u2013 a list of weights used for each n-gram category (uniform by default)\r\nExamples\r\n>>> from torchtext.data.metrics import bleu_score\r\n>>> candidate_corpus = [['My', 'full', 'pytorch', 'test'], ['Another', 'Sentence']]\r\n>>> references_corpus = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']]]\r\n>>> bleu_score(candidate_corpus, references_corpus)\r\n    0.8408964276313782\r\n \r\n\r\nsource: https://pytorch.org/text/stable/data_utils.html \r\ncontent: \r\n\r\ntorchtext.data.utils\u00b6\r\nget_tokenizer\u00b6\r\n\r\ntorchtext.data.utils.get_tokenizer(tokenizer, language='en')[source]\u00b6\r\nGenerate tokenizer function for a string sentence.\r\n\r\nParameters:\r\n\r\ntokenizer \u2013 the name of tokenizer function. If None, it returns split()\r\nfunction, which splits the string sentence by space.\r\nIf basic_english, it returns _basic_english_normalize() function,\r\nwhich normalize the string first and split by space. If a callable\r\nfunction, it will return the function. If a tokenizer library\r\n(e.g. spacy, moses, toktok, revtok, subword), it returns the\r\ncorresponding library.\r\nlanguage \u2013 Default en\r\n\r\n\r\n\r\nExamples\r\n>>> import torchtext\r\n>>> from torchtext.data import get_tokenizer\r\n>>> tokenizer = get_tokenizer(\"basic_english\")\r\n>>> tokens = tokenizer(\"You can now install TorchText using pip!\")\r\n>>> tokens\r\n>>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\r\n\r\n\r\n\r\nGenerate tokenizer function for a string sentence.\r\nParameters:\r\n\r\ntokenizer \u2013 the name of tokenizer function. If None, it returns split()\r\nfunction, which splits the string sentence by space.\r\nIf basic_english, it returns _basic_english_normalize() function,\r\nwhich normalize the string first and split by space. If a callable\r\nfunction, it will return the function. If a tokenizer library\r\n(e.g. spacy, moses, toktok, revtok, subword), it returns the\r\ncorresponding library.\r\nlanguage \u2013 Default en\r\n\r\n\r\ntokenizer \u2013 the name of tokenizer function. If None, it returns split()\r\nfunction, which splits the string sentence by space.\r\nIf basic_english, it returns _basic_english_normalize() function,\r\nwhich normalize the string first and split by space. If a callable\r\nfunction, it will return the function. If a tokenizer library\r\n(e.g. spacy, moses, toktok, revtok, subword), it returns the\r\ncorresponding library.\r\nlanguage \u2013 Default en\r\nExamples\r\n>>> import torchtext\r\n>>> from torchtext.data import get_tokenizer\r\n>>> tokenizer = get_tokenizer(\"basic_english\")\r\n>>> tokens = tokenizer(\"You can now install TorchText using pip!\")\r\n>>> tokens\r\n>>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\r\n\r\nngrams_iterator\u00b6\r\n\r\ntorchtext.data.utils.ngrams_iterator(token_list, ngrams)[source]\u00b6\r\nReturn an iterator that yields the given tokens and their ngrams.\r\n\r\nParameters:\r\n\r\ntoken_list \u2013 A list of tokens\r\nngrams \u2013 the number of ngrams.\r\n\r\n\r\n\r\nExamples\r\n>>> token_list = ['here', 'we', 'are']\r\n>>> list(ngrams_iterator(token_list, 2))\r\n>>> ['here', 'here we', 'we', 'we are', 'are']\r\n\r\n\r\n\r\nReturn an iterator that yields the given tokens and their ngrams.\r\nParameters:\r\n\r\ntoken_list \u2013 A list of tokens\r\nngrams \u2013 the number of ngrams.\r\n\r\n\r\ntoken_list \u2013 A list of tokens\r\nngrams \u2013 the number of ngrams.", "mimetype": "text/plain", "start_char_idx": 39402, "end_char_idx": 43413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d203d24e-5c30-4c7a-82df-f3b314f76f11": {"__data__": {"id_": "d203d24e-5c30-4c7a-82df-f3b314f76f11", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d2edc3f-2dcb-4019-97c2-85b8dcb7cecd", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "4f60a24ec77efb6d7db21fe37ad48f6fa13e50daef8e0d0d6197da5559423032", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32a73738-50d7-41de-a0f8-7a8cd117786e", "node_type": "1", "metadata": {}, "hash": "9b6ebaf32caacd83c7cae6b88c63e52624a68c634be307db907dc1811660f683", "class_name": "RelatedNodeInfo"}}, "text": ">>> tokens\r\n>>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\r\n\r\nngrams_iterator\u00b6\r\n\r\ntorchtext.data.utils.ngrams_iterator(token_list, ngrams)[source]\u00b6\r\nReturn an iterator that yields the given tokens and their ngrams.\r\n\r\nParameters:\r\n\r\ntoken_list \u2013 A list of tokens\r\nngrams \u2013 the number of ngrams.\r\n\r\n\r\n\r\nExamples\r\n>>> token_list = ['here', 'we', 'are']\r\n>>> list(ngrams_iterator(token_list, 2))\r\n>>> ['here', 'here we', 'we', 'we are', 'are']\r\n\r\n\r\n\r\nReturn an iterator that yields the given tokens and their ngrams.\r\nParameters:\r\n\r\ntoken_list \u2013 A list of tokens\r\nngrams \u2013 the number of ngrams.\r\n\r\n\r\ntoken_list \u2013 A list of tokens\r\nngrams \u2013 the number of ngrams.\r\nExamples\r\n>>> token_list = ['here', 'we', 'are']\r\n>>> list(ngrams_iterator(token_list, 2))\r\n>>> ['here', 'here we', 'we', 'we are', 'are']\r\n \r\n\r\nsource: https://pytorch.org/text/stable/datasets.html \r\ncontent: \r\n\r\ntorchtext.datasets\u00b6\r\nWarning\r\nThe datasets supported by torchtext are datapipes from the torchdata\r\nproject, which is still in Beta\r\nstatus. This means that the API is subject to change without deprecation\r\ncycles. In particular, we expect a lot of the current idioms to change with\r\nthe eventual release of DataLoaderV2 from torchdata.\r\nHere are a few recommendations regarding the use of datapipes:\r\nFor shuffling the datapipe, do that in the DataLoader: DataLoader(dp, shuffle=True).\r\nYou do not need to call dp.shuffle(), because torchtext has\r\nalready done that for you. Note however that the datapipe won\u2019t be\r\nshuffled unless you explicitly pass shuffle=True to the DataLoader.\r\nWhen using multi-processing (num_workers=N), use the builtin worker_init_fn:\r\nfrom torch.utils.data.backward_compatibility import worker_init_fn\r\nDataLoader(dp, num_workers=4, worker_init_fn=worker_init_fn, drop_last=True)\r\n\r\nThis will ensure that data isn\u2019t duplicated across workers.\r\nWe also recommend using drop_last=True. Without this, the batch sizes\r\nat the end of an epoch may be very small in some cases (smaller than with\r\nother map-style datasets). This might affect accuracy greatly especially\r\nwhen batch-norm is used. drop_last=True ensures that all batch sizes\r\nare equal.\r\nDistributed training with DistributedDataParallel is not yet entirely\r\nstable / supported, and we don\u2019t recommend it at this point. It will be\r\nbetter supported in DataLoaderV2. If you still wish to use DDP, make sure\r\nthat:\r\nAll workers (DDP workers and DataLoader workers) see a different part\r\nof the data. The datasets are already wrapped inside  ShardingFilter\r\nand you may need to call dp.apply_sharding(num_shards, shard_id) in order to shard the\r\ndata across ranks (DDP workers) and DataLoader workers. One way to do this\r\nis to create worker_init_fn that calls apply_sharding with appropriate\r\nnumber of shards (DDP workers * DataLoader workers) and shard id (inferred through rank\r\nand worker ID of corresponding DataLoader withing rank). Note however, that this assumes\r\nequal number of DataLoader workers for all the ranks.\r\nAll DDP workers work on the same number of batches. One way to do this\r\nis to by limit the size of the datapipe within each worker to\r\nlen(datapipe) // num_ddp_workers, but this might not suit all\r\nuse-cases.\r\nThe shuffling seed is the same across all workers. You might need to\r\ncall torch.utils.data.graph_settings.apply_shuffle_seed(dp, rng)\r\nThe shuffling seed is different across epochs.\r\nThe rest of the RNG (typically used for transformations) is\r\ndifferent across workers, for maximal entropy and optimal accuracy.\r\nGeneral use cases are as follows:\r\n# import datasets\r\nfrom torchtext.datasets import IMDB\r\n\r\ntrain_iter = IMDB(split='train')\r\n\r\ndef tokenize(label, line):\r\n    return line.split()\r\n\r\ntokens = []\r\nfor label, line in train_iter:\r\n    tokens += tokenize(label, line)\r\n\r\nThe following datasets are currently available. If you would like to contribute\r\nnew datasets to the repo or work with your own custom datasets, please refer to CONTRIBUTING_DATASETS.md guide.", "mimetype": "text/plain", "start_char_idx": 42731, "end_char_idx": 46725, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32a73738-50d7-41de-a0f8-7a8cd117786e": {"__data__": {"id_": "32a73738-50d7-41de-a0f8-7a8cd117786e", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d203d24e-5c30-4c7a-82df-f3b314f76f11", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "3dcd71a69145bb452d5635f030a5b2ca1e937ecd8f21a191cfd385b59990f7e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a294d79f-e526-4713-bfd6-e5e73f9e49b1", "node_type": "1", "metadata": {}, "hash": "35794f24c90bf13aca5790e54ea85337f8581aea307774515b14f381cd10e616", "class_name": "RelatedNodeInfo"}}, "text": "The shuffling seed is the same across all workers. You might need to\r\ncall torch.utils.data.graph_settings.apply_shuffle_seed(dp, rng)\r\nThe shuffling seed is different across epochs.\r\nThe rest of the RNG (typically used for transformations) is\r\ndifferent across workers, for maximal entropy and optimal accuracy.\r\nGeneral use cases are as follows:\r\n# import datasets\r\nfrom torchtext.datasets import IMDB\r\n\r\ntrain_iter = IMDB(split='train')\r\n\r\ndef tokenize(label, line):\r\n    return line.split()\r\n\r\ntokens = []\r\nfor label, line in train_iter:\r\n    tokens += tokenize(label, line)\r\n\r\nThe following datasets are currently available. If you would like to contribute\r\nnew datasets to the repo or work with your own custom datasets, please refer to CONTRIBUTING_DATASETS.md guide.\r\nDatasets\r\nText Classification\r\nAG_NEWS\r\nAmazonReviewFull\r\nAmazonReviewPolarity\r\nCoLA\r\nDBpedia\r\nIMDb\r\nMNLI\r\nMRPC\r\nQNLI\r\nQQP\r\nRTE\r\nSogouNews\r\nSST2\r\nSTSB\r\nWNLI\r\nYahooAnswers\r\nYelpReviewFull\r\nYelpReviewPolarity\r\nLanguage Modeling\r\nPennTreebank\r\nWikiText-2\r\nWikiText103\r\nMachine Translation\r\nIWSLT2016\r\nIWSLT2017\r\nMulti30k\r\nSequence Tagging\r\nCoNLL2000Chunking\r\nUDPOS\r\nQuestion Answer\r\nSQuAD 1.0\r\nSQuAD 2.0\r\nUnsupervised Learning\r\nCC100\r\nEnWik9\r\nText Classification\u00b6\r\nAG_NEWS\u00b6\r\n\r\ntorchtext.datasets.AG_NEWS(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'test'))[source]\u00b6\r\nAG_NEWS Dataset\r\n\r\nWarning\r\nUsing datapipes is still currently subject to a few caveats. If you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://paperswithcode.com/dataset/ag-news\r\n\r\nNumber of lines per split:\r\ntrain: 120000\r\ntest: 7600\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 4) and text\r\n\r\nReturn type:\r\n(int, str)\r\n\r\n\r\n\r\nAG_NEWS Dataset\r\nWarning\r\nUsing datapipes is still currently subject to a few caveats. If you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://paperswithcode.com/dataset/ag-news\r\nNumber of lines per split:\r\n\r\ntrain: 120000\r\ntest: 7600\r\n\r\n\r\ntrain: 120000\r\ntest: 7600\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 4) and text\r\n\r\nDataPipe that yields tuple of label (1 to 4) and text\r\nReturn type:\r\n(int, str)\r\n\r\n(int, str)\r\nAmazonReviewFull\u00b6\r\n\r\ntorchtext.datasets.AmazonReviewFull(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'test'))[source]\u00b6\r\nAmazonReviewFull Dataset\r\n\r\nWarning\r\nUsing datapipes is still currently subject to a few caveats. If you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\n\r\nNumber of lines per split:\r\ntrain: 3000000\r\ntest: 650000\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 5) and text containing the review title and text\r\n\r\nReturn type:\r\n(int, str)\r\n\r\n\r\n\r\nAmazonReviewFull Dataset\r\nWarning\r\nUsing datapipes is still currently subject to a few caveats.", "mimetype": "text/plain", "start_char_idx": 45951, "end_char_idx": 49937, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a294d79f-e526-4713-bfd6-e5e73f9e49b1": {"__data__": {"id_": "a294d79f-e526-4713-bfd6-e5e73f9e49b1", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32a73738-50d7-41de-a0f8-7a8cd117786e", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "8efc7d61e203ce138df68c57ceec6ca7755262144df3209063fd0652075a85de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c5ab448-de9d-4498-b7fb-012b9d6262e5", "node_type": "1", "metadata": {}, "hash": "437f75dc764c3f64dc946ae1d44cb486808050a78297af9c9932faf3af07a46a", "class_name": "RelatedNodeInfo"}}, "text": "If you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\n\r\nNumber of lines per split:\r\ntrain: 3000000\r\ntest: 650000\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 5) and text containing the review title and text\r\n\r\nReturn type:\r\n(int, str)\r\n\r\n\r\n\r\nAmazonReviewFull Dataset\r\nWarning\r\nUsing datapipes is still currently subject to a few caveats. If you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\nNumber of lines per split:\r\n\r\ntrain: 3000000\r\ntest: 650000\r\n\r\n\r\ntrain: 3000000\r\ntest: 650000\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 5) and text containing the review title and text\r\n\r\nDataPipe that yields tuple of label (1 to 5) and text containing the review title and text\r\nReturn type:\r\n(int, str)\r\n\r\n(int, str)\r\nAmazonReviewPolarity\u00b6\r\n\r\ntorchtext.datasets.AmazonReviewPolarity(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'test'))[source]\u00b6\r\nAmazonReviewPolarity Dataset\r\n\r\nWarning\r\nUsing datapipes is still currently subject to a few caveats. If you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\n\r\nNumber of lines per split:\r\ntrain: 3600000\r\ntest: 400000\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 2) and text containing the review title and text\r\n\r\nReturn type:\r\n(int, str)\r\n\r\n\r\n\r\nAmazonReviewPolarity Dataset\r\nWarning\r\nUsing datapipes is still currently subject to a few caveats. If you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\nNumber of lines per split:\r\n\r\ntrain: 3600000\r\ntest: 400000\r\n\r\n\r\ntrain: 3600000\r\ntest: 400000\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 2) and text containing the review title and text\r\n\r\nDataPipe that yields tuple of label (1 to 2) and text containing the review title and text\r\nReturn type:\r\n(int, str)\r\n\r\n(int, str)\r\nCoLA\u00b6\r\n\r\ntorchtext.datasets.CoLA(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'dev', 'test'))[source]\u00b6\r\nCoLA dataset\r\n\r\nWarning\r\nUsing datapipes is still currently subject to a few caveats. If you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.", "mimetype": "text/plain", "start_char_idx": 49210, "end_char_idx": 53177, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c5ab448-de9d-4498-b7fb-012b9d6262e5": {"__data__": {"id_": "1c5ab448-de9d-4498-b7fb-012b9d6262e5", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a294d79f-e526-4713-bfd6-e5e73f9e49b1", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "05f5771ba6b83af7c280b7f7ab69e1e29b6c1496c39e6e0d4487fd84f7236ab8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "086119a5-f3c9-41f9-a6de-d934cb12b77c", "node_type": "1", "metadata": {}, "hash": "53e16c28b9b6bf48f8ee983628f3954a2ab07fbdcff03895407a826a32ec0930", "class_name": "RelatedNodeInfo"}}, "text": "Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 2) and text containing the review title and text\r\n\r\nDataPipe that yields tuple of label (1 to 2) and text containing the review title and text\r\nReturn type:\r\n(int, str)\r\n\r\n(int, str)\r\nCoLA\u00b6\r\n\r\ntorchtext.datasets.CoLA(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'dev', 'test'))[source]\u00b6\r\nCoLA dataset\r\n\r\nWarning\r\nUsing datapipes is still currently subject to a few caveats. If you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://nyu-mll.github.io/CoLA/\r\n\r\nNumber of lines per split:\r\ntrain: 8551\r\ndev: 527\r\ntest: 516\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields rows from CoLA dataset (source (str), label (int), sentence (str))\r\n\r\nReturn type:\r\n(str, int, str)\r\n\r\n\r\n\r\nCoLA dataset\r\nWarning\r\nUsing datapipes is still currently subject to a few caveats. If you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://nyu-mll.github.io/CoLA/\r\nNumber of lines per split:\r\n\r\ntrain: 8551\r\ndev: 527\r\ntest: 516\r\n\r\n\r\ntrain: 8551\r\ndev: 527\r\ntest: 516\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\nReturns:\r\nDataPipe that yields rows from CoLA dataset (source (str), label (int), sentence (str))\r\n\r\nDataPipe that yields rows from CoLA dataset (source (str), label (int), sentence (str))\r\nReturn type:\r\n(str, int, str)\r\n\r\n(str, int, str)\r\nDBpedia\u00b6\r\n\r\ntorchtext.datasets.DBpedia(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'test'))[source]\u00b6\r\nDBpedia Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://www.dbpedia.org/resources/latest-core/\r\n\r\nNumber of lines per split:\r\ntrain: 560000\r\ntest: 70000\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 14) and text containing the news title and contents\r\n\r\nReturn type:\r\n(int, str)\r\n\r\n\r\n\r\nDBpedia Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://www.dbpedia.org/resources/latest-core/\r\nNumber of lines per split:\r\n\r\ntrain: 560000\r\ntest: 70000\r\n\r\n\r\ntrain: 560000\r\ntest: 70000\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings.", "mimetype": "text/plain", "start_char_idx": 52441, "end_char_idx": 56387, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "086119a5-f3c9-41f9-a6de-d934cb12b77c": {"__data__": {"id_": "086119a5-f3c9-41f9-a6de-d934cb12b77c", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c5ab448-de9d-4498-b7fb-012b9d6262e5", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "35686c0eadc369429517daf05a112b49c0e35e6bde8e43f15f5a2607c8197da9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69b0579f-7229-4d74-ac02-e13e2087cd82", "node_type": "1", "metadata": {}, "hash": "2757301e31a043911ef360e5d04816c331000340061200d193f92880fd5ca9f8", "class_name": "RelatedNodeInfo"}}, "text": "if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://www.dbpedia.org/resources/latest-core/\r\nNumber of lines per split:\r\n\r\ntrain: 560000\r\ntest: 70000\r\n\r\n\r\ntrain: 560000\r\ntest: 70000\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 14) and text containing the news title and contents\r\n\r\nDataPipe that yields tuple of label (1 to 14) and text containing the news title and contents\r\nReturn type:\r\n(int, str)\r\n\r\n(int, str)\r\nIMDb\u00b6\r\n\r\ntorchtext.datasets.IMDB(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'test'))[source]\u00b6\r\nIMDB Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to http://ai.stanford.edu/~amaas/data/sentiment/\r\n\r\nNumber of lines per split:\r\ntrain: 25000\r\ntest: 25000\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 2) and text containing the movie review\r\n\r\nReturn type:\r\n(int, str)\r\n\r\n\r\n\r\nTutorials using IMDB:\r\nT5-Base Model for Summarization, Sentiment Classification, and Translation\r\nT5-Base Model for Summarization, Sentiment Classification, and Translation\r\n\r\n\r\n\r\nIMDB Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to http://ai.stanford.edu/~amaas/data/sentiment/\r\nNumber of lines per split:\r\n\r\ntrain: 25000\r\ntest: 25000\r\n\r\n\r\ntrain: 25000\r\ntest: 25000\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 2) and text containing the movie review\r\n\r\nDataPipe that yields tuple of label (1 to 2) and text containing the movie review\r\nReturn type:\r\n(int, str)\r\n\r\n(int, str)\r\nTutorials using IMDB:\r\n\r\nT5-Base Model for Summarization, Sentiment Classification, and Translation\r\nT5-Base Model for Summarization, Sentiment Classification, and Translation\r\n\r\nT5-Base Model for Summarization, Sentiment Classification, and Translation\r\nMNLI\u00b6\r\n\r\ntorchtext.datasets.MNLI(root='.data', split=('train', 'dev_matched', 'dev_mismatched'))[source]\u00b6\r\nMNLI Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://cims.nyu.edu/~sbowman/multinli/\r\n\r\nNumber of lines per split:\r\ntrain: 392702\r\ndev_matched: 9815\r\ndev_mismatched: 9832\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev_matched, dev_mismatched)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of text and label (0 to 2).", "mimetype": "text/plain", "start_char_idx": 55683, "end_char_idx": 59761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69b0579f-7229-4d74-ac02-e13e2087cd82": {"__data__": {"id_": "69b0579f-7229-4d74-ac02-e13e2087cd82", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "086119a5-f3c9-41f9-a6de-d934cb12b77c", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "694d31cd26b77d962b1019cb85ac6dd7774b90356d8fd5b958ed9d6fb46418ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aac24df6-58b5-435a-9eb6-4b916c966e7d", "node_type": "1", "metadata": {}, "hash": "564092480207810b5cf8c69fccd6ab615bb47eb5a818c4d01bb2a8ba7c8ab5cc", "class_name": "RelatedNodeInfo"}}, "text": "if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://cims.nyu.edu/~sbowman/multinli/\r\n\r\nNumber of lines per split:\r\ntrain: 392702\r\ndev_matched: 9815\r\ndev_mismatched: 9832\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev_matched, dev_mismatched)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of text and label (0 to 2).\r\n\r\nReturn type:\r\nTuple[int, str, str]\r\n\r\n\r\n\r\nMNLI Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://cims.nyu.edu/~sbowman/multinli/\r\nNumber of lines per split:\r\n\r\ntrain: 392702\r\ndev_matched: 9815\r\ndev_mismatched: 9832\r\n\r\n\r\ntrain: 392702\r\ndev_matched: 9815\r\ndev_mismatched: 9832\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev_matched, dev_mismatched)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev_matched, dev_mismatched)\r\nReturns:\r\nDataPipe that yields tuple of text and label (0 to 2).\r\n\r\nDataPipe that yields tuple of text and label (0 to 2).\r\nReturn type:\r\nTuple[int, str, str]\r\n\r\nTuple[int, str, str]\r\nMRPC\u00b6\r\n\r\ntorchtext.datasets.MRPC(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'test'))[source]\u00b6\r\nMRPC Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://www.microsoft.com/en-us/download/details.aspx?id=52398\r\n\r\nNumber of lines per split:\r\ntrain: 4076\r\ntest: 1725\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields data points from MRPC dataset which consist of label, sentence1, sentence2\r\n\r\nReturn type:\r\n(int, str, str)\r\n\r\n\r\n\r\nMRPC Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://www.microsoft.com/en-us/download/details.aspx?id=52398\r\nNumber of lines per split:\r\n\r\ntrain: 4076\r\ntest: 1725\r\n\r\n\r\ntrain: 4076\r\ntest: 1725\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings.", "mimetype": "text/plain", "start_char_idx": 59145, "end_char_idx": 62565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aac24df6-58b5-435a-9eb6-4b916c966e7d": {"__data__": {"id_": "aac24df6-58b5-435a-9eb6-4b916c966e7d", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69b0579f-7229-4d74-ac02-e13e2087cd82", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "8f73e6dd3d9660dab62cd996add619b474baa776c70d8b2b116cacf519a00a87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c52c881-b5ce-4d79-9ea6-3f3d97441844", "node_type": "1", "metadata": {}, "hash": "1863b710edcd9278d7d4a663cddb88a555e0490891ad9236ee22f65da9bd6bc6", "class_name": "RelatedNodeInfo"}}, "text": "if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://www.microsoft.com/en-us/download/details.aspx?id=52398\r\nNumber of lines per split:\r\n\r\ntrain: 4076\r\ntest: 1725\r\n\r\n\r\ntrain: 4076\r\ntest: 1725\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\nReturns:\r\nDataPipe that yields data points from MRPC dataset which consist of label, sentence1, sentence2\r\n\r\nDataPipe that yields data points from MRPC dataset which consist of label, sentence1, sentence2\r\nReturn type:\r\n(int, str, str)\r\n\r\n(int, str, str)\r\nQNLI\u00b6\r\n\r\ntorchtext.datasets.QNLI(root='.data', split=('train', 'dev', 'test'))[source]\u00b6\r\nQNLI Dataset\r\nFor additional details refer to https://arxiv.org/pdf/1804.07461.pdf (from GLUE paper)\r\n\r\nNumber of lines per split:\r\ntrain: 104743\r\ndev: 5463\r\ntest: 5463\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of text and label (0 and 1).\r\n\r\nReturn type:\r\n(int, str, str)\r\n\r\n\r\n\r\nQNLI Dataset\r\nFor additional details refer to https://arxiv.org/pdf/1804.07461.pdf (from GLUE paper)\r\nNumber of lines per split:\r\n\r\ntrain: 104743\r\ndev: 5463\r\ntest: 5463\r\n\r\n\r\ntrain: 104743\r\ndev: 5463\r\ntest: 5463\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\nReturns:\r\nDataPipe that yields tuple of text and label (0 and 1).\r\n\r\nDataPipe that yields tuple of text and label (0 and 1).\r\nReturn type:\r\n(int, str, str)\r\n\r\n(int, str, str)\r\nQQP\u00b6\r\n\r\ntorchtext.datasets.QQP(root: str)[source]\u00b6\r\nQQP dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs\r\n\r\nParameters:\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\n\r\nReturns:\r\nDataPipe that yields rows from QQP dataset (label (int), question1 (str), question2 (str))\r\n\r\nReturn type:\r\n(int, str, str)\r\n\r\n\r\n\r\nQQP dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs\r\nParameters:\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\n\r\nroot \u2013 Directory where the datasets are saved.", "mimetype": "text/plain", "start_char_idx": 61851, "end_char_idx": 65386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c52c881-b5ce-4d79-9ea6-3f3d97441844": {"__data__": {"id_": "4c52c881-b5ce-4d79-9ea6-3f3d97441844", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aac24df6-58b5-435a-9eb6-4b916c966e7d", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "5262a8c60f5969e773bd30ea6421407cc25dd2942f9e5939d2606c465ae665e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55a675a8-2c34-4f9f-adfd-bbc8e5ad8fdd", "node_type": "1", "metadata": {}, "hash": "fd3df01e0e408273ffd4f11830724e1cb4ae81350f4ffafd8700e53e94c7c415", "class_name": "RelatedNodeInfo"}}, "text": "Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\n\r\nReturns:\r\nDataPipe that yields rows from QQP dataset (label (int), question1 (str), question2 (str))\r\n\r\nReturn type:\r\n(int, str, str)\r\n\r\n\r\n\r\nQQP dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs\r\nParameters:\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nReturns:\r\nDataPipe that yields rows from QQP dataset (label (int), question1 (str), question2 (str))\r\n\r\nDataPipe that yields rows from QQP dataset (label (int), question1 (str), question2 (str))\r\nReturn type:\r\n(int, str, str)\r\n\r\n(int, str, str)\r\nRTE\u00b6\r\n\r\ntorchtext.datasets.RTE(root='.data', split=('train', 'dev', 'test'))[source]\u00b6\r\nRTE Dataset\r\nFor additional details refer to https://aclweb.org/aclwiki/Recognizing_Textual_Entailment\r\n\r\nNumber of lines per split:\r\ntrain: 2490\r\ndev: 277\r\ntest: 3000\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of text and/or label (0 and 1). The test split only returns text.\r\n\r\nReturn type:\r\nUnion[(int, str, str), (str, str)]\r\n\r\n\r\n\r\nRTE Dataset\r\nFor additional details refer to https://aclweb.org/aclwiki/Recognizing_Textual_Entailment\r\nNumber of lines per split:\r\n\r\ntrain: 2490\r\ndev: 277\r\ntest: 3000\r\n\r\n\r\ntrain: 2490\r\ndev: 277\r\ntest: 3000\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\nReturns:\r\nDataPipe that yields tuple of text and/or label (0 and 1). The test split only returns text.\r\n\r\nDataPipe that yields tuple of text and/or label (0 and 1). The test split only returns text.\r\nReturn type:\r\nUnion[(int, str, str), (str, str)]\r\n\r\nUnion[(int, str, str), (str, str)]\r\nSogouNews\u00b6\r\n\r\ntorchtext.datasets.SogouNews(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'test'))[source]\u00b6\r\nSogouNews Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\n\r\n\r\nNumber of lines per split:\r\ntrain: 450000\r\ntest: 60000\r\n\r\n\r\nArgs:root: Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit: split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\n\r\nreturns:\r\nDataPipe that yields tuple of label (1 to 5) and text containing the news title and contents\r\n\r\nrtype:\r\n(int, str)\r\n\r\n\r\n\r\n\r\nSogouNews Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\nNumber of lines per split:\r\n\r\ntrain: 450000\r\ntest: 60000\r\n\r\n\r\ntrain: 450000\r\ntest: 60000\r\nArgs:\r\nroot: Directory where the datasets are saved.", "mimetype": "text/plain", "start_char_idx": 64704, "end_char_idx": 68527, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55a675a8-2c34-4f9f-adfd-bbc8e5ad8fdd": {"__data__": {"id_": "55a675a8-2c34-4f9f-adfd-bbc8e5ad8fdd", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c52c881-b5ce-4d79-9ea6-3f3d97441844", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "36b98c48fcd0c209f46650e9ff942381d2cb32fd56d6d4e503f62f7a7e683903", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "421aaa4f-7ad1-47b4-9bff-f330df2dffd4", "node_type": "1", "metadata": {}, "hash": "299bf851100dae3c7a452e53485ba15ccb3568ce129c48494cf0831ea4a91ce4", "class_name": "RelatedNodeInfo"}}, "text": "Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit: split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\n\r\nreturns:\r\nDataPipe that yields tuple of label (1 to 5) and text containing the news title and contents\r\n\r\nrtype:\r\n(int, str)\r\n\r\n\r\n\r\n\r\nSogouNews Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\nNumber of lines per split:\r\n\r\ntrain: 450000\r\ntest: 60000\r\n\r\n\r\ntrain: 450000\r\ntest: 60000\r\nArgs:\r\nroot: Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit: split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\nroot: Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit: split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\nreturns:\r\nDataPipe that yields tuple of label (1 to 5) and text containing the news title and contents\r\n\r\nDataPipe that yields tuple of label (1 to 5) and text containing the news title and contents\r\nrtype:\r\n(int, str)\r\n\r\n(int, str)\r\nSST2\u00b6\r\n\r\ntorchtext.datasets.SST2(root='.data', split=('train', 'dev', 'test'))[source]\u00b6\r\nSST2 Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://nlp.stanford.edu/sentiment/\r\n\r\nNumber of lines per split:\r\ntrain: 67349\r\ndev: 872\r\ntest: 1821\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of text and/or label (1 to 4). The test split only returns text.\r\n\r\nReturn type:\r\nUnion[(int, str), (str,)]\r\n\r\n\r\n\r\nTutorials using SST2:\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\n\r\n\r\nSST2 Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://nlp.stanford.edu/sentiment/\r\nNumber of lines per split:\r\n\r\ntrain: 67349\r\ndev: 872\r\ntest: 1821\r\n\r\n\r\ntrain: 67349\r\ndev: 872\r\ntest: 1821\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\nReturns:\r\nDataPipe that yields tuple of text and/or label (1 to 4). The test split only returns text.\r\n\r\nDataPipe that yields tuple of text and/or label (1 to 4). The test split only returns text.\r\nReturn type:\r\nUnion[(int, str), (str,)]\r\n\r\nUnion[(int, str), (str,)]\r\nTutorials using SST2:\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSTSB\u00b6\r\n\r\ntorchtext.datasets.STSB(root='.data', split=('train', 'dev', 'test'))[source]\u00b6\r\nSTSB Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.", "mimetype": "text/plain", "start_char_idx": 67797, "end_char_idx": 71701, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "421aaa4f-7ad1-47b4-9bff-f330df2dffd4": {"__data__": {"id_": "421aaa4f-7ad1-47b4-9bff-f330df2dffd4", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55a675a8-2c34-4f9f-adfd-bbc8e5ad8fdd", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "e022ebeab3acc5bf233a9a6e620bf7daa726f1af5a7328b3f2d83234044c01b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae9e09b0-7373-4230-9bb2-86a20ab091f1", "node_type": "1", "metadata": {}, "hash": "ad3bf2566db640fb67c137a969047a1734f5eb31d8352bbb78ac56421ddbd728", "class_name": "RelatedNodeInfo"}}, "text": "The test split only returns text.\r\n\r\nDataPipe that yields tuple of text and/or label (1 to 4). The test split only returns text.\r\nReturn type:\r\nUnion[(int, str), (str,)]\r\n\r\nUnion[(int, str), (str,)]\r\nTutorials using SST2:\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSTSB\u00b6\r\n\r\ntorchtext.datasets.STSB(root='.data', split=('train', 'dev', 'test'))[source]\u00b6\r\nSTSB Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark\r\n\r\nNumber of lines per split:\r\ntrain: 5749\r\ndev: 1500\r\ntest: 1379\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of (index (int), label (float), sentence1 (str), sentence2 (str))\r\n\r\nReturn type:\r\n(int, float, str, str)\r\n\r\n\r\n\r\nSTSB Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark\r\nNumber of lines per split:\r\n\r\ntrain: 5749\r\ndev: 1500\r\ntest: 1379\r\n\r\n\r\ntrain: 5749\r\ndev: 1500\r\ntest: 1379\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\nReturns:\r\nDataPipe that yields tuple of (index (int), label (float), sentence1 (str), sentence2 (str))\r\n\r\nDataPipe that yields tuple of (index (int), label (float), sentence1 (str), sentence2 (str))\r\nReturn type:\r\n(int, float, str, str)\r\n\r\n(int, float, str, str)\r\nWNLI\u00b6\r\n\r\ntorchtext.datasets.WNLI(root='.data', split=('train', 'dev', 'test'))[source]\u00b6\r\nWNLI Dataset\r\nFor additional details refer to https://arxiv.org/pdf/1804.07461v3.pdf\r\n\r\nNumber of lines per split:\r\ntrain: 635\r\ndev: 71\r\ntest: 146\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of text and/or label (0 to 1). The test split only returns text.\r\n\r\nReturn type:\r\nUnion[(int, str, str), (str, str)]\r\n\r\n\r\n\r\nWNLI Dataset\r\nFor additional details refer to https://arxiv.org/pdf/1804.07461v3.pdf\r\nNumber of lines per split:\r\n\r\ntrain: 635\r\ndev: 71\r\ntest: 146\r\n\r\n\r\ntrain: 635\r\ndev: 71\r\ntest: 146\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\nReturns:\r\nDataPipe that yields tuple of text and/or label (0 to 1). The test split only returns text.\r\n\r\nDataPipe that yields tuple of text and/or label (0 to 1).", "mimetype": "text/plain", "start_char_idx": 70988, "end_char_idx": 74753, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae9e09b0-7373-4230-9bb2-86a20ab091f1": {"__data__": {"id_": "ae9e09b0-7373-4230-9bb2-86a20ab091f1", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "421aaa4f-7ad1-47b4-9bff-f330df2dffd4", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "ec11ecc219cb2a77ef14635d28c9ebe72eb27c944c29b2d014bbeae2a1e51f1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0689df79-d968-47e4-85fc-532331bff96e", "node_type": "1", "metadata": {}, "hash": "5d56a0b60ba336a347c863793127ae760a654e73e8660093b23818bdad77b27e", "class_name": "RelatedNodeInfo"}}, "text": "Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev, test)\r\nReturns:\r\nDataPipe that yields tuple of text and/or label (0 to 1). The test split only returns text.\r\n\r\nDataPipe that yields tuple of text and/or label (0 to 1). The test split only returns text.\r\nReturn type:\r\nUnion[(int, str, str), (str, str)]\r\n\r\nUnion[(int, str, str), (str, str)]\r\nYahooAnswers\u00b6\r\n\r\ntorchtext.datasets.YahooAnswers(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'test'))[source]\u00b6\r\nYahooAnswers Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\n\r\nNumber of lines per split:\r\ntrain: 1400000\r\ntest: 60000\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 10) and text containing the question title, question\r\ncontent, and best answer\r\n\r\nReturn type:\r\n(int, str)\r\n\r\n\r\n\r\nYahooAnswers Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\nNumber of lines per split:\r\n\r\ntrain: 1400000\r\ntest: 60000\r\n\r\n\r\ntrain: 1400000\r\ntest: 60000\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 10) and text containing the question title, question\r\ncontent, and best answer\r\n\r\nDataPipe that yields tuple of label (1 to 10) and text containing the question title, question\r\ncontent, and best answer\r\nReturn type:\r\n(int, str)\r\n\r\n(int, str)\r\nYelpReviewFull\u00b6\r\n\r\ntorchtext.datasets.YelpReviewFull(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'test'))[source]\u00b6\r\nYelpReviewFull Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\n\r\nNumber of lines per split:\r\ntrain: 650000\r\ntest: 50000\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 5) and text containing the review\r\n\r\nReturn type:\r\n(int, str)\r\n\r\n\r\n\r\nYelpReviewFull Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\nNumber of lines per split:\r\n\r\ntrain: 650000\r\ntest: 50000\r\n\r\n\r\ntrain: 650000\r\ntest: 50000\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved.", "mimetype": "text/plain", "start_char_idx": 74226, "end_char_idx": 78167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0689df79-d968-47e4-85fc-532331bff96e": {"__data__": {"id_": "0689df79-d968-47e4-85fc-532331bff96e", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae9e09b0-7373-4230-9bb2-86a20ab091f1", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "b8bf4c9520bf4ab5b1be0ef1c320f536eee1a15f1f3f25a25033152617097db4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5d4dfd2-32e3-4e93-8da2-97575adec23c", "node_type": "1", "metadata": {}, "hash": "a8b4a99f24a3c7414eddeb9849ad91fbd8b93e1285acc409e06674c4b26541cd", "class_name": "RelatedNodeInfo"}}, "text": "Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 5) and text containing the review\r\n\r\nReturn type:\r\n(int, str)\r\n\r\n\r\n\r\nYelpReviewFull Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\nNumber of lines per split:\r\n\r\ntrain: 650000\r\ntest: 50000\r\n\r\n\r\ntrain: 650000\r\ntest: 50000\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 5) and text containing the review\r\n\r\nDataPipe that yields tuple of label (1 to 5) and text containing the review\r\nReturn type:\r\n(int, str)\r\n\r\n(int, str)\r\nYelpReviewPolarity\u00b6\r\n\r\ntorchtext.datasets.YelpReviewPolarity(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'test'))[source]\u00b6\r\nYelpReviewPolarity Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\n\r\nNumber of lines per split:\r\ntrain: 560000\r\ntest: 38000\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 2) and text containing the review\r\n\r\nReturn type:\r\n(int, str)\r\n\r\n\r\n\r\nYelpReviewPolarity Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://arxiv.org/abs/1509.01626\r\nNumber of lines per split:\r\n\r\ntrain: 560000\r\ntest: 38000\r\n\r\n\r\ntrain: 560000\r\ntest: 38000\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\nReturns:\r\nDataPipe that yields tuple of label (1 to 2) and text containing the review\r\n\r\nDataPipe that yields tuple of label (1 to 2) and text containing the review\r\nReturn type:\r\n(int, str)\r\n\r\n(int, str)\r\nLanguage Modeling\u00b6\r\nPennTreebank\u00b6\r\n\r\ntorchtext.datasets.PennTreebank(root='.data', split: Union[Tuple[str], str] = ('train', 'valid', 'test'))[source]\u00b6\r\nPennTreebank Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html\r\n\r\nNumber of lines per split:\r\ntrain: 42068\r\nvalid: 3370\r\ntest: 3761\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings.", "mimetype": "text/plain", "start_char_idx": 77437, "end_char_idx": 81349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5d4dfd2-32e3-4e93-8da2-97575adec23c": {"__data__": {"id_": "f5d4dfd2-32e3-4e93-8da2-97575adec23c", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0689df79-d968-47e4-85fc-532331bff96e", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "ebc89226bdf482985423a897dde5922a47dbe9b84779c80249f0467b11424d7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a55c566e-ded2-44c7-b36e-061f82aad713", "node_type": "1", "metadata": {}, "hash": "7589ae305d8d3e4f381118e2b7ca938f51416a10b459fd451bd29ddef00c232d", "class_name": "RelatedNodeInfo"}}, "text": "if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html\r\n\r\nNumber of lines per split:\r\ntrain: 42068\r\nvalid: 3370\r\ntest: 3761\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, valid, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields text from the Treebank corpus\r\n\r\nReturn type:\r\nstr\r\n\r\n\r\n\r\nPennTreebank Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html\r\nNumber of lines per split:\r\n\r\ntrain: 42068\r\nvalid: 3370\r\ntest: 3761\r\n\r\n\r\ntrain: 42068\r\nvalid: 3370\r\ntest: 3761\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, valid, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, valid, test)\r\nReturns:\r\nDataPipe that yields text from the Treebank corpus\r\n\r\nDataPipe that yields text from the Treebank corpus\r\nReturn type:\r\nstr\r\n\r\nstr\r\nWikiText-2\u00b6\r\n\r\ntorchtext.datasets.WikiText2(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'valid', 'test'))[source]\u00b6\r\nWikiText2 Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/\r\n\r\nNumber of lines per split:\r\ntrain: 36718\r\nvalid: 3760\r\ntest: 4358\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, valid, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields text from Wikipedia articles\r\n\r\nReturn type:\r\nstr\r\n\r\n\r\n\r\nWikiText2 Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/\r\nNumber of lines per split:\r\n\r\ntrain: 36718\r\nvalid: 3760\r\ntest: 4358\r\n\r\n\r\ntrain: 36718\r\nvalid: 3760\r\ntest: 4358\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, valid, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, valid, test)\r\nReturns:\r\nDataPipe that yields text from Wikipedia articles\r\n\r\nDataPipe that yields text from Wikipedia articles\r\nReturn type:\r\nstr\r\n\r\nstr\r\nWikiText103\u00b6\r\n\r\ntorchtext.datasets.WikiText103(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'valid', 'test'))[source]\u00b6\r\nWikiText103 Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.", "mimetype": "text/plain", "start_char_idx": 80853, "end_char_idx": 84788, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a55c566e-ded2-44c7-b36e-061f82aad713": {"__data__": {"id_": "a55c566e-ded2-44c7-b36e-061f82aad713", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5d4dfd2-32e3-4e93-8da2-97575adec23c", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "20b5592b3fe23dd365d3a73684fdbd25590322db5a428857b6d71f13a4b7e0fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02cd274e-3074-4dac-a2a6-5c04d705c5ad", "node_type": "1", "metadata": {}, "hash": "95e24234bb36fc0d7bd87833fb3f424366bff3b65a0d79137db8457a6aee5acc", "class_name": "RelatedNodeInfo"}}, "text": "Can be a string or tuple of strings. Default: (train, valid, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, valid, test)\r\nReturns:\r\nDataPipe that yields text from Wikipedia articles\r\n\r\nDataPipe that yields text from Wikipedia articles\r\nReturn type:\r\nstr\r\n\r\nstr\r\nWikiText103\u00b6\r\n\r\ntorchtext.datasets.WikiText103(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'valid', 'test'))[source]\u00b6\r\nWikiText103 Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/\r\n\r\nNumber of lines per split:\r\ntrain: 1801350\r\nvalid: 3760\r\ntest: 4358\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, valid, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields text from Wikipedia articles\r\n\r\nReturn type:\r\nstr\r\n\r\n\r\n\r\nWikiText103 Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/\r\nNumber of lines per split:\r\n\r\ntrain: 1801350\r\nvalid: 3760\r\ntest: 4358\r\n\r\n\r\ntrain: 1801350\r\nvalid: 3760\r\ntest: 4358\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, valid, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, valid, test)\r\nReturns:\r\nDataPipe that yields text from Wikipedia articles\r\n\r\nDataPipe that yields text from Wikipedia articles\r\nReturn type:\r\nstr\r\n\r\nstr\r\nMachine Translation\u00b6\r\nIWSLT2016\u00b6\r\n\r\ntorchtext.datasets.IWSLT2016(root='.data', split=('train', 'valid', 'test'), language_pair=('de', 'en'), valid_set='tst2013', test_set='tst2014')[source]\u00b6\r\nIWSLT2016 dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://wit3.fbk.eu/2016-01\r\nThe available datasets include following:\r\nLanguage pairs:\r\n\r\n\r\n\r\n\u201cen\u201d\r\n\u201cfr\u201d\r\n\u201cde\u201d\r\n\u201ccs\u201d\r\n\u201car\u201d\r\n\r\n\u201cen\u201d\r\n\r\nx\r\nx\r\nx\r\nx\r\n\r\n\u201cfr\u201d\r\nx\r\n\r\n\r\n\r\n\r\n\r\n\u201cde\u201d\r\nx\r\n\r\n\r\n\r\n\r\n\r\n\u201ccs\u201d\r\nx\r\n\r\n\r\n\r\n\r\n\r\n\u201car\u201d\r\nx\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nvalid/test sets: [\u201cdev2010\u201d, \u201ctst2010\u201d, \u201ctst2011\u201d, \u201ctst2012\u201d, \u201ctst2013\u201d, \u201ctst2014\u201d]\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (\u2018train\u2019, \u2018valid\u2019, \u2018test\u2019)\r\nlanguage_pair \u2013 tuple or list containing src and tgt language\r\nvalid_set \u2013 a string to identify validation set.\r\ntest_set \u2013 a string to identify test set.\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of source and target sentences\r\n\r\nReturn type:\r\n(str, str)\r\n\r\n\r\nExamples\r\n>>> from torchtext.datasets import IWSLT2016\r\n>>> train_iter, valid_iter, test_iter = IWSLT2016()\r\n>>> src_sentence, tgt_sentence = next(iter(train_iter))\r\n\r\n\r\n\r\nIWSLT2016 dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats.", "mimetype": "text/plain", "start_char_idx": 83999, "end_char_idx": 87891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02cd274e-3074-4dac-a2a6-5c04d705c5ad": {"__data__": {"id_": "02cd274e-3074-4dac-a2a6-5c04d705c5ad", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a55c566e-ded2-44c7-b36e-061f82aad713", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "978cf3b9276f4fc2780f6de6f32b9ce30b0ff5ed1cbc8ec05a0077b2663172a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "827479cb-6872-498c-8cbf-61fa5c524d19", "node_type": "1", "metadata": {}, "hash": "2796439d26e74c0d6f16dd26c909aaf6b9f192eefa6cb05699ed1f4a8925a26b", "class_name": "RelatedNodeInfo"}}, "text": "Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (\u2018train\u2019, \u2018valid\u2019, \u2018test\u2019)\r\nlanguage_pair \u2013 tuple or list containing src and tgt language\r\nvalid_set \u2013 a string to identify validation set.\r\ntest_set \u2013 a string to identify test set.\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of source and target sentences\r\n\r\nReturn type:\r\n(str, str)\r\n\r\n\r\nExamples\r\n>>> from torchtext.datasets import IWSLT2016\r\n>>> train_iter, valid_iter, test_iter = IWSLT2016()\r\n>>> src_sentence, tgt_sentence = next(iter(train_iter))\r\n\r\n\r\n\r\nIWSLT2016 dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://wit3.fbk.eu/2016-01\r\nThe available datasets include following:\r\nLanguage pairs:\r\n\u201cen\u201d\r\n\u201cfr\u201d\r\n\u201cde\u201d\r\n\u201ccs\u201d\r\n\u201car\u201d\r\n\u201cen\u201d\r\nx\r\nx\r\nx\r\nx\r\n\u201cfr\u201d\r\nx\r\n\u201cde\u201d\r\nx\r\n\u201ccs\u201d\r\nx\r\n\u201car\u201d\r\nx\r\nvalid/test sets: [\u201cdev2010\u201d, \u201ctst2010\u201d, \u201ctst2011\u201d, \u201ctst2012\u201d, \u201ctst2013\u201d, \u201ctst2014\u201d]\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (\u2018train\u2019, \u2018valid\u2019, \u2018test\u2019)\r\nlanguage_pair \u2013 tuple or list containing src and tgt language\r\nvalid_set \u2013 a string to identify validation set.\r\ntest_set \u2013 a string to identify test set.\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (\u2018train\u2019, \u2018valid\u2019, \u2018test\u2019)\r\nlanguage_pair \u2013 tuple or list containing src and tgt language\r\nvalid_set \u2013 a string to identify validation set.\r\ntest_set \u2013 a string to identify test set.\r\nReturns:\r\nDataPipe that yields tuple of source and target sentences\r\n\r\nDataPipe that yields tuple of source and target sentences\r\nReturn type:\r\n(str, str)\r\n\r\n(str, str)\r\nExamples\r\n>>> from torchtext.datasets import IWSLT2016\r\n>>> train_iter, valid_iter, test_iter = IWSLT2016()\r\n>>> src_sentence, tgt_sentence = next(iter(train_iter))\r\n\r\nIWSLT2017\u00b6\r\n\r\ntorchtext.datasets.IWSLT2017(root='.data', split=('train', 'valid', 'test'), language_pair=('de', 'en'))[source]\u00b6\r\nIWSLT2017 dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://wit3.fbk.eu/2017-01\r\nThe available datasets include following:\r\nLanguage pairs:\r\n\r\n\r\n\r\n\u201cen\u201d\r\n\u201cnl\u201d\r\n\u201cde\u201d\r\n\u201cit\u201d\r\n\u201cro\u201d\r\n\r\n\u201cen\u201d\r\n\r\nx\r\nx\r\nx\r\nx\r\n\r\n\u201cnl\u201d\r\nx\r\n\r\nx\r\nx\r\nx\r\n\r\n\u201cde\u201d\r\nx\r\nx\r\n\r\nx\r\nx\r\n\r\n\u201cit\u201d\r\nx\r\nx\r\nx\r\n\r\nx\r\n\r\n\u201cro\u201d\r\nx\r\nx\r\nx\r\nx\r\n\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (\u2018train\u2019, \u2018valid\u2019, \u2018test\u2019)\r\nlanguage_pair \u2013 tuple or list containing src and tgt language\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of source and target sentences\r\n\r\nReturn type:\r\n(str, str)\r\n\r\n\r\nExamples\r\n>>> from torchtext.datasets import IWSLT2017\r\n>>> train_iter, valid_iter, test_iter = IWSLT2017()\r\n>>> src_sentence, tgt_sentence = next(iter(train_iter))\r\n\r\n\r\n\r\nIWSLT2017 dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.", "mimetype": "text/plain", "start_char_idx": 87205, "end_char_idx": 90852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "827479cb-6872-498c-8cbf-61fa5c524d19": {"__data__": {"id_": "827479cb-6872-498c-8cbf-61fa5c524d19", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02cd274e-3074-4dac-a2a6-5c04d705c5ad", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "e01c7b33e50c9612ee9ef690afcc7f47c58500a80e3b43a369d8bee082f0136e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0de8490c-abed-4e40-9875-2542ddd06182", "node_type": "1", "metadata": {}, "hash": "4efa6d366197340358eb23e571e9571fb45b15a0009fc417f4df071365001803", "class_name": "RelatedNodeInfo"}}, "text": "Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (\u2018train\u2019, \u2018valid\u2019, \u2018test\u2019)\r\nlanguage_pair \u2013 tuple or list containing src and tgt language\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of source and target sentences\r\n\r\nReturn type:\r\n(str, str)\r\n\r\n\r\nExamples\r\n>>> from torchtext.datasets import IWSLT2017\r\n>>> train_iter, valid_iter, test_iter = IWSLT2017()\r\n>>> src_sentence, tgt_sentence = next(iter(train_iter))\r\n\r\n\r\n\r\nIWSLT2017 dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://wit3.fbk.eu/2017-01\r\nThe available datasets include following:\r\nLanguage pairs:\r\n\u201cen\u201d\r\n\u201cnl\u201d\r\n\u201cde\u201d\r\n\u201cit\u201d\r\n\u201cro\u201d\r\n\u201cen\u201d\r\nx\r\nx\r\nx\r\nx\r\n\u201cnl\u201d\r\nx\r\nx\r\nx\r\nx\r\n\u201cde\u201d\r\nx\r\nx\r\nx\r\nx\r\n\u201cit\u201d\r\nx\r\nx\r\nx\r\nx\r\n\u201cro\u201d\r\nx\r\nx\r\nx\r\nx\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (\u2018train\u2019, \u2018valid\u2019, \u2018test\u2019)\r\nlanguage_pair \u2013 tuple or list containing src and tgt language\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (\u2018train\u2019, \u2018valid\u2019, \u2018test\u2019)\r\nlanguage_pair \u2013 tuple or list containing src and tgt language\r\nReturns:\r\nDataPipe that yields tuple of source and target sentences\r\n\r\nDataPipe that yields tuple of source and target sentences\r\nReturn type:\r\n(str, str)\r\n\r\n(str, str)\r\nExamples\r\n>>> from torchtext.datasets import IWSLT2017\r\n>>> train_iter, valid_iter, test_iter = IWSLT2017()\r\n>>> src_sentence, tgt_sentence = next(iter(train_iter))\r\n\r\nMulti30k\u00b6\r\n\r\ntorchtext.datasets.Multi30k(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'valid', 'test'), language_pair: Tuple[str] = ('de', 'en'))[source]\u00b6\r\nMulti30k dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://www.statmt.org/wmt16/multimodal-task.html#task1\r\n\r\nNumber of lines per split:\r\ntrain: 29000\r\nvalid: 1014\r\ntest: 1000\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (\u2018train\u2019, \u2018valid\u2019, \u2018test\u2019)\r\nlanguage_pair \u2013 tuple or list containing src and tgt language. Available options are (\u2018de\u2019,\u2019en\u2019) and (\u2018en\u2019, \u2018de\u2019)\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of source and target sentences\r\n\r\nReturn type:\r\n(str, str)\r\n\r\n\r\n\r\nTutorials using Multi30k:\r\nT5-Base Model for Summarization, Sentiment Classification, and Translation\r\nT5-Base Model for Summarization, Sentiment Classification, and Translation\r\n\r\n\r\n\r\nMulti30k dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://www.statmt.org/wmt16/multimodal-task.html#task1\r\nNumber of lines per split:\r\n\r\ntrain: 29000\r\nvalid: 1014\r\ntest: 1000\r\n\r\n\r\ntrain: 29000\r\nvalid: 1014\r\ntest: 1000\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (\u2018train\u2019, \u2018valid\u2019, \u2018test\u2019)\r\nlanguage_pair \u2013 tuple or list containing src and tgt language.", "mimetype": "text/plain", "start_char_idx": 90118, "end_char_idx": 93930, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0de8490c-abed-4e40-9875-2542ddd06182": {"__data__": {"id_": "0de8490c-abed-4e40-9875-2542ddd06182", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "827479cb-6872-498c-8cbf-61fa5c524d19", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "e776c73de9bf9227b0a974bfcfa2da52d4fd824fdc84df65be0e67627aaa4cac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba8be7fd-b71f-4059-aa52-3370a5d560d3", "node_type": "1", "metadata": {}, "hash": "98e3a0f5bea4670a03d10800580fe6353a38c3bf4b79dcc71856d5bd100146e8", "class_name": "RelatedNodeInfo"}}, "text": "if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://www.statmt.org/wmt16/multimodal-task.html#task1\r\nNumber of lines per split:\r\n\r\ntrain: 29000\r\nvalid: 1014\r\ntest: 1000\r\n\r\n\r\ntrain: 29000\r\nvalid: 1014\r\ntest: 1000\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (\u2018train\u2019, \u2018valid\u2019, \u2018test\u2019)\r\nlanguage_pair \u2013 tuple or list containing src and tgt language. Available options are (\u2018de\u2019,\u2019en\u2019) and (\u2018en\u2019, \u2018de\u2019)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (\u2018train\u2019, \u2018valid\u2019, \u2018test\u2019)\r\nlanguage_pair \u2013 tuple or list containing src and tgt language. Available options are (\u2018de\u2019,\u2019en\u2019) and (\u2018en\u2019, \u2018de\u2019)\r\nReturns:\r\nDataPipe that yields tuple of source and target sentences\r\n\r\nDataPipe that yields tuple of source and target sentences\r\nReturn type:\r\n(str, str)\r\n\r\n(str, str)\r\nTutorials using Multi30k:\r\n\r\nT5-Base Model for Summarization, Sentiment Classification, and Translation\r\nT5-Base Model for Summarization, Sentiment Classification, and Translation\r\n\r\nT5-Base Model for Summarization, Sentiment Classification, and Translation\r\nSequence Tagging\u00b6\r\nCoNLL2000Chunking\u00b6\r\n\r\ntorchtext.datasets.CoNLL2000Chunking(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'test'))[source]\u00b6\r\nCoNLL2000Chunking Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://www.clips.uantwerpen.be/conll2000/chunking/\r\n\r\nNumber of lines per split:\r\ntrain: 8936\r\ntest: 2012\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields list of words along with corresponding Parts-of-speech tag and chunk tag\r\n\r\nReturn type:\r\n[list(str), list(str), list(str)]\r\n\r\n\r\n\r\nCoNLL2000Chunking Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://www.clips.uantwerpen.be/conll2000/chunking/\r\nNumber of lines per split:\r\n\r\ntrain: 8936\r\ntest: 2012\r\n\r\n\r\ntrain: 8936\r\ntest: 2012\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, test)\r\nReturns:\r\nDataPipe that yields list of words along with corresponding Parts-of-speech tag and chunk tag\r\n\r\nDataPipe that yields list of words along with corresponding Parts-of-speech tag and chunk tag\r\nReturn type:\r\n[list(str), list(str), list(str)]\r\n\r\n[list(str), list(str), list(str)]\r\nUDPOS\u00b6\r\n\r\ntorchtext.datasets.UDPOS(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'valid', 'test'))[source]\u00b6\r\nUDPOS Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\n\r\nNumber of lines per split:\r\ntrain: 12543\r\nvalid: 2002\r\ntest: 2077\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved.", "mimetype": "text/plain", "start_char_idx": 93298, "end_char_idx": 97282, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba8be7fd-b71f-4059-aa52-3370a5d560d3": {"__data__": {"id_": "ba8be7fd-b71f-4059-aa52-3370a5d560d3", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0de8490c-abed-4e40-9875-2542ddd06182", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "275e03b928fe4ae4379225ea6231d38dbbf55817e6ab046bbf789e7bed50d017", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f28292fb-4254-42b0-9560-78964ac5b5bd", "node_type": "1", "metadata": {}, "hash": "282e99155f99e2772978026d72510eed27baa4136849744f7174bf9f9ed1d815", "class_name": "RelatedNodeInfo"}}, "text": "Default: (train, test)\r\nReturns:\r\nDataPipe that yields list of words along with corresponding Parts-of-speech tag and chunk tag\r\n\r\nDataPipe that yields list of words along with corresponding Parts-of-speech tag and chunk tag\r\nReturn type:\r\n[list(str), list(str), list(str)]\r\n\r\n[list(str), list(str), list(str)]\r\nUDPOS\u00b6\r\n\r\ntorchtext.datasets.UDPOS(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'valid', 'test'))[source]\u00b6\r\nUDPOS Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\n\r\nNumber of lines per split:\r\ntrain: 12543\r\nvalid: 2002\r\ntest: 2077\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, valid, test)\r\n\r\n\r\nReturns:\r\nDataPipe that yields list of words along with corresponding parts-of-speech tags\r\n\r\nReturn type:\r\n[list(str), list(str)]\r\n\r\n\r\n\r\nUDPOS Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nNumber of lines per split:\r\n\r\ntrain: 12543\r\nvalid: 2002\r\ntest: 2077\r\n\r\n\r\ntrain: 12543\r\nvalid: 2002\r\ntest: 2077\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, valid, test)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, valid, test)\r\nReturns:\r\nDataPipe that yields list of words along with corresponding parts-of-speech tags\r\n\r\nDataPipe that yields list of words along with corresponding parts-of-speech tags\r\nReturn type:\r\n[list(str), list(str)]\r\n\r\n[list(str), list(str)]\r\nQuestion Answer\u00b6\r\nSQuAD 1.0\u00b6\r\n\r\ntorchtext.datasets.SQuAD1(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'dev'))[source]\u00b6\r\nSQuAD1 Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://rajpurkar.github.io/SQuAD-explorer/\r\n\r\nNumber of lines per split:\r\ntrain: 87599\r\ndev: 10570\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev)\r\n\r\n\r\nReturns:\r\nDataPipe that yields data points from SQuaAD1 dataset which consist of context, question, list of answers and corresponding index in context\r\n\r\nReturn type:\r\n(str, str, list(str), list(int))\r\n\r\n\r\n\r\nSQuAD1 Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://rajpurkar.github.io/SQuAD-explorer/\r\nNumber of lines per split:\r\n\r\ntrain: 87599\r\ndev: 10570\r\n\r\n\r\ntrain: 87599\r\ndev: 10570\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings.", "mimetype": "text/plain", "start_char_idx": 96475, "end_char_idx": 100313, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f28292fb-4254-42b0-9560-78964ac5b5bd": {"__data__": {"id_": "f28292fb-4254-42b0-9560-78964ac5b5bd", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba8be7fd-b71f-4059-aa52-3370a5d560d3", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "f92e3d4bf93d29872c54fcff06804cccaec96ebd0df2000d0df6c9c3b4fd8a40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7c47131-e38c-4f01-aa72-92b29686df5c", "node_type": "1", "metadata": {}, "hash": "bb7407261e257d855f607e1d8fad140eb0947d537ff49929a0dfc69d4c3396fa", "class_name": "RelatedNodeInfo"}}, "text": "if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://rajpurkar.github.io/SQuAD-explorer/\r\nNumber of lines per split:\r\n\r\ntrain: 87599\r\ndev: 10570\r\n\r\n\r\ntrain: 87599\r\ndev: 10570\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev)\r\nReturns:\r\nDataPipe that yields data points from SQuaAD1 dataset which consist of context, question, list of answers and corresponding index in context\r\n\r\nDataPipe that yields data points from SQuaAD1 dataset which consist of context, question, list of answers and corresponding index in context\r\nReturn type:\r\n(str, str, list(str), list(int))\r\n\r\n(str, str, list(str), list(int))\r\nSQuAD 2.0\u00b6\r\n\r\ntorchtext.datasets.SQuAD2(root: str = '.data', split: Union[Tuple[str], str] = ('train', 'dev'))[source]\u00b6\r\nSQuAD2 Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://rajpurkar.github.io/SQuAD-explorer/\r\n\r\nNumber of lines per split:\r\ntrain: 130319\r\ndev: 11873\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev)\r\n\r\n\r\nReturns:\r\nDataPipe that yields data points from SQuaAD1 dataset which consist of context, question, list of answers and corresponding index in context\r\n\r\nReturn type:\r\n(str, str, list(str), list(int))\r\n\r\n\r\n\r\nSQuAD2 Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://rajpurkar.github.io/SQuAD-explorer/\r\nNumber of lines per split:\r\n\r\ntrain: 130319\r\ndev: 11873\r\n\r\n\r\ntrain: 130319\r\ndev: 11873\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev)\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nsplit \u2013 split or splits to be returned. Can be a string or tuple of strings. Default: (train, dev)\r\nReturns:\r\nDataPipe that yields data points from SQuaAD1 dataset which consist of context, question, list of answers and corresponding index in context\r\n\r\nDataPipe that yields data points from SQuaAD1 dataset which consist of context, question, list of answers and corresponding index in context\r\nReturn type:\r\n(str, str, list(str), list(int))\r\n\r\n(str, str, list(str), list(int))\r\nUnsupervised Learning\u00b6\r\nCC100\u00b6\r\n\r\ntorchtext.datasets.CC100(root: str, language_code: str = 'en')[source]\u00b6\r\nCC100 Dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://data.statmt.org/cc-100/\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nlanguage_code \u2013 the language of the dataset\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of language code and text\r\n\r\nReturn type:\r\n(str, str)\r\n\r\n\r\n\r\nCC100 Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://data.statmt.org/cc-100/\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved.", "mimetype": "text/plain", "start_char_idx": 99617, "end_char_idx": 103843, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7c47131-e38c-4f01-aa72-92b29686df5c": {"__data__": {"id_": "e7c47131-e38c-4f01-aa72-92b29686df5c", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f28292fb-4254-42b0-9560-78964ac5b5bd", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "8c5b86ef75cb2ecd322d3991e6a2bb80ce74c03bf720f6b7db443ba0a1b744c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a55f34b5-bee4-485c-95c2-8f43a072c6f3", "node_type": "1", "metadata": {}, "hash": "de947623807bfdfd70b5fe43a7462dd19a3abff1489820c11ead882d43c299f0", "class_name": "RelatedNodeInfo"}}, "text": "if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to https://data.statmt.org/cc-100/\r\n\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nlanguage_code \u2013 the language of the dataset\r\n\r\n\r\nReturns:\r\nDataPipe that yields tuple of language code and text\r\n\r\nReturn type:\r\n(str, str)\r\n\r\n\r\n\r\nCC100 Dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to https://data.statmt.org/cc-100/\r\nParameters:\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nlanguage_code \u2013 the language of the dataset\r\n\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nlanguage_code \u2013 the language of the dataset\r\nReturns:\r\nDataPipe that yields tuple of language code and text\r\n\r\nDataPipe that yields tuple of language code and text\r\nReturn type:\r\n(str, str)\r\n\r\n(str, str)\r\nEnWik9\u00b6\r\n\r\ntorchtext.datasets.EnWik9(root: str)[source]\u00b6\r\nEnWik9 dataset\r\n\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\n\r\nFor additional details refer to http://mattmahoney.net/dc/textdata.html\r\nNumber of lines in dataset: 13147026\r\n\r\nParameters:\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\n\r\nReturns:\r\nDataPipe that yields raw text rows from WnWik9 dataset\r\n\r\nReturn type:\r\nstr\r\n\r\n\r\n\r\nEnWik9 dataset\r\nWarning\r\nusing datapipes is still currently subject to a few caveats. if you wish\r\nto use this dataset with shuffling, multi-processing, or distributed\r\nlearning, please see this note for further\r\ninstructions.\r\nFor additional details refer to http://mattmahoney.net/dc/textdata.html\r\nNumber of lines in dataset: 13147026\r\nParameters:\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\n\r\nroot \u2013 Directory where the datasets are saved. Default: os.path.expanduser(\u2018~/.torchtext/cache\u2019)\r\nReturns:\r\nDataPipe that yields raw text rows from WnWik9 dataset\r\n\r\nDataPipe that yields raw text rows from WnWik9 dataset\r\nReturn type:\r\nstr\r\n\r\nstr \r\n\r\nsource: https://pytorch.org/text/stable/vocab.html \r\ncontent: \r\n\r\ntorchtext.vocab\u00b6\r\nVocab\u00b6\r\n\r\nclass torchtext.vocab.Vocab(vocab)[source]\u00b6\r\n\r\n\r\n__contains__(token: str) \u2192 bool[source]\u00b6\r\n\r\nParameters:\r\ntoken \u2013 The token for which to check the membership.\r\n\r\nReturns:\r\nWhether the token is member of vocab or not.\r\n\r\n\r\n\r\n\r\n\r\n__getitem__(token: str) \u2192 int[source]\u00b6\r\n\r\nParameters:\r\ntoken \u2013 The token used to lookup the corresponding index.\r\n\r\nReturns:\r\nThe index corresponding to the associated token.\r\n\r\n\r\n\r\n\r\n\r\n__init__(vocab) \u2192 None[source]\u00b6\r\nInitialize internal Module state, shared by both nn.Module and ScriptModule.\r\n\r\n\r\n\r\n__jit_unused_properties__ = ['is_jitable']\u00b6\r\nCreates a vocab object which maps tokens to indices.\r\n\r\nParameters:\r\nvocab (torch.classes.torchtext.Vocab or torchtext._torchtext.Vocab) \u2013 a cpp vocab object.\r\n\r\n\r\n\r\n\r\n\r\n__len__() \u2192 int[source]\u00b6\r\n\r\nReturns:\r\nThe length of the vocab.\r\n\r\n\r\n\r\n\r\n\r\n__prepare_scriptable__()[source]\u00b6\r\nReturn a JITable Vocab.\r\n\r\n\r\n\r\nappend_token(token: str) \u2192 None[source]\u00b6\r\n\r\nParameters:\r\ntoken \u2013 The token used to lookup the corresponding index.\r\n\r\nRaises:\r\nRuntimeError \u2013 If token already exists in the vocab\r\n\r\n\r\n\r\n\r\n\r\nforward(tokens: List[str]) \u2192 List[int][source]\u00b6\r\nCalls the lookup_indices method\r\n\r\nParameters:\r\ntokens \u2013 a list of tokens used to lookup their corresponding indices.\r\n\r\nReturns:\r\nThe indices associated with a list of tokens.\r\n\r\n\r\n\r\n\r\n\r\nget_default_index() \u2192 Optional[int][source]\u00b6\r\n\r\nReturns:\r\nValue of default index if it is set.\r\n\r\n\r\n\r\n\r\n\r\nget_itos() \u2192 List[str][source]\u00b6\r\n\r\nReturns:\r\nList mapping indices to tokens.\r\n\r\n\r\n\r\n\r\n\r\nget_stoi() \u2192 Dict[str, int][source]\u00b6\r\n\r\nReturns:\r\nDictionary mapping tokens to indices.\r\n\r\n\r\n\r\n\r\n\r\ninsert_token(token: str, index: int) \u2192 None[source]\u00b6\r\n\r\nParameters:\r\n\r\ntoken \u2013 The token used to lookup the corresponding index.\r\nindex \u2013 The index corresponding to the associated token.", "mimetype": "text/plain", "start_char_idx": 103019, "end_char_idx": 107445, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a55f34b5-bee4-485c-95c2-8f43a072c6f3": {"__data__": {"id_": "a55f34b5-bee4-485c-95c2-8f43a072c6f3", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7c47131-e38c-4f01-aa72-92b29686df5c", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "a2d73512eb1b39021e5dd08f1fe5d6086dc3f254324cc9e5136d2717d33f77b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5492f8c-6c8d-4791-ada3-4cf81c8e038f", "node_type": "1", "metadata": {}, "hash": "bf5d495c5d0526e36afc5af2e454788735fe586806277a550ab6e9fc1a31fecc", "class_name": "RelatedNodeInfo"}}, "text": "append_token(token: str) \u2192 None[source]\u00b6\r\n\r\nParameters:\r\ntoken \u2013 The token used to lookup the corresponding index.\r\n\r\nRaises:\r\nRuntimeError \u2013 If token already exists in the vocab\r\n\r\n\r\n\r\n\r\n\r\nforward(tokens: List[str]) \u2192 List[int][source]\u00b6\r\nCalls the lookup_indices method\r\n\r\nParameters:\r\ntokens \u2013 a list of tokens used to lookup their corresponding indices.\r\n\r\nReturns:\r\nThe indices associated with a list of tokens.\r\n\r\n\r\n\r\n\r\n\r\nget_default_index() \u2192 Optional[int][source]\u00b6\r\n\r\nReturns:\r\nValue of default index if it is set.\r\n\r\n\r\n\r\n\r\n\r\nget_itos() \u2192 List[str][source]\u00b6\r\n\r\nReturns:\r\nList mapping indices to tokens.\r\n\r\n\r\n\r\n\r\n\r\nget_stoi() \u2192 Dict[str, int][source]\u00b6\r\n\r\nReturns:\r\nDictionary mapping tokens to indices.\r\n\r\n\r\n\r\n\r\n\r\ninsert_token(token: str, index: int) \u2192 None[source]\u00b6\r\n\r\nParameters:\r\n\r\ntoken \u2013 The token used to lookup the corresponding index.\r\nindex \u2013 The index corresponding to the associated token.\r\n\r\n\r\nRaises:\r\nRuntimeError \u2013 If index is not in range [0, Vocab.size()] or if token already exists in the vocab.\r\n\r\n\r\n\r\n\r\n\r\nlookup_indices(tokens: List[str]) \u2192 List[int][source]\u00b6\r\n\r\nParameters:\r\ntokens \u2013 the tokens used to lookup their corresponding indices.\r\n\r\nReturns:\r\nThe \u2018indices` associated with tokens.\r\n\r\n\r\n\r\n\r\n\r\nlookup_token(index: int) \u2192 str[source]\u00b6\r\n\r\nParameters:\r\nindex \u2013 The index corresponding to the associated token.\r\n\r\nReturns:\r\nThe token used to lookup the corresponding index.\r\n\r\nReturn type:\r\ntoken\r\n\r\nRaises:\r\nRuntimeError \u2013 If index not in range [0, itos.size()).\r\n\r\n\r\n\r\n\r\n\r\nlookup_tokens(indices: List[int]) \u2192 List[str][source]\u00b6\r\n\r\nParameters:\r\nindices \u2013 The indices used to lookup their corresponding`tokens`.\r\n\r\nReturns:\r\nThe tokens associated with indices.\r\n\r\nRaises:\r\nRuntimeError \u2013 If an index within indices is not int range [0, itos.size()).\r\n\r\n\r\n\r\n\r\n\r\nset_default_index(index: Optional[int]) \u2192 None[source]\u00b6\r\n\r\nParameters:\r\nindex \u2013 Value of default index. This index will be returned when OOV token is queried.\r\n\r\n\r\n\r\n\r\n\r\n__contains__(token: str) \u2192 bool[source]\u00b6\r\n\r\nParameters:\r\ntoken \u2013 The token for which to check the membership.\r\n\r\nReturns:\r\nWhether the token is member of vocab or not.\r\n\r\n\r\n\r\nParameters:\r\ntoken \u2013 The token for which to check the membership.\r\n\r\ntoken \u2013 The token for which to check the membership.\r\nReturns:\r\nWhether the token is member of vocab or not.\r\n\r\nWhether the token is member of vocab or not.\r\n\r\n__getitem__(token: str) \u2192 int[source]\u00b6\r\n\r\nParameters:\r\ntoken \u2013 The token used to lookup the corresponding index.\r\n\r\nReturns:\r\nThe index corresponding to the associated token.\r\n\r\n\r\n\r\nParameters:\r\ntoken \u2013 The token used to lookup the corresponding index.\r\n\r\ntoken \u2013 The token used to lookup the corresponding index.\r\nReturns:\r\nThe index corresponding to the associated token.\r\n\r\nThe index corresponding to the associated token.\r\n\r\n__init__(vocab) \u2192 None[source]\u00b6\r\nInitialize internal Module state, shared by both nn.Module and ScriptModule.\r\n\r\nInitialize internal Module state, shared by both nn.Module and ScriptModule.\r\n\r\n__jit_unused_properties__ = ['is_jitable']\u00b6\r\nCreates a vocab object which maps tokens to indices.\r\n\r\nParameters:\r\nvocab (torch.classes.torchtext.Vocab or torchtext._torchtext.Vocab) \u2013 a cpp vocab object.\r\n\r\n\r\n\r\nCreates a vocab object which maps tokens to indices.\r\nParameters:\r\nvocab (torch.classes.torchtext.Vocab or torchtext._torchtext.Vocab) \u2013 a cpp vocab object.\r\n\r\nvocab (torch.classes.torchtext.Vocab or torchtext._torchtext.Vocab) \u2013 a cpp vocab object.\r\n\r\n__len__() \u2192 int[source]\u00b6\r\n\r\nReturns:\r\nThe length of the vocab.\r\n\r\n\r\n\r\nReturns:\r\nThe length of the vocab.\r\n\r\nThe length of the vocab.\r\n\r\n__prepare_scriptable__()[source]\u00b6\r\nReturn a JITable Vocab.\r\n\r\nReturn a JITable Vocab.\r\n\r\nappend_token(token: str) \u2192 None[source]\u00b6\r\n\r\nParameters:\r\ntoken \u2013 The token used to lookup the corresponding index.\r\n\r\nRaises:\r\nRuntimeError \u2013 If token already exists in the vocab\r\n\r\n\r\n\r\nParameters:\r\ntoken \u2013 The token used to lookup the corresponding index.\r\n\r\ntoken \u2013 The token used to lookup the corresponding index.\r\nRaises:\r\nRuntimeError \u2013 If token already exists in the vocab\r\n\r\nRuntimeError \u2013 If token already exists in the vocab\r\n\r\nforward(tokens: List[str]) \u2192 List[int][source]\u00b6\r\nCalls the lookup_indices method\r\n\r\nParameters:\r\ntokens \u2013 a list of tokens used to lookup their corresponding indices.\r\n\r\nReturns:\r\nThe indices associated with a list of tokens.\r\n\r\n\r\n\r\nCalls the lookup_indices method\r\nParameters:\r\ntokens \u2013 a list of tokens used to lookup their corresponding indices.\r\n\r\ntokens \u2013 a list of tokens used to lookup their corresponding indices.\r\nReturns:\r\nThe indices associated with a list of tokens.\r\n\r\nThe indices associated with a list of tokens.\r\n\r\nget_default_index() \u2192 Optional[int][source]\u00b6\r\n\r\nReturns:\r\nValue of default index if it is set.\r\n\r\n\r\n\r\nReturns:\r\nValue of default index if it is set.", "mimetype": "text/plain", "start_char_idx": 106539, "end_char_idx": 111332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5492f8c-6c8d-4791-ada3-4cf81c8e038f": {"__data__": {"id_": "d5492f8c-6c8d-4791-ada3-4cf81c8e038f", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a55f34b5-bee4-485c-95c2-8f43a072c6f3", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "81eee3a47a66b8ba79d7722df405e5aedb3ae13fd2930867913fe3dc79bd3f6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35d60436-502c-40ac-900b-3d46e992427b", "node_type": "1", "metadata": {}, "hash": "a9a1c62224a81d0d9569e274c878c569ea9f329c7c8b7e6113c1f9e13239b333", "class_name": "RelatedNodeInfo"}}, "text": "Raises:\r\nRuntimeError \u2013 If token already exists in the vocab\r\n\r\n\r\n\r\nParameters:\r\ntoken \u2013 The token used to lookup the corresponding index.\r\n\r\ntoken \u2013 The token used to lookup the corresponding index.\r\nRaises:\r\nRuntimeError \u2013 If token already exists in the vocab\r\n\r\nRuntimeError \u2013 If token already exists in the vocab\r\n\r\nforward(tokens: List[str]) \u2192 List[int][source]\u00b6\r\nCalls the lookup_indices method\r\n\r\nParameters:\r\ntokens \u2013 a list of tokens used to lookup their corresponding indices.\r\n\r\nReturns:\r\nThe indices associated with a list of tokens.\r\n\r\n\r\n\r\nCalls the lookup_indices method\r\nParameters:\r\ntokens \u2013 a list of tokens used to lookup their corresponding indices.\r\n\r\ntokens \u2013 a list of tokens used to lookup their corresponding indices.\r\nReturns:\r\nThe indices associated with a list of tokens.\r\n\r\nThe indices associated with a list of tokens.\r\n\r\nget_default_index() \u2192 Optional[int][source]\u00b6\r\n\r\nReturns:\r\nValue of default index if it is set.\r\n\r\n\r\n\r\nReturns:\r\nValue of default index if it is set.\r\n\r\nValue of default index if it is set.\r\n\r\nget_itos() \u2192 List[str][source]\u00b6\r\n\r\nReturns:\r\nList mapping indices to tokens.\r\n\r\n\r\n\r\nReturns:\r\nList mapping indices to tokens.\r\n\r\nList mapping indices to tokens.\r\n\r\nget_stoi() \u2192 Dict[str, int][source]\u00b6\r\n\r\nReturns:\r\nDictionary mapping tokens to indices.\r\n\r\n\r\n\r\nReturns:\r\nDictionary mapping tokens to indices.\r\n\r\nDictionary mapping tokens to indices.\r\n\r\ninsert_token(token: str, index: int) \u2192 None[source]\u00b6\r\n\r\nParameters:\r\n\r\ntoken \u2013 The token used to lookup the corresponding index.\r\nindex \u2013 The index corresponding to the associated token.\r\n\r\n\r\nRaises:\r\nRuntimeError \u2013 If index is not in range [0, Vocab.size()] or if token already exists in the vocab.\r\n\r\n\r\n\r\nParameters:\r\n\r\ntoken \u2013 The token used to lookup the corresponding index.\r\nindex \u2013 The index corresponding to the associated token.\r\n\r\n\r\ntoken \u2013 The token used to lookup the corresponding index.\r\nindex \u2013 The index corresponding to the associated token.\r\nRaises:\r\nRuntimeError \u2013 If index is not in range [0, Vocab.size()] or if token already exists in the vocab.\r\n\r\nRuntimeError \u2013 If index is not in range [0, Vocab.size()] or if token already exists in the vocab.\r\n\r\nlookup_indices(tokens: List[str]) \u2192 List[int][source]\u00b6\r\n\r\nParameters:\r\ntokens \u2013 the tokens used to lookup their corresponding indices.\r\n\r\nReturns:\r\nThe \u2018indices` associated with tokens.\r\n\r\n\r\n\r\nParameters:\r\ntokens \u2013 the tokens used to lookup their corresponding indices.\r\n\r\ntokens \u2013 the tokens used to lookup their corresponding indices.\r\nReturns:\r\nThe \u2018indices` associated with tokens.\r\n\r\nThe \u2018indices` associated with tokens.\r\n\r\nlookup_token(index: int) \u2192 str[source]\u00b6\r\n\r\nParameters:\r\nindex \u2013 The index corresponding to the associated token.\r\n\r\nReturns:\r\nThe token used to lookup the corresponding index.\r\n\r\nReturn type:\r\ntoken\r\n\r\nRaises:\r\nRuntimeError \u2013 If index not in range [0, itos.size()).\r\n\r\n\r\n\r\nParameters:\r\nindex \u2013 The index corresponding to the associated token.\r\n\r\nindex \u2013 The index corresponding to the associated token.\r\nReturns:\r\nThe token used to lookup the corresponding index.\r\n\r\nThe token used to lookup the corresponding index.\r\nReturn type:\r\ntoken\r\n\r\ntoken\r\nRaises:\r\nRuntimeError \u2013 If index not in range [0, itos.size()).\r\n\r\nRuntimeError \u2013 If index not in range [0, itos.size()).\r\n\r\nlookup_tokens(indices: List[int]) \u2192 List[str][source]\u00b6\r\n\r\nParameters:\r\nindices \u2013 The indices used to lookup their corresponding`tokens`.\r\n\r\nReturns:\r\nThe tokens associated with indices.\r\n\r\nRaises:\r\nRuntimeError \u2013 If an index within indices is not int range [0, itos.size()).\r\n\r\n\r\n\r\nParameters:\r\nindices \u2013 The indices used to lookup their corresponding`tokens`.\r\n\r\nindices \u2013 The indices used to lookup their corresponding`tokens`.\r\nReturns:\r\nThe tokens associated with indices.\r\n\r\nThe tokens associated with indices.\r\nRaises:\r\nRuntimeError \u2013 If an index within indices is not int range [0, itos.size()).\r\n\r\nRuntimeError \u2013 If an index within indices is not int range [0, itos.size()).\r\n\r\nset_default_index(index: Optional[int]) \u2192 None[source]\u00b6\r\n\r\nParameters:\r\nindex \u2013 Value of default index. This index will be returned when OOV token is queried.\r\n\r\n\r\n\r\nParameters:\r\nindex \u2013 Value of default index. This index will be returned when OOV token is queried.\r\n\r\nindex \u2013 Value of default index. This index will be returned when OOV token is queried.\r\nvocab\u00b6\r\n\r\ntorchtext.vocab.vocab(ordered_dict: Dict, min_freq: int = 1, specials: Optional[List[str]] = None, special_first: bool = True) \u2192 Vocab[source]\u00b6\r\nFactory method for creating a vocab object which maps tokens to indices.\r\nNote that the ordering in which key value pairs were inserted in the ordered_dict will be respected when building the vocab.\r\nTherefore if sorting by token frequency is important to the user, the ordered_dict should be created in a way to reflect this.\r\n\r\nParameters:\r\n\r\nordered_dict \u2013 Ordered Dictionary mapping tokens to their corresponding occurance frequencies.", "mimetype": "text/plain", "start_char_idx": 110333, "end_char_idx": 115240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35d60436-502c-40ac-900b-3d46e992427b": {"__data__": {"id_": "35d60436-502c-40ac-900b-3d46e992427b", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5492f8c-6c8d-4791-ada3-4cf81c8e038f", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "b20fe77b3f2326c9ad8f7565637eac5bec859c29cc18addfb8cfa00a1ff77906", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86eb3f8e-3809-45cb-b9f0-b163198b8f72", "node_type": "1", "metadata": {}, "hash": "32921a3181bddbf121af5187e5e8f5b06bf1976457670fe712c1af6088d8dbd1", "class_name": "RelatedNodeInfo"}}, "text": "set_default_index(index: Optional[int]) \u2192 None[source]\u00b6\r\n\r\nParameters:\r\nindex \u2013 Value of default index. This index will be returned when OOV token is queried.\r\n\r\n\r\n\r\nParameters:\r\nindex \u2013 Value of default index. This index will be returned when OOV token is queried.\r\n\r\nindex \u2013 Value of default index. This index will be returned when OOV token is queried.\r\nvocab\u00b6\r\n\r\ntorchtext.vocab.vocab(ordered_dict: Dict, min_freq: int = 1, specials: Optional[List[str]] = None, special_first: bool = True) \u2192 Vocab[source]\u00b6\r\nFactory method for creating a vocab object which maps tokens to indices.\r\nNote that the ordering in which key value pairs were inserted in the ordered_dict will be respected when building the vocab.\r\nTherefore if sorting by token frequency is important to the user, the ordered_dict should be created in a way to reflect this.\r\n\r\nParameters:\r\n\r\nordered_dict \u2013 Ordered Dictionary mapping tokens to their corresponding occurance frequencies.\r\nmin_freq \u2013 The minimum frequency needed to include a token in the vocabulary.\r\nspecials \u2013 Special symbols to add. The order of supplied tokens will be preserved.\r\nspecial_first \u2013 Indicates whether to insert symbols at the beginning or at the end.\r\n\r\n\r\nReturns:\r\nA Vocab object\r\n\r\nReturn type:\r\ntorchtext.vocab.Vocab\r\n\r\n\r\nExamples\r\n>>> from torchtext.vocab import vocab\r\n>>> from collections import Counter, OrderedDict\r\n>>> counter = Counter([\"a\", \"a\", \"b\", \"b\", \"b\"])\r\n>>> sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\r\n>>> ordered_dict = OrderedDict(sorted_by_freq_tuples)\r\n>>> v1 = vocab(ordered_dict)\r\n>>> print(v1['a']) #prints 1\r\n>>> print(v1['out of vocab']) #raise RuntimeError since default index is not set\r\n>>> tokens = ['e', 'd', 'c', 'b', 'a']\r\n>>> #adding <unk> token and default index\r\n>>> unk_token = '<unk>'\r\n>>> default_index = -1\r\n>>> v2 = vocab(OrderedDict([(token, 1) for token in tokens]), specials=[unk_token])\r\n>>> v2.set_default_index(default_index)\r\n>>> print(v2['<unk>']) #prints 0\r\n>>> print(v2['out of vocab']) #prints -1\r\n>>> #make default index same as index of unk_token\r\n>>> v2.set_default_index(v2[unk_token])\r\n>>> v2['out of vocab'] is v2[unk_token] #prints True\r\n\r\n\r\n\r\nFactory method for creating a vocab object which maps tokens to indices.\r\nNote that the ordering in which key value pairs were inserted in the ordered_dict will be respected when building the vocab.\r\nTherefore if sorting by token frequency is important to the user, the ordered_dict should be created in a way to reflect this.\r\nParameters:\r\n\r\nordered_dict \u2013 Ordered Dictionary mapping tokens to their corresponding occurance frequencies.\r\nmin_freq \u2013 The minimum frequency needed to include a token in the vocabulary.\r\nspecials \u2013 Special symbols to add. The order of supplied tokens will be preserved.\r\nspecial_first \u2013 Indicates whether to insert symbols at the beginning or at the end.\r\n\r\n\r\nordered_dict \u2013 Ordered Dictionary mapping tokens to their corresponding occurance frequencies.\r\nmin_freq \u2013 The minimum frequency needed to include a token in the vocabulary.\r\nspecials \u2013 Special symbols to add. The order of supplied tokens will be preserved.\r\nspecial_first \u2013 Indicates whether to insert symbols at the beginning or at the end.", "mimetype": "text/plain", "start_char_idx": 114289, "end_char_idx": 117516, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86eb3f8e-3809-45cb-b9f0-b163198b8f72": {"__data__": {"id_": "86eb3f8e-3809-45cb-b9f0-b163198b8f72", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35d60436-502c-40ac-900b-3d46e992427b", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7c2ae497cc23b6e92f3eb9ed4c087d8f676bf8254432ec62b225fc61bd7483ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0578511b-a91a-4dac-bb2c-c2ce505cd162", "node_type": "1", "metadata": {}, "hash": "5a3bda674868c9fe9febc7d5b500cc7b3c071ee4f5e24ac73da7e0feb14fd589", "class_name": "RelatedNodeInfo"}}, "text": "Note that the ordering in which key value pairs were inserted in the ordered_dict will be respected when building the vocab.\r\nTherefore if sorting by token frequency is important to the user, the ordered_dict should be created in a way to reflect this.\r\nParameters:\r\n\r\nordered_dict \u2013 Ordered Dictionary mapping tokens to their corresponding occurance frequencies.\r\nmin_freq \u2013 The minimum frequency needed to include a token in the vocabulary.\r\nspecials \u2013 Special symbols to add. The order of supplied tokens will be preserved.\r\nspecial_first \u2013 Indicates whether to insert symbols at the beginning or at the end.\r\n\r\n\r\nordered_dict \u2013 Ordered Dictionary mapping tokens to their corresponding occurance frequencies.\r\nmin_freq \u2013 The minimum frequency needed to include a token in the vocabulary.\r\nspecials \u2013 Special symbols to add. The order of supplied tokens will be preserved.\r\nspecial_first \u2013 Indicates whether to insert symbols at the beginning or at the end.\r\nReturns:\r\nA Vocab object\r\n\r\nA Vocab object\r\nReturn type:\r\ntorchtext.vocab.Vocab\r\n\r\ntorchtext.vocab.Vocab\r\nExamples\r\n>>> from torchtext.vocab import vocab\r\n>>> from collections import Counter, OrderedDict\r\n>>> counter = Counter([\"a\", \"a\", \"b\", \"b\", \"b\"])\r\n>>> sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\r\n>>> ordered_dict = OrderedDict(sorted_by_freq_tuples)\r\n>>> v1 = vocab(ordered_dict)\r\n>>> print(v1['a']) #prints 1\r\n>>> print(v1['out of vocab']) #raise RuntimeError since default index is not set\r\n>>> tokens = ['e', 'd', 'c', 'b', 'a']\r\n>>> #adding <unk> token and default index\r\n>>> unk_token = '<unk>'\r\n>>> default_index = -1\r\n>>> v2 = vocab(OrderedDict([(token, 1) for token in tokens]), specials=[unk_token])\r\n>>> v2.set_default_index(default_index)\r\n>>> print(v2['<unk>']) #prints 0\r\n>>> print(v2['out of vocab']) #prints -1\r\n>>> #make default index same as index of unk_token\r\n>>> v2.set_default_index(v2[unk_token])\r\n>>> v2['out of vocab'] is v2[unk_token] #prints True\r\n\r\nbuild_vocab_from_iterator\u00b6\r\n\r\ntorchtext.vocab.build_vocab_from_iterator(iterator: Iterable, min_freq: int = 1, specials: Optional[List[str]] = None, special_first: bool = True, max_tokens: Optional[int] = None) \u2192 Vocab[source]\u00b6\r\nBuild a Vocab from an iterator.\r\n\r\nParameters:\r\n\r\niterator \u2013 Iterator used to build Vocab. Must yield list or iterator of tokens.\r\nmin_freq \u2013 The minimum frequency needed to include a token in the vocabulary.\r\nspecials \u2013 Special symbols to add. The order of supplied tokens will be preserved.\r\nspecial_first \u2013 Indicates whether to insert symbols at the beginning or at the end.\r\nmax_tokens \u2013 If provided, creates the vocab from the max_tokens - len(specials) most frequent tokens.\r\n\r\n\r\nReturns:\r\nA Vocab object\r\n\r\nReturn type:\r\ntorchtext.vocab.Vocab\r\n\r\n\r\nExamples\r\n>>> #generating vocab from text file\r\n>>> import io\r\n>>> from torchtext.vocab import build_vocab_from_iterator\r\n>>> def yield_tokens(file_path):\r\n>>>     with io.open(file_path, encoding = 'utf-8') as f:\r\n>>>         for line in f:\r\n>>>             yield line.strip().split()\r\n>>> vocab = build_vocab_from_iterator(yield_tokens(file_path), specials=[\"<unk>\"])\r\n\r\n\r\n\r\nBuild a Vocab from an iterator.\r\nParameters:\r\n\r\niterator \u2013 Iterator used to build Vocab. Must yield list or iterator of tokens.\r\nmin_freq \u2013 The minimum frequency needed to include a token in the vocabulary.\r\nspecials \u2013 Special symbols to add. The order of supplied tokens will be preserved.\r\nspecial_first \u2013 Indicates whether to insert symbols at the beginning or at the end.\r\nmax_tokens \u2013 If provided, creates the vocab from the max_tokens - len(specials) most frequent tokens.\r\n\r\n\r\niterator \u2013 Iterator used to build Vocab. Must yield list or iterator of tokens.\r\nmin_freq \u2013 The minimum frequency needed to include a token in the vocabulary.\r\nspecials \u2013 Special symbols to add. The order of supplied tokens will be preserved.\r\nspecial_first \u2013 Indicates whether to insert symbols at the beginning or at the end.\r\nmax_tokens \u2013 If provided, creates the vocab from the max_tokens - len(specials) most frequent tokens.", "mimetype": "text/plain", "start_char_idx": 116557, "end_char_idx": 120615, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0578511b-a91a-4dac-bb2c-c2ce505cd162": {"__data__": {"id_": "0578511b-a91a-4dac-bb2c-c2ce505cd162", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86eb3f8e-3809-45cb-b9f0-b163198b8f72", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "2cd09e50c40a8c7ef4344f0a942060abcd3d9538fde75b7e5f7a9de9b8eaa462", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26c3f524-6755-477e-9cb9-4183fffdcb1b", "node_type": "1", "metadata": {}, "hash": "541d11f3b7a9f1fe66441a1244cc896472e289fc4003ff59c03a70a0895d9ca2", "class_name": "RelatedNodeInfo"}}, "text": "Parameters:\r\n\r\niterator \u2013 Iterator used to build Vocab. Must yield list or iterator of tokens.\r\nmin_freq \u2013 The minimum frequency needed to include a token in the vocabulary.\r\nspecials \u2013 Special symbols to add. The order of supplied tokens will be preserved.\r\nspecial_first \u2013 Indicates whether to insert symbols at the beginning or at the end.\r\nmax_tokens \u2013 If provided, creates the vocab from the max_tokens - len(specials) most frequent tokens.\r\n\r\n\r\niterator \u2013 Iterator used to build Vocab. Must yield list or iterator of tokens.\r\nmin_freq \u2013 The minimum frequency needed to include a token in the vocabulary.\r\nspecials \u2013 Special symbols to add. The order of supplied tokens will be preserved.\r\nspecial_first \u2013 Indicates whether to insert symbols at the beginning or at the end.\r\nmax_tokens \u2013 If provided, creates the vocab from the max_tokens - len(specials) most frequent tokens.\r\nReturns:\r\nA Vocab object\r\n\r\nA Vocab object\r\nReturn type:\r\ntorchtext.vocab.Vocab\r\n\r\ntorchtext.vocab.Vocab\r\nExamples\r\n>>> #generating vocab from text file\r\n>>> import io\r\n>>> from torchtext.vocab import build_vocab_from_iterator\r\n>>> def yield_tokens(file_path):\r\n>>>     with io.open(file_path, encoding = 'utf-8') as f:\r\n>>>         for line in f:\r\n>>>             yield line.strip().split()\r\n>>> vocab = build_vocab_from_iterator(yield_tokens(file_path), specials=[\"<unk>\"])\r\n\r\nVectors\u00b6\r\n\r\nclass torchtext.vocab.Vectors(name, cache=None, url=None, unk_init=None, max_vectors=None)[source]\u00b6\r\n\r\n\r\n__init__(name, cache=None, url=None, unk_init=None, max_vectors=None) \u2192 None[source]\u00b6\r\n\r\nParameters:\r\n\r\nname \u2013 name of the file that contains the vectors\r\ncache \u2013 directory for cached vectors\r\nurl \u2013 url for download if vectors not found in cache\r\nunk_init (callback) \u2013 by default, initialize out-of-vocabulary word vectors\r\nto zero vectors; can be any function that takes in a Tensor and returns a Tensor of the same size\r\nmax_vectors (int) \u2013 this can be used to limit the number of\r\npre-trained vectors loaded.\r\nMost pre-trained vector sets are sorted\r\nin the descending order of word frequency.\r\nThus, in situations where the entire set doesn\u2019t fit in memory,\r\nor is not needed for another reason, passing max_vectors\r\ncan limit the size of the loaded set.\r\n\r\n\r\n\r\n\r\n\r\n\r\nget_vecs_by_tokens(tokens, lower_case_backup=False)[source]\u00b6\r\nLook up embedding vectors of tokens.\r\n\r\nParameters:\r\n\r\ntokens \u2013 a token or a list of tokens. if tokens is a string,\r\nreturns a 1-D tensor of shape self.dim; if tokens is a\r\nlist of strings, returns a 2-D tensor of shape=(len(tokens),\r\nself.dim).\r\nlower_case_backup \u2013 Whether to look up the token in the lower case.\r\nIf False, each token in the original case will be looked up;\r\nif True, each token in the original case will be looked up first,\r\nif not found in the keys of the property stoi, the token in the\r\nlower case will be looked up. Default: False.\r\n\r\n\r\n\r\nExamples\r\n>>> examples = ['chip', 'baby', 'Beautiful']\r\n>>> vec = text.vocab.GloVe(name='6B', dim=50)\r\n>>> ret = vec.get_vecs_by_tokens(examples, lower_case_backup=True)\r\n\r\n\r\n\r\n\r\n\r\n__init__(name, cache=None, url=None, unk_init=None, max_vectors=None) \u2192 None[source]\u00b6\r\n\r\nParameters:\r\n\r\nname \u2013 name of the file that contains the vectors\r\ncache \u2013 directory for cached vectors\r\nurl \u2013 url for download if vectors not found in cache\r\nunk_init (callback) \u2013 by default, initialize out-of-vocabulary word vectors\r\nto zero vectors; can be any function that takes in a Tensor and returns a Tensor of the same size\r\nmax_vectors (int) \u2013 this can be used to limit the number of\r\npre-trained vectors loaded.\r\nMost pre-trained vector sets are sorted\r\nin the descending order of word frequency.\r\nThus, in situations where the entire set doesn\u2019t fit in memory,\r\nor is not needed for another reason, passing max_vectors\r\ncan limit the size of the loaded set.\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nname \u2013 name of the file that contains the vectors\r\ncache \u2013 directory for cached vectors\r\nurl \u2013 url for download if vectors not found in cache\r\nunk_init (callback) \u2013 by default, initialize out-of-vocabulary word vectors\r\nto zero vectors; can be any function that takes in a Tensor and returns a Tensor of the same size\r\nmax_vectors (int) \u2013 this can be used to limit the number of\r\npre-trained vectors loaded.\r\nMost pre-trained vector sets are sorted\r\nin the descending order of word frequency.\r\nThus, in situations where the entire set doesn\u2019t fit in memory,\r\nor is not needed for another reason, passing max_vectors\r\ncan limit the size of the loaded set.", "mimetype": "text/plain", "start_char_idx": 119734, "end_char_idx": 124228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26c3f524-6755-477e-9cb9-4183fffdcb1b": {"__data__": {"id_": "26c3f524-6755-477e-9cb9-4183fffdcb1b", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0578511b-a91a-4dac-bb2c-c2ce505cd162", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "5db386b30b99516555681ba94f9f20ba238d805f7ffe47660d5940a65c9cb174", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4792cdb9-9ff0-4132-ab23-000c4c0b0b9a", "node_type": "1", "metadata": {}, "hash": "ac019d05b0450f89e348a09235ae4d2659f003990b630e1e7d139e7b72a952d8", "class_name": "RelatedNodeInfo"}}, "text": "Most pre-trained vector sets are sorted\r\nin the descending order of word frequency.\r\nThus, in situations where the entire set doesn\u2019t fit in memory,\r\nor is not needed for another reason, passing max_vectors\r\ncan limit the size of the loaded set.\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nname \u2013 name of the file that contains the vectors\r\ncache \u2013 directory for cached vectors\r\nurl \u2013 url for download if vectors not found in cache\r\nunk_init (callback) \u2013 by default, initialize out-of-vocabulary word vectors\r\nto zero vectors; can be any function that takes in a Tensor and returns a Tensor of the same size\r\nmax_vectors (int) \u2013 this can be used to limit the number of\r\npre-trained vectors loaded.\r\nMost pre-trained vector sets are sorted\r\nin the descending order of word frequency.\r\nThus, in situations where the entire set doesn\u2019t fit in memory,\r\nor is not needed for another reason, passing max_vectors\r\ncan limit the size of the loaded set.\r\n\r\n\r\nname \u2013 name of the file that contains the vectors\r\ncache \u2013 directory for cached vectors\r\nurl \u2013 url for download if vectors not found in cache\r\nunk_init (callback) \u2013 by default, initialize out-of-vocabulary word vectors\r\nto zero vectors; can be any function that takes in a Tensor and returns a Tensor of the same size\r\nmax_vectors (int) \u2013 this can be used to limit the number of\r\npre-trained vectors loaded.\r\nMost pre-trained vector sets are sorted\r\nin the descending order of word frequency.\r\nThus, in situations where the entire set doesn\u2019t fit in memory,\r\nor is not needed for another reason, passing max_vectors\r\ncan limit the size of the loaded set.\r\n\r\nget_vecs_by_tokens(tokens, lower_case_backup=False)[source]\u00b6\r\nLook up embedding vectors of tokens.\r\n\r\nParameters:\r\n\r\ntokens \u2013 a token or a list of tokens. if tokens is a string,\r\nreturns a 1-D tensor of shape self.dim; if tokens is a\r\nlist of strings, returns a 2-D tensor of shape=(len(tokens),\r\nself.dim).\r\nlower_case_backup \u2013 Whether to look up the token in the lower case.\r\nIf False, each token in the original case will be looked up;\r\nif True, each token in the original case will be looked up first,\r\nif not found in the keys of the property stoi, the token in the\r\nlower case will be looked up. Default: False.\r\n\r\n\r\n\r\nExamples\r\n>>> examples = ['chip', 'baby', 'Beautiful']\r\n>>> vec = text.vocab.GloVe(name='6B', dim=50)\r\n>>> ret = vec.get_vecs_by_tokens(examples, lower_case_backup=True)\r\n\r\n\r\n\r\nLook up embedding vectors of tokens.\r\nParameters:\r\n\r\ntokens \u2013 a token or a list of tokens. if tokens is a string,\r\nreturns a 1-D tensor of shape self.dim; if tokens is a\r\nlist of strings, returns a 2-D tensor of shape=(len(tokens),\r\nself.dim).\r\nlower_case_backup \u2013 Whether to look up the token in the lower case.\r\nIf False, each token in the original case will be looked up;\r\nif True, each token in the original case will be looked up first,\r\nif not found in the keys of the property stoi, the token in the\r\nlower case will be looked up. Default: False.\r\n\r\n\r\ntokens \u2013 a token or a list of tokens. if tokens is a string,\r\nreturns a 1-D tensor of shape self.dim; if tokens is a\r\nlist of strings, returns a 2-D tensor of shape=(len(tokens),\r\nself.dim).\r\nlower_case_backup \u2013 Whether to look up the token in the lower case.\r\nIf False, each token in the original case will be looked up;\r\nif True, each token in the original case will be looked up first,\r\nif not found in the keys of the property stoi, the token in the\r\nlower case will be looked up. Default: False.\r\nExamples\r\n>>> examples = ['chip', 'baby', 'Beautiful']\r\n>>> vec = text.vocab.GloVe(name='6B', dim=50)\r\n>>> ret = vec.get_vecs_by_tokens(examples, lower_case_backup=True)\r\n\r\nPretrained Word Embeddings\u00b6\r\nGloVe\u00b6\r\n\r\nclass torchtext.vocab.GloVe(name='840B', dim=300, **kwargs)[source]\u00b6\r\n\r\nFastText\u00b6\r\n\r\nclass torchtext.vocab.FastText(language='en', **kwargs)[source]\u00b6\r\n\r\nCharNGram\u00b6\r\n\r\nclass torchtext.vocab.CharNGram(**kwargs)[source]\u00b6\r\n \r\n\r\nsource: https://pytorch.org/text/stable/utils.html \r\ncontent: \r\n\r\ntorchtext.utils\u00b6\r\nreporthook\u00b6\r\n\r\ntorchtext.utils.reporthook(t)[source]\u00b6\r\nhttps://github.com/tqdm/tqdm.\r\n\r\nhttps://github.com/tqdm/tqdm.", "mimetype": "text/plain", "start_char_idx": 123304, "end_char_idx": 127392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4792cdb9-9ff0-4132-ab23-000c4c0b0b9a": {"__data__": {"id_": "4792cdb9-9ff0-4132-ab23-000c4c0b0b9a", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26c3f524-6755-477e-9cb9-4183fffdcb1b", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "31a656e97d45384cd88a38efc04aa5e85e118ee11334629f4403e53baf388853", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8abe0411-3437-46ec-9d4e-eae3466c8df5", "node_type": "1", "metadata": {}, "hash": "877ffef88a00eb7a90481d349689dbfc5cba5a6f3e68b25771728463bfbdf5af", "class_name": "RelatedNodeInfo"}}, "text": "Default: False.\r\nExamples\r\n>>> examples = ['chip', 'baby', 'Beautiful']\r\n>>> vec = text.vocab.GloVe(name='6B', dim=50)\r\n>>> ret = vec.get_vecs_by_tokens(examples, lower_case_backup=True)\r\n\r\nPretrained Word Embeddings\u00b6\r\nGloVe\u00b6\r\n\r\nclass torchtext.vocab.GloVe(name='840B', dim=300, **kwargs)[source]\u00b6\r\n\r\nFastText\u00b6\r\n\r\nclass torchtext.vocab.FastText(language='en', **kwargs)[source]\u00b6\r\n\r\nCharNGram\u00b6\r\n\r\nclass torchtext.vocab.CharNGram(**kwargs)[source]\u00b6\r\n \r\n\r\nsource: https://pytorch.org/text/stable/utils.html \r\ncontent: \r\n\r\ntorchtext.utils\u00b6\r\nreporthook\u00b6\r\n\r\ntorchtext.utils.reporthook(t)[source]\u00b6\r\nhttps://github.com/tqdm/tqdm.\r\n\r\nhttps://github.com/tqdm/tqdm.\r\ndownload_from_url\u00b6\r\n\r\ntorchtext.utils.download_from_url(url, path=None, root='.data', overwrite=False, hash_value=None, hash_type='sha256')[source]\u00b6\r\nDownload file, with logic (from tensor2tensor) for Google Drive. Returns\r\nthe path to the downloaded file.\r\n:param url: the url of the file from URL header. (None)\r\n:param path: path where file will be saved\r\n:param root: download folder used to store the file in (.data)\r\n:param overwrite: overwrite existing files (False)\r\n:param hash_value: hash for url (Default: None).\r\n:type hash_value: str, optional\r\n:param hash_type: hash type, among \u201csha256\u201d and \u201cmd5\u201d (Default: \"sha256\").\r\n:type hash_type: str, optional\r\nExamples\r\n>>> url = 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz'\r\n>>> torchtext.utils.download_from_url(url)\r\n>>> url = 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz'\r\n>>> torchtext.utils.download_from_url(url)\r\n>>> '.data/validation.tar.gz'\r\n\r\n\r\n\r\nDownload file, with logic (from tensor2tensor) for Google Drive. Returns\r\nthe path to the downloaded file.\r\n:param url: the url of the file from URL header. (None)\r\n:param path: path where file will be saved\r\n:param root: download folder used to store the file in (.data)\r\n:param overwrite: overwrite existing files (False)\r\n:param hash_value: hash for url (Default: None).\r\n:type hash_value: str, optional\r\n:param hash_type: hash type, among \u201csha256\u201d and \u201cmd5\u201d (Default: \"sha256\").\r\n:type hash_type: str, optional\r\nExamples\r\n>>> url = 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz'\r\n>>> torchtext.utils.download_from_url(url)\r\n>>> url = 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz'\r\n>>> torchtext.utils.download_from_url(url)\r\n>>> '.data/validation.tar.gz'\r\n\r\nextract_archive\u00b6\r\n\r\ntorchtext.utils.extract_archive(from_path, to_path=None, overwrite=False)[source]\u00b6\r\nExtract archive.\r\n:param from_path: the path of the archive.\r\n:param to_path: the root path of the extracted files (directory of from_path)\r\n:param overwrite: overwrite existing files (False)\r\n\r\nReturns:\r\nList of paths to extracted files even if not overwritten.\r\n\r\n\r\nExamples\r\n>>> url = 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz'\r\n>>> from_path = './validation.tar.gz'\r\n>>> to_path = './'\r\n>>> torchtext.utils.download_from_url(url, from_path)\r\n>>> torchtext.utils.extract_archive(from_path, to_path)\r\n>>> ['.data/val.de', '.data/val.en']\r\n>>> torchtext.utils.download_from_url(url, from_path)\r\n>>> torchtext.utils.extract_archive(from_path, to_path)\r\n>>> ['.data/val.de', '.data/val.en']\r\n\r\n\r\n\r\nExtract archive.\r\n:param from_path: the path of the archive.\r\n:param to_path: the root path of the extracted files (directory of from_path)\r\n:param overwrite: overwrite existing files (False)\r\nReturns:\r\nList of paths to extracted files even if not overwritten.\r\n\r\nList of paths to extracted files even if not overwritten.", "mimetype": "text/plain", "start_char_idx": 126738, "end_char_idx": 130305, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8abe0411-3437-46ec-9d4e-eae3466c8df5": {"__data__": {"id_": "8abe0411-3437-46ec-9d4e-eae3466c8df5", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4792cdb9-9ff0-4132-ab23-000c4c0b0b9a", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "f9949c51cfcddbfd38cc2eadeae1b65cfb9c4d841a02edc18af1a65d9a06c684", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fa1f032-dec8-461c-8ff0-6dadce612a1f", "node_type": "1", "metadata": {}, "hash": "234558d98c541ec7a130d89a183cdea025005f435078cb30dd2faf7a7a6b195e", "class_name": "RelatedNodeInfo"}}, "text": "Examples\r\n>>> url = 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz'\r\n>>> from_path = './validation.tar.gz'\r\n>>> to_path = './'\r\n>>> torchtext.utils.download_from_url(url, from_path)\r\n>>> torchtext.utils.extract_archive(from_path, to_path)\r\n>>> ['.data/val.de', '.data/val.en']\r\n>>> torchtext.utils.download_from_url(url, from_path)\r\n>>> torchtext.utils.extract_archive(from_path, to_path)\r\n>>> ['.data/val.de', '.data/val.en']\r\n\r\n\r\n\r\nExtract archive.\r\n:param from_path: the path of the archive.\r\n:param to_path: the root path of the extracted files (directory of from_path)\r\n:param overwrite: overwrite existing files (False)\r\nReturns:\r\nList of paths to extracted files even if not overwritten.\r\n\r\nList of paths to extracted files even if not overwritten.\r\nExamples\r\n>>> url = 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz'\r\n>>> from_path = './validation.tar.gz'\r\n>>> to_path = './'\r\n>>> torchtext.utils.download_from_url(url, from_path)\r\n>>> torchtext.utils.extract_archive(from_path, to_path)\r\n>>> ['.data/val.de', '.data/val.en']\r\n>>> torchtext.utils.download_from_url(url, from_path)\r\n>>> torchtext.utils.extract_archive(from_path, to_path)\r\n>>> ['.data/val.de', '.data/val.en']\r\n \r\n\r\nsource: https://pytorch.org/text/stable/transforms.html \r\ncontent: \r\n\r\ntorchtext.transforms\u00b6\r\nTransforms are common text transforms. They can be chained together using torch.nn.Sequential or using torchtext.transforms.Sequential to support torch-scriptability.\r\nSentencePieceTokenizer\u00b6\r\n\r\nclass torchtext.transforms.SentencePieceTokenizer(sp_model_path: str)[source]\u00b6\r\nTransform for Sentence Piece tokenizer from pre-trained sentencepiece model\r\nAdditional details: https://github.com/google/sentencepiece\r\n\r\nParameters:\r\nsp_model_path (str) \u2013 Path to pre-trained sentencepiece model\r\n\r\n\r\n\r\nExample>>> from torchtext.transforms import SentencePieceTokenizer\r\n>>> transform = SentencePieceTokenizer(\"spm_model\")\r\n>>> transform([\"hello world\", \"attention is all you need!\"])\r\n\r\n\r\n\r\nTutorials using SentencePieceTokenizer:\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\n\r\n\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\nReturns:\r\ntokenized text\r\n\r\nReturn type:\r\nUnion[List[str], List[List[str]]]\r\n\r\n\r\n\r\n\r\nTransform for Sentence Piece tokenizer from pre-trained sentencepiece model\r\nAdditional details: https://github.com/google/sentencepiece\r\nParameters:\r\nsp_model_path (str) \u2013 Path to pre-trained sentencepiece model\r\n\r\nsp_model_path (str) \u2013 Path to pre-trained sentencepiece model\r\nExample\r\n>>> from torchtext.transforms import SentencePieceTokenizer\r\n>>> transform = SentencePieceTokenizer(\"spm_model\")\r\n>>> transform([\"hello world\", \"attention is all you need!\"])\r\n\r\n\r\n\r\n>>> from torchtext.transforms import SentencePieceTokenizer\r\n>>> transform = SentencePieceTokenizer(\"spm_model\")\r\n>>> transform([\"hello world\", \"attention is all you need!\"])\r\n\r\nTutorials using SentencePieceTokenizer:\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\nReturns:\r\ntokenized text\r\n\r\nReturn type:\r\nUnion[List[str], List[List[str]]]\r\n\r\n\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\nReturns:\r\ntokenized text\r\n\r\ntokenized text\r\nReturn type:\r\nUnion[List[str], List[List[str]]]\r\n\r\nUnion[List[str], List[List[str]]]\r\nGPT2BPETokenizer\u00b6\r\n\r\nclass torchtext.transforms.GPT2BPETokenizer(encoder_json_path: str, vocab_bpe_path: str, return_tokens: bool = False)[source]\u00b6\r\nTransform for GPT-2 BPE Tokenizer.\r\nReimplements openai GPT-2 BPE in TorchScript. Original openai implementation\r\nhttps://github.com/openai/gpt-2/blob/master/src/encoder.py\r\n\r\nParameters:\r\n\r\nencoder_json_path (str) \u2013 Path to GPT-2 BPE encoder json file.\r\nvocab_bpe_path (str) \u2013 Path to bpe vocab file.\r\nreturn_tokens \u2013 Indicate whether to return split tokens.", "mimetype": "text/plain", "start_char_idx": 129530, "end_char_idx": 133888, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fa1f032-dec8-461c-8ff0-6dadce612a1f": {"__data__": {"id_": "9fa1f032-dec8-461c-8ff0-6dadce612a1f", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8abe0411-3437-46ec-9d4e-eae3466c8df5", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "9ffc454f1cc64d92278c384ba7fb83a3c91a6e6e26162eba33f860bd00c51cec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cac3f1f9-740b-42c1-a2b0-9632a8124bd0", "node_type": "1", "metadata": {}, "hash": "a3af5aeb8299381fee022ca279ce361c89912338a2531b64884ebc08e8010af8", "class_name": "RelatedNodeInfo"}}, "text": "input (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\nReturns:\r\ntokenized text\r\n\r\ntokenized text\r\nReturn type:\r\nUnion[List[str], List[List[str]]]\r\n\r\nUnion[List[str], List[List[str]]]\r\nGPT2BPETokenizer\u00b6\r\n\r\nclass torchtext.transforms.GPT2BPETokenizer(encoder_json_path: str, vocab_bpe_path: str, return_tokens: bool = False)[source]\u00b6\r\nTransform for GPT-2 BPE Tokenizer.\r\nReimplements openai GPT-2 BPE in TorchScript. Original openai implementation\r\nhttps://github.com/openai/gpt-2/blob/master/src/encoder.py\r\n\r\nParameters:\r\n\r\nencoder_json_path (str) \u2013 Path to GPT-2 BPE encoder json file.\r\nvocab_bpe_path (str) \u2013 Path to bpe vocab file.\r\nreturn_tokens \u2013 Indicate whether to return split tokens. If False, it will return encoded token IDs as strings (default: False)\r\n\r\n\r\n\r\n\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\nReturns:\r\ntokenized text\r\n\r\nReturn type:\r\nUnion[List[str], List[List(str)]]\r\n\r\n\r\n\r\n\r\nTransform for GPT-2 BPE Tokenizer.\r\nReimplements openai GPT-2 BPE in TorchScript. Original openai implementation\r\nhttps://github.com/openai/gpt-2/blob/master/src/encoder.py\r\nParameters:\r\n\r\nencoder_json_path (str) \u2013 Path to GPT-2 BPE encoder json file.\r\nvocab_bpe_path (str) \u2013 Path to bpe vocab file.\r\nreturn_tokens \u2013 Indicate whether to return split tokens. If False, it will return encoded token IDs as strings (default: False)\r\n\r\n\r\nencoder_json_path (str) \u2013 Path to GPT-2 BPE encoder json file.\r\nvocab_bpe_path (str) \u2013 Path to bpe vocab file.\r\nreturn_tokens \u2013 Indicate whether to return split tokens. If False, it will return encoded token IDs as strings (default: False)\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\nReturns:\r\ntokenized text\r\n\r\nReturn type:\r\nUnion[List[str], List[List(str)]]\r\n\r\n\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\nReturns:\r\ntokenized text\r\n\r\ntokenized text\r\nReturn type:\r\nUnion[List[str], List[List(str)]]\r\n\r\nUnion[List[str], List[List(str)]]\r\nCLIPTokenizer\u00b6\r\n\r\nclass torchtext.transforms.CLIPTokenizer(merges_path: str, encoder_json_path: Optional[str] = None, num_merges: Optional[int] = None, return_tokens: bool = False)[source]\u00b6\r\nTransform for CLIP Tokenizer. Based on Byte-Level BPE.\r\nReimplements CLIP Tokenizer in TorchScript. Original implementation:\r\nhttps://github.com/mlfoundations/open_clip/blob/main/src/clip/tokenizer.py\r\nThis tokenizer has been trained to treat spaces like parts of the tokens\r\n(a bit like sentencepiece) so a word will be encoded differently whether it\r\nis at the beginning of the sentence (without space) or not.\r\nThe below code snippet shows how to use the CLIP tokenizer with encoder and merges file\r\ntaken from the original paper implementation.\r\n\r\nExample>>> from torchtext.transforms import CLIPTokenizer\r\n>>> MERGES_FILE = \"http://download.pytorch.org/models/text/clip_merges.bpe\"\r\n>>> ENCODER_FILE = \"http://download.pytorch.org/models/text/clip_encoder.json\"\r\n>>> tokenizer = CLIPTokenizer(merges_path=MERGES_FILE, encoder_json_path=ENCODER_FILE)\r\n>>> tokenizer(\"the quick brown fox jumped over the lazy dog\")\r\n\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nmerges_path (str) \u2013 Path to bpe merges file.\r\nencoder_json_path (str) \u2013 Optional, path to BPE encoder json file. When specified, this is used\r\nto infer num_merges.\r\nnum_merges (int) \u2013 Optional, number of merges to read from the bpe merges file.\r\nreturn_tokens \u2013 Indicate whether to return split tokens. If False, it will return encoded token IDs as strings (default: False)\r\n\r\n\r\n\r\n\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\nReturns:\r\ntokenized text\r\n\r\nReturn type:\r\nUnion[List[str], List[List(str)]]\r\n\r\n\r\n\r\n\r\nTransform for CLIP Tokenizer. Based on Byte-Level BPE.\r\nReimplements CLIP Tokenizer in TorchScript.", "mimetype": "text/plain", "start_char_idx": 133152, "end_char_idx": 137314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cac3f1f9-740b-42c1-a2b0-9632a8124bd0": {"__data__": {"id_": "cac3f1f9-740b-42c1-a2b0-9632a8124bd0", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fa1f032-dec8-461c-8ff0-6dadce612a1f", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "e8726dc17e34c04eca2cf2ed93da21b7396a5e01cedbc0a37b0845514a05236c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11c964a7-4218-47a2-9133-779c576bc2d7", "node_type": "1", "metadata": {}, "hash": "196cc9ff5c6a28bcf815a82f1c51516e7fd4f802a78dfde5751f963ab08e54e6", "class_name": "RelatedNodeInfo"}}, "text": "encoder_json_path (str) \u2013 Optional, path to BPE encoder json file. When specified, this is used\r\nto infer num_merges.\r\nnum_merges (int) \u2013 Optional, number of merges to read from the bpe merges file.\r\nreturn_tokens \u2013 Indicate whether to return split tokens. If False, it will return encoded token IDs as strings (default: False)\r\n\r\n\r\n\r\n\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\nReturns:\r\ntokenized text\r\n\r\nReturn type:\r\nUnion[List[str], List[List(str)]]\r\n\r\n\r\n\r\n\r\nTransform for CLIP Tokenizer. Based on Byte-Level BPE.\r\nReimplements CLIP Tokenizer in TorchScript. Original implementation:\r\nhttps://github.com/mlfoundations/open_clip/blob/main/src/clip/tokenizer.py\r\nThis tokenizer has been trained to treat spaces like parts of the tokens\r\n(a bit like sentencepiece) so a word will be encoded differently whether it\r\nis at the beginning of the sentence (without space) or not.\r\nThe below code snippet shows how to use the CLIP tokenizer with encoder and merges file\r\ntaken from the original paper implementation.\r\nExample\r\n>>> from torchtext.transforms import CLIPTokenizer\r\n>>> MERGES_FILE = \"http://download.pytorch.org/models/text/clip_merges.bpe\"\r\n>>> ENCODER_FILE = \"http://download.pytorch.org/models/text/clip_encoder.json\"\r\n>>> tokenizer = CLIPTokenizer(merges_path=MERGES_FILE, encoder_json_path=ENCODER_FILE)\r\n>>> tokenizer(\"the quick brown fox jumped over the lazy dog\")\r\n\r\n\r\n\r\n>>> from torchtext.transforms import CLIPTokenizer\r\n>>> MERGES_FILE = \"http://download.pytorch.org/models/text/clip_merges.bpe\"\r\n>>> ENCODER_FILE = \"http://download.pytorch.org/models/text/clip_encoder.json\"\r\n>>> tokenizer = CLIPTokenizer(merges_path=MERGES_FILE, encoder_json_path=ENCODER_FILE)\r\n>>> tokenizer(\"the quick brown fox jumped over the lazy dog\")\r\n\r\nParameters:\r\n\r\nmerges_path (str) \u2013 Path to bpe merges file.\r\nencoder_json_path (str) \u2013 Optional, path to BPE encoder json file. When specified, this is used\r\nto infer num_merges.\r\nnum_merges (int) \u2013 Optional, number of merges to read from the bpe merges file.\r\nreturn_tokens \u2013 Indicate whether to return split tokens. If False, it will return encoded token IDs as strings (default: False)\r\n\r\n\r\nmerges_path (str) \u2013 Path to bpe merges file.\r\nencoder_json_path (str) \u2013 Optional, path to BPE encoder json file. When specified, this is used\r\nto infer num_merges.\r\nnum_merges (int) \u2013 Optional, number of merges to read from the bpe merges file.\r\nreturn_tokens \u2013 Indicate whether to return split tokens. If False, it will return encoded token IDs as strings (default: False)\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\nReturns:\r\ntokenized text\r\n\r\nReturn type:\r\nUnion[List[str], List[List(str)]]\r\n\r\n\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\nReturns:\r\ntokenized text\r\n\r\ntokenized text\r\nReturn type:\r\nUnion[List[str], List[List(str)]]\r\n\r\nUnion[List[str], List[List(str)]]\r\nRegexTokenizer\u00b6\r\n\r\nclass torchtext.transforms.RegexTokenizer(patterns_list)[source]\u00b6\r\nRegex tokenizer for a string sentence that applies all regex replacements defined in patterns_list. It is backed by the C++ RE2 regular expression engine from Google.\r\n\r\nParameters:\r\n\r\npatterns_list (List[Tuple[str, str]]) \u2013 a list of tuples (ordered pairs) which contain the regex pattern string\r\nelement. (as the first element and the replacement string as the second) \u2013 \r\n\r\n\r\n\r\n\r\nCaveats\r\nThe RE2 library does not support arbitrary lookahead or lookbehind assertions, nor does it support backreferences. Look at the docs here for more info.\r\nThe final tokenization step always uses spaces as separators. To split strings based on a specific regex pattern, similar to Python\u2019s re.split, a tuple of ('<regex_pattern>', ' ') can be provided.", "mimetype": "text/plain", "start_char_idx": 136640, "end_char_idx": 140675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11c964a7-4218-47a2-9133-779c576bc2d7": {"__data__": {"id_": "11c964a7-4218-47a2-9133-779c576bc2d7", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cac3f1f9-740b-42c1-a2b0-9632a8124bd0", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "4f878d19a0cd03edea3901a8772d2e2c838961210fc40380bfdd3bfd1e98acd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94eebc7d-8f60-48dc-90c7-268222f56ee5", "node_type": "1", "metadata": {}, "hash": "e31e55cfdd11d8f3ec135b8b56f15699c596d289d13f8564a4c14e8b46f258b3", "class_name": "RelatedNodeInfo"}}, "text": "It is backed by the C++ RE2 regular expression engine from Google.\r\n\r\nParameters:\r\n\r\npatterns_list (List[Tuple[str, str]]) \u2013 a list of tuples (ordered pairs) which contain the regex pattern string\r\nelement. (as the first element and the replacement string as the second) \u2013 \r\n\r\n\r\n\r\n\r\nCaveats\r\nThe RE2 library does not support arbitrary lookahead or lookbehind assertions, nor does it support backreferences. Look at the docs here for more info.\r\nThe final tokenization step always uses spaces as separators. To split strings based on a specific regex pattern, similar to Python\u2019s re.split, a tuple of ('<regex_pattern>', ' ') can be provided.\r\n\r\n\r\nExample\r\nRegex tokenization based on (patterns, replacements) list.>>> import torch\r\n>>> from torchtext.transforms import RegexTokenizer\r\n>>> test_sample = 'Basic Regex Tokenization for a Line of Text'\r\n>>> patterns_list = [\r\n    (r''', ' '  '),\r\n    (r'\"', '')]\r\n>>> reg_tokenizer = RegexTokenizer(patterns_list)\r\n>>> jit_reg_tokenizer = torch.jit.script(reg_tokenizer)\r\n>>> tokens = jit_reg_tokenizer(test_sample)\r\n\r\n\r\n\r\nRegex tokenization based on (single_pattern, ' ') list.>>> import torch\r\n>>> from torchtext.transforms import RegexTokenizer\r\n>>> test_sample = 'Basic.Regex,Tokenization_for+a..Line,,of  Text'\r\n>>> patterns_list = [\r\n    (r'[,._+ ]+', r' ')]\r\n>>> reg_tokenizer = RegexTokenizer(patterns_list)\r\n>>> jit_reg_tokenizer = torch.jit.script(reg_tokenizer)\r\n>>> tokens = jit_reg_tokenizer(test_sample)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nforward(line: str) \u2192 List[str][source]\u00b6\r\n\r\nParameters:\r\nlines (str) \u2013 a text string to tokenize.\r\n\r\nReturns:\r\na token list after regex.\r\n\r\nReturn type:\r\nList[str]\r\n\r\n\r\n\r\n\r\nRegex tokenizer for a string sentence that applies all regex replacements defined in patterns_list. It is backed by the C++ RE2 regular expression engine from Google.\r\nParameters:\r\n\r\npatterns_list (List[Tuple[str, str]]) \u2013 a list of tuples (ordered pairs) which contain the regex pattern string\r\nelement. (as the first element and the replacement string as the second) \u2013 \r\n\r\n\r\npatterns_list (List[Tuple[str, str]]) \u2013 a list of tuples (ordered pairs) which contain the regex pattern string\r\nelement. (as the first element and the replacement string as the second) \u2013 \r\nCaveats\r\n\r\nThe RE2 library does not support arbitrary lookahead or lookbehind assertions, nor does it support backreferences. Look at the docs here for more info.\r\nThe final tokenization step always uses spaces as separators. To split strings based on a specific regex pattern, similar to Python\u2019s re.split, a tuple of ('<regex_pattern>', ' ') can be provided.\r\n\r\n\r\nThe RE2 library does not support arbitrary lookahead or lookbehind assertions, nor does it support backreferences. Look at the docs here for more info.\r\nThe final tokenization step always uses spaces as separators. To split strings based on a specific regex pattern, similar to Python\u2019s re.split, a tuple of ('<regex_pattern>', ' ') can be provided.\r\nExample\r\n\r\nRegex tokenization based on (patterns, replacements) list.>>> import torch\r\n>>> from torchtext.transforms import RegexTokenizer\r\n>>> test_sample = 'Basic Regex Tokenization for a Line of Text'\r\n>>> patterns_list = [\r\n    (r''', ' '  '),\r\n    (r'\"', '')]\r\n>>> reg_tokenizer = RegexTokenizer(patterns_list)\r\n>>> jit_reg_tokenizer = torch.jit.script(reg_tokenizer)\r\n>>> tokens = jit_reg_tokenizer(test_sample)\r\n\r\n\r\n\r\nRegex tokenization based on (single_pattern, ' ') list.>>> import torch\r\n>>> from torchtext.transforms import RegexTokenizer\r\n>>> test_sample = 'Basic.Regex,Tokenization_for+a..Line,,of  Text'\r\n>>> patterns_list = [\r\n    (r'[,._+ ]+', r' ')]\r\n>>> reg_tokenizer = RegexTokenizer(patterns_list)\r\n>>> jit_reg_tokenizer = torch.jit.script(reg_tokenizer)\r\n>>> tokens = jit_reg_tokenizer(test_sample)\r\n\r\n\r\n\r\n\r\n\r\nRegex tokenization based on (patterns, replacements) list.", "mimetype": "text/plain", "start_char_idx": 140034, "end_char_idx": 143861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94eebc7d-8f60-48dc-90c7-268222f56ee5": {"__data__": {"id_": "94eebc7d-8f60-48dc-90c7-268222f56ee5", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11c964a7-4218-47a2-9133-779c576bc2d7", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "e46a7f43bd5bb3fe3af1c45887e8c12a54f959cdf41928c8ed6df87be9f722e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1410b72d-0a68-4708-814b-1e3869f653ea", "node_type": "1", "metadata": {}, "hash": "b2e82cdc22abac5ab7f268a6a5c0137a49b7de14c1f897aeee8778fdc4ca16fd", "class_name": "RelatedNodeInfo"}}, "text": ">>> import torch\r\n>>> from torchtext.transforms import RegexTokenizer\r\n>>> test_sample = 'Basic Regex Tokenization for a Line of Text'\r\n>>> patterns_list = [\r\n    (r''', ' '  '),\r\n    (r'\"', '')]\r\n>>> reg_tokenizer = RegexTokenizer(patterns_list)\r\n>>> jit_reg_tokenizer = torch.jit.script(reg_tokenizer)\r\n>>> tokens = jit_reg_tokenizer(test_sample)\r\n\r\n\r\n\r\n>>> import torch\r\n>>> from torchtext.transforms import RegexTokenizer\r\n>>> test_sample = 'Basic Regex Tokenization for a Line of Text'\r\n>>> patterns_list = [\r\n    (r''', ' '  '),\r\n    (r'\"', '')]\r\n>>> reg_tokenizer = RegexTokenizer(patterns_list)\r\n>>> jit_reg_tokenizer = torch.jit.script(reg_tokenizer)\r\n>>> tokens = jit_reg_tokenizer(test_sample)\r\n\r\nRegex tokenization based on (single_pattern, ' ') list.\r\n>>> import torch\r\n>>> from torchtext.transforms import RegexTokenizer\r\n>>> test_sample = 'Basic.Regex,Tokenization_for+a..Line,,of  Text'\r\n>>> patterns_list = [\r\n    (r'[,._+ ]+', r' ')]\r\n>>> reg_tokenizer = RegexTokenizer(patterns_list)\r\n>>> jit_reg_tokenizer = torch.jit.script(reg_tokenizer)\r\n>>> tokens = jit_reg_tokenizer(test_sample)\r\n\r\n\r\n\r\n>>> import torch\r\n>>> from torchtext.transforms import RegexTokenizer\r\n>>> test_sample = 'Basic.Regex,Tokenization_for+a..Line,,of  Text'\r\n>>> patterns_list = [\r\n    (r'[,._+ ]+', r' ')]\r\n>>> reg_tokenizer = RegexTokenizer(patterns_list)\r\n>>> jit_reg_tokenizer = torch.jit.script(reg_tokenizer)\r\n>>> tokens = jit_reg_tokenizer(test_sample)\r\n\r\n\r\nforward(line: str) \u2192 List[str][source]\u00b6\r\n\r\nParameters:\r\nlines (str) \u2013 a text string to tokenize.\r\n\r\nReturns:\r\na token list after regex.\r\n\r\nReturn type:\r\nList[str]\r\n\r\n\r\n\r\nParameters:\r\nlines (str) \u2013 a text string to tokenize.\r\n\r\nlines (str) \u2013 a text string to tokenize.\r\nReturns:\r\na token list after regex.\r\n\r\na token list after regex.\r\nReturn type:\r\nList[str]\r\n\r\nList[str]\r\nBERTTokenizer\u00b6\r\n\r\nclass torchtext.transforms.BERTTokenizer(vocab_path: str, do_lower_case: bool = True, strip_accents: Optional[bool] = None, return_tokens=False, never_split: Optional[List[str]] = None)[source]\u00b6\r\nTransform for BERT Tokenizer.\r\nBased on WordPiece algorithm introduced in paper:\r\nhttps://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf\r\nThe backend kernel implementation is taken and modified from https://github.com/LieluoboAi/radish.\r\nSee PR https://github.com/pytorch/text/pull/1707 summary for more details.\r\nThe below code snippet shows how to use the BERT tokenizer using the pre-trained vocab files.\r\n\r\nExample>>> from torchtext.transforms import BERTTokenizer\r\n>>> VOCAB_FILE = \"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\"\r\n>>> tokenizer = BERTTokenizer(vocab_path=VOCAB_FILE, do_lower_case=True, return_tokens=True)\r\n>>> tokenizer(\"Hello World, How are you!\") # single sentence input\r\n>>> tokenizer([\"Hello World\",\"How are you!\"]) # batch input\r\n\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nvocab_path (str) \u2013 Path to pre-trained vocabulary file. The path can be either local or URL.\r\ndo_lower_case (Optional[bool]) \u2013 Indicate whether to do lower case. (default: True)\r\nstrip_accents (Optional[bool]) \u2013 Indicate whether to strip accents. (default: None)\r\nreturn_tokens (bool) \u2013 Indicate whether to return tokens. If false, returns corresponding token IDs as strings (default: False)\r\nnever_split (Optional[List[str]]) \u2013 Collection of tokens which will not be split during tokenization. (default: None)\r\n\r\n\r\n\r\n\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\nReturns:\r\ntokenized text\r\n\r\nReturn type:\r\nUnion[List[str], List[List(str)]]\r\n\r\n\r\n\r\n\r\nTransform for BERT Tokenizer.\r\nBased on WordPiece algorithm introduced in paper:\r\nhttps://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf\r\nThe backend kernel implementation is taken and modified from https://github.com/LieluoboAi/radish.\r\nSee PR https://github.com/pytorch/text/pull/1707 summary for more details.\r\nThe below code snippet shows how to use the BERT tokenizer using the pre-trained vocab files.", "mimetype": "text/plain", "start_char_idx": 143863, "end_char_idx": 147944, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1410b72d-0a68-4708-814b-1e3869f653ea": {"__data__": {"id_": "1410b72d-0a68-4708-814b-1e3869f653ea", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94eebc7d-8f60-48dc-90c7-268222f56ee5", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "be528d7de75e5cb29acb24a46331f0a9d6c1212d97f9c8b9ee4bbfba79705755", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec49ba9f-d67c-4366-8bc5-fc4dee8e8b31", "node_type": "1", "metadata": {}, "hash": "6d6f8564ea94eea869e0a2b215f30c04f0e177e57b2363d63bdf1fbdf49aa1c2", "class_name": "RelatedNodeInfo"}}, "text": "If false, returns corresponding token IDs as strings (default: False)\r\nnever_split (Optional[List[str]]) \u2013 Collection of tokens which will not be split during tokenization. (default: None)\r\n\r\n\r\n\r\n\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\nReturns:\r\ntokenized text\r\n\r\nReturn type:\r\nUnion[List[str], List[List(str)]]\r\n\r\n\r\n\r\n\r\nTransform for BERT Tokenizer.\r\nBased on WordPiece algorithm introduced in paper:\r\nhttps://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf\r\nThe backend kernel implementation is taken and modified from https://github.com/LieluoboAi/radish.\r\nSee PR https://github.com/pytorch/text/pull/1707 summary for more details.\r\nThe below code snippet shows how to use the BERT tokenizer using the pre-trained vocab files.\r\nExample\r\n>>> from torchtext.transforms import BERTTokenizer\r\n>>> VOCAB_FILE = \"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\"\r\n>>> tokenizer = BERTTokenizer(vocab_path=VOCAB_FILE, do_lower_case=True, return_tokens=True)\r\n>>> tokenizer(\"Hello World, How are you!\") # single sentence input\r\n>>> tokenizer([\"Hello World\",\"How are you!\"]) # batch input\r\n\r\n\r\n\r\n>>> from torchtext.transforms import BERTTokenizer\r\n>>> VOCAB_FILE = \"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\"\r\n>>> tokenizer = BERTTokenizer(vocab_path=VOCAB_FILE, do_lower_case=True, return_tokens=True)\r\n>>> tokenizer(\"Hello World, How are you!\") # single sentence input\r\n>>> tokenizer([\"Hello World\",\"How are you!\"]) # batch input\r\n\r\nParameters:\r\n\r\nvocab_path (str) \u2013 Path to pre-trained vocabulary file. The path can be either local or URL.\r\ndo_lower_case (Optional[bool]) \u2013 Indicate whether to do lower case. (default: True)\r\nstrip_accents (Optional[bool]) \u2013 Indicate whether to strip accents. (default: None)\r\nreturn_tokens (bool) \u2013 Indicate whether to return tokens. If false, returns corresponding token IDs as strings (default: False)\r\nnever_split (Optional[List[str]]) \u2013 Collection of tokens which will not be split during tokenization. (default: None)\r\n\r\n\r\nvocab_path (str) \u2013 Path to pre-trained vocabulary file. The path can be either local or URL.\r\ndo_lower_case (Optional[bool]) \u2013 Indicate whether to do lower case. (default: True)\r\nstrip_accents (Optional[bool]) \u2013 Indicate whether to strip accents. (default: None)\r\nreturn_tokens (bool) \u2013 Indicate whether to return tokens. If false, returns corresponding token IDs as strings (default: False)\r\nnever_split (Optional[List[str]]) \u2013 Collection of tokens which will not be split during tokenization. (default: None)\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\nReturns:\r\ntokenized text\r\n\r\nReturn type:\r\nUnion[List[str], List[List(str)]]\r\n\r\n\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\nReturns:\r\ntokenized text\r\n\r\ntokenized text\r\nReturn type:\r\nUnion[List[str], List[List(str)]]\r\n\r\nUnion[List[str], List[List(str)]]\r\nVocabTransform\u00b6\r\n\r\nclass torchtext.transforms.VocabTransform(vocab: Vocab)[source]\u00b6\r\nVocab transform to convert input batch of tokens into corresponding token ids\r\n\r\nParameters:\r\nvocab \u2013 an instance of torchtext.vocab.Vocab class.", "mimetype": "text/plain", "start_char_idx": 147066, "end_char_idx": 150531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec49ba9f-d67c-4366-8bc5-fc4dee8e8b31": {"__data__": {"id_": "ec49ba9f-d67c-4366-8bc5-fc4dee8e8b31", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1410b72d-0a68-4708-814b-1e3869f653ea", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "7dce296e3d6700cacb8ade55ed6640ab7d656a46a6cd043e8b1993eec05bc183", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56471f04-f7d5-4c56-8e46-64c330e2b09c", "node_type": "1", "metadata": {}, "hash": "79dceef4d4c959b5d386ee7c079e7280a255d6c613708371615546d0b3a94b4e", "class_name": "RelatedNodeInfo"}}, "text": "(default: None)\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\nReturns:\r\ntokenized text\r\n\r\nReturn type:\r\nUnion[List[str], List[List(str)]]\r\n\r\n\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\ninput (Union[str, List[str]]) \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\nReturns:\r\ntokenized text\r\n\r\ntokenized text\r\nReturn type:\r\nUnion[List[str], List[List(str)]]\r\n\r\nUnion[List[str], List[List(str)]]\r\nVocabTransform\u00b6\r\n\r\nclass torchtext.transforms.VocabTransform(vocab: Vocab)[source]\u00b6\r\nVocab transform to convert input batch of tokens into corresponding token ids\r\n\r\nParameters:\r\nvocab \u2013 an instance of torchtext.vocab.Vocab class.\r\n\r\n\r\nExample\r\n>>> import torch\r\n>>> from torchtext.vocab import vocab\r\n>>> from torchtext.transforms import VocabTransform\r\n>>> from collections import OrderedDict\r\n>>> vocab_obj = vocab(OrderedDict([('a', 1), ('b', 1), ('c', 1)]))\r\n>>> vocab_transform = VocabTransform(vocab_obj)\r\n>>> output = vocab_transform([['a','b'],['a','b','c']])\r\n>>> jit_vocab_transform = torch.jit.script(vocab_transform)\r\n\r\n\r\n\r\nTutorials using VocabTransform:\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\n\r\n\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[List[str], List[List[str]]]) \u2013 Input batch of token to convert to correspnding token ids\r\n\r\nReturns:\r\nConverted input into corresponding token ids\r\n\r\nReturn type:\r\nUnion[List[int], List[List[int]]]\r\n\r\n\r\n\r\n\r\nVocab transform to convert input batch of tokens into corresponding token ids\r\nParameters:\r\nvocab \u2013 an instance of torchtext.vocab.Vocab class.\r\n\r\nvocab \u2013 an instance of torchtext.vocab.Vocab class.\r\nExample\r\n>>> import torch\r\n>>> from torchtext.vocab import vocab\r\n>>> from torchtext.transforms import VocabTransform\r\n>>> from collections import OrderedDict\r\n>>> vocab_obj = vocab(OrderedDict([('a', 1), ('b', 1), ('c', 1)]))\r\n>>> vocab_transform = VocabTransform(vocab_obj)\r\n>>> output = vocab_transform([['a','b'],['a','b','c']])\r\n>>> jit_vocab_transform = torch.jit.script(vocab_transform)\r\n\r\nTutorials using VocabTransform:\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[List[str], List[List[str]]]) \u2013 Input batch of token to convert to correspnding token ids\r\n\r\nReturns:\r\nConverted input into corresponding token ids\r\n\r\nReturn type:\r\nUnion[List[int], List[List[int]]]\r\n\r\n\r\n\r\nParameters:\r\ninput (Union[List[str], List[List[str]]]) \u2013 Input batch of token to convert to correspnding token ids\r\n\r\ninput (Union[List[str], List[List[str]]]) \u2013 Input batch of token to convert to correspnding token ids\r\nReturns:\r\nConverted input into corresponding token ids\r\n\r\nConverted input into corresponding token ids\r\nReturn type:\r\nUnion[List[int], List[List[int]]]\r\n\r\nUnion[List[int], List[List[int]]]\r\nToTensor\u00b6\r\n\r\nclass torchtext.transforms.ToTensor(padding_value: Optional[int] = None, dtype: dtype = torch.int64)[source]\u00b6\r\nConvert input to torch tensor\r\n\r\nParameters:\r\n\r\npadding_value (Optional[int]) \u2013 Pad value to make each input in the batch of length equal to the longest sequence in the batch.\r\ndtype (torch.dtype) \u2013 torch.dtype of output tensor\r\n\r\n\r\n\r\n\r\n\r\nforward(input: Any) \u2192 Tensor[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[List[int], List[List[int]]]) \u2013 Sequence or batch of token ids\r\n\r\nReturn type:\r\nTensor\r\n\r\n\r\n\r\n\r\nConvert input to torch tensor\r\nParameters:\r\n\r\npadding_value (Optional[int]) \u2013 Pad value to make each input in the batch of length equal to the longest sequence in the batch.\r\ndtype (torch.dtype) \u2013 torch.dtype of output tensor\r\n\r\n\r\npadding_value (Optional[int]) \u2013 Pad value to make each input in the batch of length equal to the longest sequence in the batch.", "mimetype": "text/plain", "start_char_idx": 149707, "end_char_idx": 153734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56471f04-f7d5-4c56-8e46-64c330e2b09c": {"__data__": {"id_": "56471f04-f7d5-4c56-8e46-64c330e2b09c", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec49ba9f-d67c-4366-8bc5-fc4dee8e8b31", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "0f50031cc276d0b73fceff5c7cadc197be438f14eacd07d1170e106d3fe0f733", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8548960-ae21-4f44-a467-ab3a9e3485e9", "node_type": "1", "metadata": {}, "hash": "fc1b50eeedadf3ead4bf0f1b17ca9c54f9c7dab6845adb7d4b937a74113e6e4f", "class_name": "RelatedNodeInfo"}}, "text": "dtype (torch.dtype) \u2013 torch.dtype of output tensor\r\n\r\n\r\n\r\n\r\n\r\nforward(input: Any) \u2192 Tensor[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[List[int], List[List[int]]]) \u2013 Sequence or batch of token ids\r\n\r\nReturn type:\r\nTensor\r\n\r\n\r\n\r\n\r\nConvert input to torch tensor\r\nParameters:\r\n\r\npadding_value (Optional[int]) \u2013 Pad value to make each input in the batch of length equal to the longest sequence in the batch.\r\ndtype (torch.dtype) \u2013 torch.dtype of output tensor\r\n\r\n\r\npadding_value (Optional[int]) \u2013 Pad value to make each input in the batch of length equal to the longest sequence in the batch.\r\ndtype (torch.dtype) \u2013 torch.dtype of output tensor\r\n\r\nforward(input: Any) \u2192 Tensor[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[List[int], List[List[int]]]) \u2013 Sequence or batch of token ids\r\n\r\nReturn type:\r\nTensor\r\n\r\n\r\n\r\nParameters:\r\ninput (Union[List[int], List[List[int]]]) \u2013 Sequence or batch of token ids\r\n\r\ninput (Union[List[int], List[List[int]]]) \u2013 Sequence or batch of token ids\r\nReturn type:\r\nTensor\r\n\r\nTensor\r\nLabelToIndex\u00b6\r\n\r\nclass torchtext.transforms.LabelToIndex(label_names: Optional[List[str]] = None, label_path: Optional[str] = None, sort_names=False)[source]\u00b6\r\nTransform labels from string names to ids.\r\n\r\nParameters:\r\n\r\nlabel_names (Optional[List[str]]) \u2013 a list of unique label names\r\nlabel_path (Optional[str]) \u2013 a path to file containing unique label names containing 1 label per line. Note that either label_names or label_path should be supplied\r\nbut not both.\r\n\r\n\r\n\r\n\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input labels to convert to corresponding ids\r\n\r\nReturn type:\r\nUnion[int, List[int]]\r\n\r\n\r\n\r\n\r\nTransform labels from string names to ids.\r\nParameters:\r\n\r\nlabel_names (Optional[List[str]]) \u2013 a list of unique label names\r\nlabel_path (Optional[str]) \u2013 a path to file containing unique label names containing 1 label per line. Note that either label_names or label_path should be supplied\r\nbut not both.\r\n\r\n\r\nlabel_names (Optional[List[str]]) \u2013 a list of unique label names\r\nlabel_path (Optional[str]) \u2013 a path to file containing unique label names containing 1 label per line. Note that either label_names or label_path should be supplied\r\nbut not both.", "mimetype": "text/plain", "start_char_idx": 153152, "end_char_idx": 155358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8548960-ae21-4f44-a467-ab3a9e3485e9": {"__data__": {"id_": "e8548960-ae21-4f44-a467-ab3a9e3485e9", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56471f04-f7d5-4c56-8e46-64c330e2b09c", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "1524c1b46174598cd9679dc855a552d9bf5a1a13f1e521ffac8c542ad6126c2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec35cec0-59d3-4405-a9c3-34b684cd6e55", "node_type": "1", "metadata": {}, "hash": "79f46988834d05a7dc60522ce2ebd760e958323a90a3dd624da9c813491ee5f8", "class_name": "RelatedNodeInfo"}}, "text": "Note that either label_names or label_path should be supplied\r\nbut not both.\r\n\r\n\r\n\r\n\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input labels to convert to corresponding ids\r\n\r\nReturn type:\r\nUnion[int, List[int]]\r\n\r\n\r\n\r\n\r\nTransform labels from string names to ids.\r\nParameters:\r\n\r\nlabel_names (Optional[List[str]]) \u2013 a list of unique label names\r\nlabel_path (Optional[str]) \u2013 a path to file containing unique label names containing 1 label per line. Note that either label_names or label_path should be supplied\r\nbut not both.\r\n\r\n\r\nlabel_names (Optional[List[str]]) \u2013 a list of unique label names\r\nlabel_path (Optional[str]) \u2013 a path to file containing unique label names containing 1 label per line. Note that either label_names or label_path should be supplied\r\nbut not both.\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input labels to convert to corresponding ids\r\n\r\nReturn type:\r\nUnion[int, List[int]]\r\n\r\n\r\n\r\nParameters:\r\ninput (Union[str, List[str]]) \u2013 Input labels to convert to corresponding ids\r\n\r\ninput (Union[str, List[str]]) \u2013 Input labels to convert to corresponding ids\r\nReturn type:\r\nUnion[int, List[int]]\r\n\r\nUnion[int, List[int]]\r\nTruncate\u00b6\r\n\r\nclass torchtext.transforms.Truncate(max_seq_len: int)[source]\u00b6\r\nTruncate input sequence\r\n\r\nParameters:\r\nmax_seq_len (int) \u2013 The maximum allowable length for input sequence\r\n\r\n\r\n\r\nTutorials using Truncate:\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\n\r\n\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[List[Union[str, int]], List[List[Union[str, int]]]]) \u2013 Input sequence or batch of sequence to be truncated\r\n\r\nReturns:\r\nTruncated sequence\r\n\r\nReturn type:\r\nUnion[List[Union[str, int]], List[List[Union[str, int]]]]\r\n\r\n\r\n\r\n\r\nTruncate input sequence\r\nParameters:\r\nmax_seq_len (int) \u2013 The maximum allowable length for input sequence\r\n\r\nmax_seq_len (int) \u2013 The maximum allowable length for input sequence\r\nTutorials using Truncate:\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[List[Union[str, int]], List[List[Union[str, int]]]]) \u2013 Input sequence or batch of sequence to be truncated\r\n\r\nReturns:\r\nTruncated sequence\r\n\r\nReturn type:\r\nUnion[List[Union[str, int]], List[List[Union[str, int]]]]\r\n\r\n\r\n\r\nParameters:\r\ninput (Union[List[Union[str, int]], List[List[Union[str, int]]]]) \u2013 Input sequence or batch of sequence to be truncated\r\n\r\ninput (Union[List[Union[str, int]], List[List[Union[str, int]]]]) \u2013 Input sequence or batch of sequence to be truncated\r\nReturns:\r\nTruncated sequence\r\n\r\nTruncated sequence\r\nReturn type:\r\nUnion[List[Union[str, int]], List[List[Union[str, int]]]]\r\n\r\nUnion[List[Union[str, int]], List[List[Union[str, int]]]]\r\nAddToken\u00b6\r\n\r\nclass torchtext.transforms.AddToken(token: Union[int, str], begin: bool = True)[source]\u00b6\r\nAdd token to beginning or end of sequence\r\n\r\nParameters:\r\n\r\ntoken (Union[int, str]) \u2013 The token to be added\r\nbegin (bool, optional) \u2013 Whether to insert token at start or end or sequence, defaults to True\r\n\r\n\r\n\r\n\r\nTutorials using AddToken:\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\n\r\n\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[List[Union[str, int]], List[List[Union[str, int]]]]) \u2013 Input sequence or batch\r\n\r\n\r\n\r\n\r\nAdd token to beginning or end of sequence\r\nParameters:\r\n\r\ntoken (Union[int, str]) \u2013 The token to be added\r\nbegin (bool, optional) \u2013 Whether to insert token at start or end or sequence, defaults to True\r\n\r\n\r\ntoken (Union[int, str]) \u2013 The token to be added\r\nbegin (bool, optional) \u2013 Whether to insert token at start or end or sequence, defaults to True\r\nTutorials using AddToken:\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[List[Union[str, int]], List[List[Union[str, int]]]]) \u2013 Input sequence or batch\r\n\r\n\r\n\r\nParameters:\r\ninput (Union[List[Union[str, int]], List[List[Union[str, int]]]]) \u2013 Input sequence or batch\r\n\r\ninput (Union[List[Union[str, int]], List[List[Union[str, int]]]]) \u2013 Input sequence or batch\r\nSequential\u00b6\r\n\r\nclass torchtext.transforms.Sequential(*args: Module)[source]\u00b6\r\n\r\nclass torchtext.transforms.Sequential(arg: OrderedDict[str, Module])\r\nA container to host a sequence of text transforms.", "mimetype": "text/plain", "start_char_idx": 154539, "end_char_idx": 159231, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec35cec0-59d3-4405-a9c3-34b684cd6e55": {"__data__": {"id_": "ec35cec0-59d3-4405-a9c3-34b684cd6e55", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8548960-ae21-4f44-a467-ab3a9e3485e9", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "0bcf50ca3eedea4caebebccb8bbc585b110f6656a9b192f411e4bbd0b3a259b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca5d608f-6a76-4cd8-931c-53a5bcdcb3d1", "node_type": "1", "metadata": {}, "hash": "0b94175755ad2d227178b970e3033442aa9c6b8462ff1b91521d1981f24fc568", "class_name": "RelatedNodeInfo"}}, "text": "Tutorials using Sequential:\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\n\r\n\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Any) \u2013 Input sequence or batch. The input type must be supported by the first transform in the sequence.\r\n\r\n\r\n\r\n\r\nA container to host a sequence of text transforms.\r\nTutorials using Sequential:\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Any) \u2013 Input sequence or batch. The input type must be supported by the first transform in the sequence.\r\n\r\n\r\n\r\nParameters:\r\ninput (Any) \u2013 Input sequence or batch. The input type must be supported by the first transform in the sequence.\r\n\r\ninput (Any) \u2013 Input sequence or batch. The input type must be supported by the first transform in the sequence.\r\nPadTransform\u00b6\r\n\r\nclass torchtext.transforms.PadTransform(max_length: int, pad_value: int)[source]\u00b6\r\nPad tensor to a fixed length with given padding value.\r\n\r\nParameters:\r\n\r\nmax_length (int) \u2013 Maximum length to pad to\r\npad_value (bool) \u2013 Value to pad the tensor with\r\n\r\n\r\n\r\n\r\n\r\nforward(x: Tensor) \u2192 Tensor[source]\u00b6\r\n\r\nParameters:\r\nx (Tensor) \u2013 The tensor to pad\r\n\r\nReturns:\r\nTensor padded up to max_length with pad_value\r\n\r\nReturn type:\r\nTensor\r\n\r\n\r\n\r\n\r\nPad tensor to a fixed length with given padding value.\r\nParameters:\r\n\r\nmax_length (int) \u2013 Maximum length to pad to\r\npad_value (bool) \u2013 Value to pad the tensor with\r\n\r\n\r\nmax_length (int) \u2013 Maximum length to pad to\r\npad_value (bool) \u2013 Value to pad the tensor with\r\n\r\nforward(x: Tensor) \u2192 Tensor[source]\u00b6\r\n\r\nParameters:\r\nx (Tensor) \u2013 The tensor to pad\r\n\r\nReturns:\r\nTensor padded up to max_length with pad_value\r\n\r\nReturn type:\r\nTensor\r\n\r\n\r\n\r\nParameters:\r\nx (Tensor) \u2013 The tensor to pad\r\n\r\nx (Tensor) \u2013 The tensor to pad\r\nReturns:\r\nTensor padded up to max_length with pad_value\r\n\r\nTensor padded up to max_length with pad_value\r\nReturn type:\r\nTensor\r\n\r\nTensor\r\nStrToIntTransform\u00b6\r\n\r\nclass torchtext.transforms.StrToIntTransform[source]\u00b6\r\nConvert string tokens to integers (either single sequence or batch).\r\n\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[List[str], List[List[str]]]) \u2013 sequence or batch of string tokens to convert\r\n\r\nReturns:\r\nsequence or batch converted into corresponding token ids\r\n\r\nReturn type:\r\nUnion[List[int], List[List[int]]]\r\n\r\n\r\n\r\n\r\nConvert string tokens to integers (either single sequence or batch).\r\n\r\nforward(input: Any) \u2192 Any[source]\u00b6\r\n\r\nParameters:\r\ninput (Union[List[str], List[List[str]]]) \u2013 sequence or batch of string tokens to convert\r\n\r\nReturns:\r\nsequence or batch converted into corresponding token ids\r\n\r\nReturn type:\r\nUnion[List[int], List[List[int]]]\r\n\r\n\r\n\r\nParameters:\r\ninput (Union[List[str], List[List[str]]]) \u2013 sequence or batch of string tokens to convert\r\n\r\ninput (Union[List[str], List[List[str]]]) \u2013 sequence or batch of string tokens to convert\r\nReturns:\r\nsequence or batch converted into corresponding token ids\r\n\r\nsequence or batch converted into corresponding token ids\r\nReturn type:\r\nUnion[List[int], List[List[int]]]\r\n\r\nUnion[List[int], List[List[int]]]\r\nCharBPETokenizer\u00b6\r\n\r\nclass torchtext.transforms.CharBPETokenizer(bpe_encoder_path: str, bpe_merges_path: str, return_tokens: bool = False, unk_token: Optional[str] = None, suffix: Optional[str] = None, special_tokens: Optional[List[str]] = None)[source]\u00b6\r\nTransform for a Character Byte-Pair-Encoding Tokenizer.\r\n:param : param bpe_encoder_path: Path to the BPE encoder json file.\r\n:param : type bpe_encoder_path: str\r\n:param : param bpe_merges_path: Path to the BPE merges text file.\r\n:param : type bpe_merges_path: str\r\n:param : param return_tokens: Indicate whether to return split tokens. If False, it will return encoded token IDs (default: False).\r\n:param : type return_tokens: bool\r\n:param : param unk_token: The unknown token. If provided, it must exist in encoder.\r\n:param : type unk_token: Optional[str]\r\n:param : param suffix: The suffix to be used for every subword that is an end-of-word.\r\n:param : type suffix: Optional[str]\r\n:param : param special_tokens: Special tokens which should not be split into individual characters. If provided, these must exist in encoder.", "mimetype": "text/plain", "start_char_idx": 159235, "end_char_idx": 163591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca5d608f-6a76-4cd8-931c-53a5bcdcb3d1": {"__data__": {"id_": "ca5d608f-6a76-4cd8-931c-53a5bcdcb3d1", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec35cec0-59d3-4405-a9c3-34b684cd6e55", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "ddb97d4f9f6d32e548c342ef6ad77367cb36888c5ef59c0a1843e9755002935c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8175003-6a75-44d6-94e4-b54df2003d00", "node_type": "1", "metadata": {}, "hash": "5279c83fe06dac713e42a8ee81d82cca3391cece7efe4b482d03a8157fe9e5e9", "class_name": "RelatedNodeInfo"}}, "text": ":param : param bpe_encoder_path: Path to the BPE encoder json file.\r\n:param : type bpe_encoder_path: str\r\n:param : param bpe_merges_path: Path to the BPE merges text file.\r\n:param : type bpe_merges_path: str\r\n:param : param return_tokens: Indicate whether to return split tokens. If False, it will return encoded token IDs (default: False).\r\n:param : type return_tokens: bool\r\n:param : param unk_token: The unknown token. If provided, it must exist in encoder.\r\n:param : type unk_token: Optional[str]\r\n:param : param suffix: The suffix to be used for every subword that is an end-of-word.\r\n:param : type suffix: Optional[str]\r\n:param : param special_tokens: Special tokens which should not be split into individual characters. If provided, these must exist in encoder.\r\n:param : type special_tokens: Optional[List[str]]\r\n\r\n\r\nforward(input: Union[str, List[str]]) \u2192 Union[List, List[List]][source]\u00b6\r\nForward method of module encodes strings or list of strings into token ids\r\n\r\nParameters:\r\ninput \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\nReturns:\r\nA list or list of lists of token IDs\r\n\r\n\r\n\r\n\r\nTransform for a Character Byte-Pair-Encoding Tokenizer.\r\n:param : param bpe_encoder_path: Path to the BPE encoder json file.\r\n:param : type bpe_encoder_path: str\r\n:param : param bpe_merges_path: Path to the BPE merges text file.\r\n:param : type bpe_merges_path: str\r\n:param : param return_tokens: Indicate whether to return split tokens. If False, it will return encoded token IDs (default: False).\r\n:param : type return_tokens: bool\r\n:param : param unk_token: The unknown token. If provided, it must exist in encoder.\r\n:param : type unk_token: Optional[str]\r\n:param : param suffix: The suffix to be used for every subword that is an end-of-word.\r\n:param : type suffix: Optional[str]\r\n:param : param special_tokens: Special tokens which should not be split into individual characters. If provided, these must exist in encoder.\r\n:param : type special_tokens: Optional[List[str]]\r\n\r\nforward(input: Union[str, List[str]]) \u2192 Union[List, List[List]][source]\u00b6\r\nForward method of module encodes strings or list of strings into token ids\r\n\r\nParameters:\r\ninput \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\nReturns:\r\nA list or list of lists of token IDs\r\n\r\n\r\n\r\nForward method of module encodes strings or list of strings into token ids\r\nParameters:\r\ninput \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\n\r\ninput \u2013 Input sentence or list of sentences on which to apply tokenizer.\r\nReturns:\r\nA list or list of lists of token IDs\r\n\r\nA list or list of lists of token IDs \r\n\r\nsource: https://pytorch.org/text/stable/functional.html \r\ncontent: \r\n\r\ntorchtext.functional\u00b6\r\nto_tensor\u00b6\r\n\r\ntorchtext.functional.to_tensor(input: Any, padding_value: Optional[int] = None, dtype: dtype = torch.int64) \u2192 Tensor[source]\u00b6\r\nConvert input to torch tensor\r\n\r\nParameters:\r\n\r\npadding_value (Optional[int]) \u2013 Pad value to make each input in the batch of length equal to the longest sequence in the batch.\r\ndtype (torch.dtype) \u2013 torch.dtype of output tensor\r\ninput (Union[List[int], List[List[int]]]) \u2013 Sequence or batch of token ids\r\n\r\n\r\nReturn type:\r\nTensor\r\n\r\n\r\n\r\nTutorials using to_tensor:\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\n\r\n\r\nConvert input to torch tensor\r\nParameters:\r\n\r\npadding_value (Optional[int]) \u2013 Pad value to make each input in the batch of length equal to the longest sequence in the batch.\r\ndtype (torch.dtype) \u2013 torch.dtype of output tensor\r\ninput (Union[List[int], List[List[int]]]) \u2013 Sequence or batch of token ids\r\n\r\n\r\npadding_value (Optional[int]) \u2013 Pad value to make each input in the batch of length equal to the longest sequence in the batch.", "mimetype": "text/plain", "start_char_idx": 162823, "end_char_idx": 166604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8175003-6a75-44d6-94e4-b54df2003d00": {"__data__": {"id_": "f8175003-6a75-44d6-94e4-b54df2003d00", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca5d608f-6a76-4cd8-931c-53a5bcdcb3d1", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "d86feecb51909a26ddafd197d638a46fad4f4cc8c394438169edf920bed23275", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09181249-7d8c-4c6a-a831-97cb7b3b9f73", "node_type": "1", "metadata": {}, "hash": "2f8237564eb4eceacee34597be15fbf0f1d5af6a1c5c9f8d4b3447ee63071814", "class_name": "RelatedNodeInfo"}}, "text": "dtype (torch.dtype) \u2013 torch.dtype of output tensor\r\ninput (Union[List[int], List[List[int]]]) \u2013 Sequence or batch of token ids\r\n\r\n\r\nReturn type:\r\nTensor\r\n\r\n\r\n\r\nTutorials using to_tensor:\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\n\r\n\r\nConvert input to torch tensor\r\nParameters:\r\n\r\npadding_value (Optional[int]) \u2013 Pad value to make each input in the batch of length equal to the longest sequence in the batch.\r\ndtype (torch.dtype) \u2013 torch.dtype of output tensor\r\ninput (Union[List[int], List[List[int]]]) \u2013 Sequence or batch of token ids\r\n\r\n\r\npadding_value (Optional[int]) \u2013 Pad value to make each input in the batch of length equal to the longest sequence in the batch.\r\ndtype (torch.dtype) \u2013 torch.dtype of output tensor\r\ninput (Union[List[int], List[List[int]]]) \u2013 Sequence or batch of token ids\r\nReturn type:\r\nTensor\r\n\r\nTensor\r\nTutorials using to_tensor:\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\n\r\nSST-2 Binary text classification with XLM-RoBERTa model\r\ntruncate\u00b6\r\n\r\ntorchtext.functional.truncate(input: Any, max_seq_len: int) \u2192 Any[source]\u00b6\r\nTruncate input sequence or batch\r\n\r\nParameters:\r\n\r\ninput (Union[List[Union[str, int]], List[List[Union[str, int]]]]) \u2013 Input sequence or batch to be truncated\r\nmax_seq_len (int) \u2013 Maximum length beyond which input is discarded\r\n\r\n\r\nReturns:\r\nTruncated sequence\r\n\r\nReturn type:\r\nUnion[List[Union[str, int]], List[List[Union[str, int]]]]\r\n\r\n\r\n\r\nTruncate input sequence or batch\r\nParameters:\r\n\r\ninput (Union[List[Union[str, int]], List[List[Union[str, int]]]]) \u2013 Input sequence or batch to be truncated\r\nmax_seq_len (int) \u2013 Maximum length beyond which input is discarded\r\n\r\n\r\ninput (Union[List[Union[str, int]], List[List[Union[str, int]]]]) \u2013 Input sequence or batch to be truncated\r\nmax_seq_len (int) \u2013 Maximum length beyond which input is discarded\r\nReturns:\r\nTruncated sequence\r\n\r\nTruncated sequence\r\nReturn type:\r\nUnion[List[Union[str, int]], List[List[Union[str, int]]]]\r\n\r\nUnion[List[Union[str, int]], List[List[Union[str, int]]]]\r\nadd_token\u00b6\r\n\r\ntorchtext.functional.add_token(input: Any, token_id: Any, begin: bool = True) \u2192 Any[source]\u00b6\r\nAdd token to start or end of sequence\r\n\r\nParameters:\r\n\r\ninput (Union[List[Union[str, int]], List[List[Union[str, int]]]]) \u2013 Input sequence or batch\r\ntoken_id (Union[str, int]) \u2013 token to be added\r\nbegin (bool, optional) \u2013 Whether to insert token at start or end or sequence, defaults to True\r\n\r\n\r\nReturns:\r\nsequence or batch with token_id added to begin or end or input\r\n\r\nReturn type:\r\nUnion[List[Union[str, int]], List[List[Union[str, int]]]]\r\n\r\n\r\n\r\nAdd token to start or end of sequence\r\nParameters:\r\n\r\ninput (Union[List[Union[str, int]], List[List[Union[str, int]]]]) \u2013 Input sequence or batch\r\ntoken_id (Union[str, int]) \u2013 token to be added\r\nbegin (bool, optional) \u2013 Whether to insert token at start or end or sequence, defaults to True\r\n\r\n\r\ninput (Union[List[Union[str, int]], List[List[Union[str, int]]]]) \u2013 Input sequence or batch\r\ntoken_id (Union[str, int]) \u2013 token to be added\r\nbegin (bool, optional) \u2013 Whether to insert token at start or end or sequence, defaults to True\r\nReturns:\r\nsequence or batch with token_id added to begin or end or input\r\n\r\nsequence or batch with token_id added to begin or end or input\r\nReturn type:\r\nUnion[List[Union[str, int]], List[List[Union[str, int]]]]\r\n\r\nUnion[List[Union[str, int]], List[List[Union[str, int]]]]\r\nstr_to_int\u00b6\r\n\r\ntorchtext.functional.str_to_int(input: Any) \u2192 Any[source]\u00b6\r\nConvert string tokens to integers (either single sequence or batch).\r\n\r\nParameters:\r\ninput (Union[List[str], List[List[str]]]) \u2013 Input sequence or batch\r\n\r\nReturns:\r\nSequence or batch of string tokens converted to integers\r\n\r\nReturn type:\r\nUnion[List[int], List[List[int]]]\r\n\r\n\r\n\r\nConvert string tokens to integers (either single sequence or batch).", "mimetype": "text/plain", "start_char_idx": 165862, "end_char_idx": 169784, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09181249-7d8c-4c6a-a831-97cb7b3b9f73": {"__data__": {"id_": "09181249-7d8c-4c6a-a831-97cb7b3b9f73", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8175003-6a75-44d6-94e4-b54df2003d00", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "92b58a03e372010fbd487dfc509f80eb20d27714b03ada5a6d1944202f628543", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "914a3b82-b51f-49fd-a9a0-a1e5885b5455", "node_type": "1", "metadata": {}, "hash": "8729045e72239d1ea2ca06df1dcbfe7a206e27c5769ffc5b167dd8cd6cd2f726", "class_name": "RelatedNodeInfo"}}, "text": "Parameters:\r\ninput (Union[List[str], List[List[str]]]) \u2013 Input sequence or batch\r\n\r\nReturns:\r\nSequence or batch of string tokens converted to integers\r\n\r\nReturn type:\r\nUnion[List[int], List[List[int]]]\r\n\r\n\r\n\r\nConvert string tokens to integers (either single sequence or batch).\r\nParameters:\r\ninput (Union[List[str], List[List[str]]]) \u2013 Input sequence or batch\r\n\r\ninput (Union[List[str], List[List[str]]]) \u2013 Input sequence or batch\r\nReturns:\r\nSequence or batch of string tokens converted to integers\r\n\r\nSequence or batch of string tokens converted to integers\r\nReturn type:\r\nUnion[List[int], List[List[int]]]\r\n\r\nUnion[List[int], List[List[int]]] \r\n\r\nsource: https://pytorch.org/text/stable/models.html \r\ncontent: \r\n\r\ntorchtext.models\u00b6\r\nRobertaBundle\u00b6\r\n\r\nclass torchtext.models.RobertaBundle(_params: torchtext.models.RobertaEncoderParams, _path: Optional[str] = None, _head: Optional[torch.nn.Module] = None, transform: Optional[Callable] = None)[source]\u00b6\r\n\r\nExample - Pretrained base xlmr encoder>>> import torch, torchtext\r\n>>> from torchtext.functional import to_tensor\r\n>>> xlmr_base = torchtext.models.XLMR_BASE_ENCODER\r\n>>> model = xlmr_base.get_model()\r\n>>> transform = xlmr_base.transform()\r\n>>> input_batch = [\"Hello world\", \"How are you!\"]\r\n>>> model_input = to_tensor(transform(input_batch), padding_value=1)\r\n>>> output = model(model_input)\r\n>>> output.shape\r\ntorch.Size([2, 6, 768])\r\n\r\n\r\n\r\nExample - Pretrained large xlmr encoder attached to un-initialized classification head>>> import torch, torchtext\r\n>>> from torchtext.models import RobertaClassificationHead\r\n>>> from torchtext.functional import to_tensor\r\n>>> xlmr_large = torchtext.models.XLMR_LARGE_ENCODER\r\n>>> classifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)\r\n>>> model = xlmr_large.get_model(head=classifier_head)\r\n>>> transform = xlmr_large.transform()\r\n>>> input_batch = [\"Hello world\", \"How are you!\"]\r\n>>> model_input = to_tensor(transform(input_batch), padding_value=1)\r\n>>> output = model(model_input)\r\n>>> output.shape\r\ntorch.Size([1, 2])\r\n\r\n\r\n\r\nExample - User-specified configuration and checkpoint>>> from torchtext.models import RobertaEncoderConf, RobertaBundle, RobertaClassificationHead\r\n>>> model_weights_path = \"https://download.pytorch.org/models/text/xlmr.base.encoder.pt\"\r\n>>> encoder_conf = RobertaEncoderConf(vocab_size=250002)\r\n>>> classifier_head = RobertaClassificationHead(num_classes=2, input_dim=768)\r\n>>> model = RobertaBundle.build_model(encoder_conf=encoder_conf, head=classifier_head, checkpoint=model_weights_path)\r\n\r\n\r\n\r\n\r\n\r\n\r\nget_model(head: Optional[torch.nn.Module] = None, load_weights: bool = True, freeze_encoder: bool = False, *, dl_kwargs=None) \u2192 torchtext.models.RobertaModel[source]\u00b6\r\n\r\nParameters:\r\n\r\nhead (nn.Module) \u2013 A module to be attached to the encoder to perform specific task. If provided, it will replace the default member head (Default: None)\r\nload_weights (bool) \u2013 Indicates whether or not to load weights if available. (Default: True)\r\nfreeze_encoder (bool) \u2013 Indicates whether or not to freeze the encoder weights. (Default: False)\r\ndl_kwargs (dictionary of keyword arguments) \u2013 Passed to torch.hub.load_state_dict_from_url(). (Default: None)\r\n\r\n\r\n\r\n\r\n\r\nExample - Pretrained base xlmr encoder\r\n>>> import torch, torchtext\r\n>>> from torchtext.functional import to_tensor\r\n>>> xlmr_base = torchtext.models.XLMR_BASE_ENCODER\r\n>>> model = xlmr_base.get_model()\r\n>>> transform = xlmr_base.transform()\r\n>>> input_batch = [\"Hello world\", \"How are you!\"]\r\n>>> model_input = to_tensor(transform(input_batch), padding_value=1)\r\n>>> output = model(model_input)\r\n>>> output.shape\r\ntorch.Size([2, 6, 768])\r\n\r\n\r\n\r\n>>> import torch, torchtext\r\n>>> from torchtext.functional import to_tensor\r\n>>> xlmr_base = torchtext.models.XLMR_BASE_ENCODER\r\n>>> model = xlmr_base.get_model()\r\n>>> transform = xlmr_base.transform()\r\n>>> input_batch = [\"Hello world\", \"How are you!\"]", "mimetype": "text/plain", "start_char_idx": 169507, "end_char_idx": 173435, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "914a3b82-b51f-49fd-a9a0-a1e5885b5455": {"__data__": {"id_": "914a3b82-b51f-49fd-a9a0-a1e5885b5455", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09181249-7d8c-4c6a-a831-97cb7b3b9f73", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "0c41b2d173c84dbf774f9b602f54d7134153740420f3f54670053c738979b42e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9fc2bbb-8c32-4c30-bbb9-5887ac0ee6d9", "node_type": "1", "metadata": {}, "hash": "0f2d5b0a8452fa5e73de261b76b74259fc5d76a9234b83c59e181f27f4c6aa0a", "class_name": "RelatedNodeInfo"}}, "text": "(Default: None)\r\n\r\n\r\n\r\n\r\n\r\nExample - Pretrained base xlmr encoder\r\n>>> import torch, torchtext\r\n>>> from torchtext.functional import to_tensor\r\n>>> xlmr_base = torchtext.models.XLMR_BASE_ENCODER\r\n>>> model = xlmr_base.get_model()\r\n>>> transform = xlmr_base.transform()\r\n>>> input_batch = [\"Hello world\", \"How are you!\"]\r\n>>> model_input = to_tensor(transform(input_batch), padding_value=1)\r\n>>> output = model(model_input)\r\n>>> output.shape\r\ntorch.Size([2, 6, 768])\r\n\r\n\r\n\r\n>>> import torch, torchtext\r\n>>> from torchtext.functional import to_tensor\r\n>>> xlmr_base = torchtext.models.XLMR_BASE_ENCODER\r\n>>> model = xlmr_base.get_model()\r\n>>> transform = xlmr_base.transform()\r\n>>> input_batch = [\"Hello world\", \"How are you!\"]\r\n>>> model_input = to_tensor(transform(input_batch), padding_value=1)\r\n>>> output = model(model_input)\r\n>>> output.shape\r\ntorch.Size([2, 6, 768])\r\n\r\nExample - Pretrained large xlmr encoder attached to un-initialized classification head\r\n>>> import torch, torchtext\r\n>>> from torchtext.models import RobertaClassificationHead\r\n>>> from torchtext.functional import to_tensor\r\n>>> xlmr_large = torchtext.models.XLMR_LARGE_ENCODER\r\n>>> classifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)\r\n>>> model = xlmr_large.get_model(head=classifier_head)\r\n>>> transform = xlmr_large.transform()\r\n>>> input_batch = [\"Hello world\", \"How are you!\"]\r\n>>> model_input = to_tensor(transform(input_batch), padding_value=1)\r\n>>> output = model(model_input)\r\n>>> output.shape\r\ntorch.Size([1, 2])\r\n\r\n\r\n\r\n>>> import torch, torchtext\r\n>>> from torchtext.models import RobertaClassificationHead\r\n>>> from torchtext.functional import to_tensor\r\n>>> xlmr_large = torchtext.models.XLMR_LARGE_ENCODER\r\n>>> classifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)\r\n>>> model = xlmr_large.get_model(head=classifier_head)\r\n>>> transform = xlmr_large.transform()\r\n>>> input_batch = [\"Hello world\", \"How are you!\"]\r\n>>> model_input = to_tensor(transform(input_batch), padding_value=1)\r\n>>> output = model(model_input)\r\n>>> output.shape\r\ntorch.Size([1, 2])\r\n\r\nExample - User-specified configuration and checkpoint\r\n>>> from torchtext.models import RobertaEncoderConf, RobertaBundle, RobertaClassificationHead\r\n>>> model_weights_path = \"https://download.pytorch.org/models/text/xlmr.base.encoder.pt\"\r\n>>> encoder_conf = RobertaEncoderConf(vocab_size=250002)\r\n>>> classifier_head = RobertaClassificationHead(num_classes=2, input_dim=768)\r\n>>> model = RobertaBundle.build_model(encoder_conf=encoder_conf, head=classifier_head, checkpoint=model_weights_path)\r\n\r\n\r\n\r\n>>> from torchtext.models import RobertaEncoderConf, RobertaBundle, RobertaClassificationHead\r\n>>> model_weights_path = \"https://download.pytorch.org/models/text/xlmr.base.encoder.pt\"\r\n>>> encoder_conf = RobertaEncoderConf(vocab_size=250002)\r\n>>> classifier_head = RobertaClassificationHead(num_classes=2, input_dim=768)\r\n>>> model = RobertaBundle.build_model(encoder_conf=encoder_conf, head=classifier_head, checkpoint=model_weights_path)\r\n\r\n\r\nget_model(head: Optional[torch.nn.Module] = None, load_weights: bool = True, freeze_encoder: bool = False, *, dl_kwargs=None) \u2192 torchtext.models.RobertaModel[source]\u00b6\r\n\r\nParameters:\r\n\r\nhead (nn.Module) \u2013 A module to be attached to the encoder to perform specific task. If provided, it will replace the default member head (Default: None)\r\nload_weights (bool) \u2013 Indicates whether or not to load weights if available. (Default: True)\r\nfreeze_encoder (bool) \u2013 Indicates whether or not to freeze the encoder weights. (Default: False)\r\ndl_kwargs (dictionary of keyword arguments) \u2013 Passed to torch.hub.load_state_dict_from_url(). (Default: None)\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nhead (nn.Module) \u2013 A module to be attached to the encoder to perform specific task. If provided, it will replace the default member head (Default: None)\r\nload_weights (bool) \u2013 Indicates whether or not to load weights if available. (Default: True)\r\nfreeze_encoder (bool) \u2013 Indicates whether or not to freeze the encoder weights. (Default: False)\r\ndl_kwargs (dictionary of keyword arguments) \u2013 Passed to torch.hub.load_state_dict_from_url(). (Default: None)\r\n\r\n\r\nhead (nn.Module) \u2013 A module to be attached to the encoder to perform specific task.", "mimetype": "text/plain", "start_char_idx": 172710, "end_char_idx": 176993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9fc2bbb-8c32-4c30-bbb9-5887ac0ee6d9": {"__data__": {"id_": "f9fc2bbb-8c32-4c30-bbb9-5887ac0ee6d9", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "914a3b82-b51f-49fd-a9a0-a1e5885b5455", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "c5ce4a48e76ff062f8bc7793bb2a23e284958e1c259eb51e3b9458c532084c47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92cb0d67-d62e-4b38-98df-eeb64760146f", "node_type": "1", "metadata": {}, "hash": "3833618276547125e6b08bbdb07106e2684f839a87c5388af1b7e99bb06d5b01", "class_name": "RelatedNodeInfo"}}, "text": "(Default: True)\r\nfreeze_encoder (bool) \u2013 Indicates whether or not to freeze the encoder weights. (Default: False)\r\ndl_kwargs (dictionary of keyword arguments) \u2013 Passed to torch.hub.load_state_dict_from_url(). (Default: None)\r\n\r\n\r\n\r\n\r\nParameters:\r\n\r\nhead (nn.Module) \u2013 A module to be attached to the encoder to perform specific task. If provided, it will replace the default member head (Default: None)\r\nload_weights (bool) \u2013 Indicates whether or not to load weights if available. (Default: True)\r\nfreeze_encoder (bool) \u2013 Indicates whether or not to freeze the encoder weights. (Default: False)\r\ndl_kwargs (dictionary of keyword arguments) \u2013 Passed to torch.hub.load_state_dict_from_url(). (Default: None)\r\n\r\n\r\nhead (nn.Module) \u2013 A module to be attached to the encoder to perform specific task. If provided, it will replace the default member head (Default: None)\r\nload_weights (bool) \u2013 Indicates whether or not to load weights if available. (Default: True)\r\nfreeze_encoder (bool) \u2013 Indicates whether or not to freeze the encoder weights. (Default: False)\r\ndl_kwargs (dictionary of keyword arguments) \u2013 Passed to torch.hub.load_state_dict_from_url(). (Default: None)\r\nXLMR_BASE_ENCODER\u00b6\r\n\r\ntorchtext.models.XLMR_BASE_ENCODER\u00b6\r\nXLM-R Encoder with Base configuration\r\nThe XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation Learning\r\nat Scale <https://arxiv.org/abs/1911.02116>. It is a large multi-lingual language model,\r\ntrained on 2.5TB of filtered CommonCrawl data and based on the RoBERTa model architecture.\r\nOriginally published by the authors of XLM-RoBERTa under MIT License\r\nand redistributed with the same license.\r\n[License,\r\nSource]\r\nPlease refer to torchtext.models.RobertaBundle() for the usage.\r\n\r\nXLM-R Encoder with Base configuration\r\nThe XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation Learning\r\nat Scale <https://arxiv.org/abs/1911.02116>. It is a large multi-lingual language model,\r\ntrained on 2.5TB of filtered CommonCrawl data and based on the RoBERTa model architecture.\r\nOriginally published by the authors of XLM-RoBERTa under MIT License\r\nand redistributed with the same license.\r\n[License,\r\nSource]\r\nPlease refer to torchtext.models.RobertaBundle() for the usage.\r\nXLMR_LARGE_ENCODER\u00b6\r\n\r\ntorchtext.models.XLMR_LARGE_ENCODER\u00b6\r\nXLM-R Encoder with Large configuration\r\nThe XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation Learning\r\nat Scale <https://arxiv.org/abs/1911.02116>. It is a large multi-lingual language model,\r\ntrained on 2.5TB of filtered CommonCrawl data and based on the RoBERTa model architecture.\r\nOriginally published by the authors of XLM-RoBERTa under MIT License\r\nand redistributed with the same license.\r\n[License,\r\nSource]\r\nPlease refer to torchtext.models.RobertaBundle() for the usage.\r\n\r\nXLM-R Encoder with Large configuration\r\nThe XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation Learning\r\nat Scale <https://arxiv.org/abs/1911.02116>. It is a large multi-lingual language model,\r\ntrained on 2.5TB of filtered CommonCrawl data and based on the RoBERTa model architecture.\r\nOriginally published by the authors of XLM-RoBERTa under MIT License\r\nand redistributed with the same license.\r\n[License,\r\nSource]\r\nPlease refer to torchtext.models.RobertaBundle() for the usage.\r\nROBERTA_BASE_ENCODER\u00b6\r\n\r\ntorchtext.models.ROBERTA_BASE_ENCODER\u00b6\r\nRoberta Encoder with Base configuration\r\nRoBERTa iterates on BERT\u2019s pretraining procedure, including training the model longer,\r\nwith bigger batches over more data; removing the next sentence prediction objective;\r\ntraining on longer sequences; and dynamically changing the masking pattern applied\r\nto the training data.\r\nThe RoBERTa model was pretrained on the reunion of five datasets: BookCorpus,\r\nEnglish Wikipedia, CC-News, OpenWebText, and STORIES. Together theses datasets\r\ncontain over a 160GB of text.\r\nOriginally published by the authors of RoBERTa under MIT License\r\nand redistributed with the same license.\r\n[License,\r\nSource]\r\nPlease refer to torchtext.models.RobertaBundle() for the usage.", "mimetype": "text/plain", "start_char_idx": 176200, "end_char_idx": 180293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92cb0d67-d62e-4b38-98df-eeb64760146f": {"__data__": {"id_": "92cb0d67-d62e-4b38-98df-eeb64760146f", "embedding": null, "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ecd805b4-e6ab-42ec-90d2-1d1d05af982a", "node_type": "4", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "84de6c130d0bc0dc860be1d0c866a874cef7728909db4048e908d583e0dce8dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9fc2bbb-8c32-4c30-bbb9-5887ac0ee6d9", "node_type": "1", "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}, "hash": "6c7fa566cfa63122310040afb6442c734a0eb55dbea20329ef307daf61aaba1a", "class_name": "RelatedNodeInfo"}}, "text": "[License,\r\nSource]\r\nPlease refer to torchtext.models.RobertaBundle() for the usage.\r\nROBERTA_BASE_ENCODER\u00b6\r\n\r\ntorchtext.models.ROBERTA_BASE_ENCODER\u00b6\r\nRoberta Encoder with Base configuration\r\nRoBERTa iterates on BERT\u2019s pretraining procedure, including training the model longer,\r\nwith bigger batches over more data; removing the next sentence prediction objective;\r\ntraining on longer sequences; and dynamically changing the masking pattern applied\r\nto the training data.\r\nThe RoBERTa model was pretrained on the reunion of five datasets: BookCorpus,\r\nEnglish Wikipedia, CC-News, OpenWebText, and STORIES. Together theses datasets\r\ncontain over a 160GB of text.\r\nOriginally published by the authors of RoBERTa under MIT License\r\nand redistributed with the same license.\r\n[License,\r\nSource]\r\nPlease refer to torchtext.models.RobertaBundle() for the usage.\r\n\r\nRoberta Encoder with Base configuration\r\nRoBERTa iterates on BERT\u2019s pretraining procedure, including training the model longer,\r\nwith bigger batches over more data; removing the next sentence prediction objective;\r\ntraining on longer sequences; and dynamically changing the masking pattern applied\r\nto the training data.\r\nThe RoBERTa model was pretrained on the reunion of five datasets: BookCorpus,\r\nEnglish Wikipedia, CC-News, OpenWebText, and STORIES. Together theses datasets\r\ncontain over a 160GB of text.\r\nOriginally published by the authors of RoBERTa under MIT License\r\nand redistributed with the same license.\r\n[License,\r\nSource]\r\nPlease refer to torchtext.models.RobertaBundle() for the usage.\r\nROBERTA_LARGE_ENCODER\u00b6\r\n\r\ntorchtext.models.ROBERTA_LARGE_ENCODER\u00b6\r\nRoberta Encoder with Large configuration\r\nRoBERTa iterates on BERT\u2019s pretraining procedure, including training the model longer,\r\nwith bigger batches over more data; removing the next sentence prediction objective;\r\ntraining on longer sequences; and dynamically changing the masking pattern applied\r\nto the training data.\r\nThe RoBERTa model was pretrained on the reunion of five datasets: BookCorpus,\r\nEnglish Wikipedia, CC-News, OpenWebText, and STORIES. Together theses datasets\r\ncontain over a 160GB of text.\r\nOriginally published by the authors of RoBERTa under MIT License\r\nand redistributed with the same license.\r\n[License,\r\nSource]\r\nPlease refer to torchtext.models.RobertaBundle() for the usage.\r\n\r\nRoberta Encoder with Large configuration\r\nRoBERTa iterates on BERT\u2019s pretraining procedure, including training the model longer,\r\nwith bigger batches over more data; removing the next sentence prediction objective;\r\ntraining on longer sequences; and dynamically changing the masking pattern applied\r\nto the training data.\r\nThe RoBERTa model was pretrained on the reunion of five datasets: BookCorpus,\r\nEnglish Wikipedia, CC-News, OpenWebText, and STORIES. Together theses datasets\r\ncontain over a 160GB of text.\r\nOriginally published by the authors of RoBERTa under MIT License\r\nand redistributed with the same license.\r\n[License,\r\nSource]\r\nPlease refer to torchtext.models.RobertaBundle() for the usage.", "mimetype": "text/plain", "start_char_idx": 179440, "end_char_idx": 182481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"ecd805b4-e6ab-42ec-90d2-1d1d05af982a": {"node_ids": ["5ed5ad09-e30c-4058-8415-6f804f47e3a5", "7f5207e9-ddc6-4215-ab0f-d6c553dd9ebf", "457eab00-ae9f-4aac-a4e7-4a98408e91e1", "685ee7e0-d8c4-42c6-bb8c-19c1b3ab4395", "154af448-4dda-4800-8055-adcd44e2d197", "e84af432-ebde-4b71-85f6-add7b8ae6593", "a0fda24d-c163-498d-987c-f1e996539c97", "47d4ec04-876d-4cf0-aa87-6b06343d8884", "2a39ee67-88e3-4253-91e9-96233076bf73", "688399e1-d7bc-48a3-8be2-881b291cd407", "a041d4ab-217e-449c-95ca-6f91b6317ff6", "5c1f2183-5922-4fe0-804f-593e9c0d9d85", "1d2edc3f-2dcb-4019-97c2-85b8dcb7cecd", "d203d24e-5c30-4c7a-82df-f3b314f76f11", "32a73738-50d7-41de-a0f8-7a8cd117786e", "a294d79f-e526-4713-bfd6-e5e73f9e49b1", "1c5ab448-de9d-4498-b7fb-012b9d6262e5", "086119a5-f3c9-41f9-a6de-d934cb12b77c", "69b0579f-7229-4d74-ac02-e13e2087cd82", "aac24df6-58b5-435a-9eb6-4b916c966e7d", "4c52c881-b5ce-4d79-9ea6-3f3d97441844", "55a675a8-2c34-4f9f-adfd-bbc8e5ad8fdd", "421aaa4f-7ad1-47b4-9bff-f330df2dffd4", "ae9e09b0-7373-4230-9bb2-86a20ab091f1", "0689df79-d968-47e4-85fc-532331bff96e", "f5d4dfd2-32e3-4e93-8da2-97575adec23c", "a55c566e-ded2-44c7-b36e-061f82aad713", "02cd274e-3074-4dac-a2a6-5c04d705c5ad", "827479cb-6872-498c-8cbf-61fa5c524d19", "0de8490c-abed-4e40-9875-2542ddd06182", "ba8be7fd-b71f-4059-aa52-3370a5d560d3", "f28292fb-4254-42b0-9560-78964ac5b5bd", "e7c47131-e38c-4f01-aa72-92b29686df5c", "a55f34b5-bee4-485c-95c2-8f43a072c6f3", "d5492f8c-6c8d-4791-ada3-4cf81c8e038f", "35d60436-502c-40ac-900b-3d46e992427b", "86eb3f8e-3809-45cb-b9f0-b163198b8f72", "0578511b-a91a-4dac-bb2c-c2ce505cd162", "26c3f524-6755-477e-9cb9-4183fffdcb1b", "4792cdb9-9ff0-4132-ab23-000c4c0b0b9a", "8abe0411-3437-46ec-9d4e-eae3466c8df5", "9fa1f032-dec8-461c-8ff0-6dadce612a1f", "cac3f1f9-740b-42c1-a2b0-9632a8124bd0", "11c964a7-4218-47a2-9133-779c576bc2d7", "94eebc7d-8f60-48dc-90c7-268222f56ee5", "1410b72d-0a68-4708-814b-1e3869f653ea", "ec49ba9f-d67c-4366-8bc5-fc4dee8e8b31", "56471f04-f7d5-4c56-8e46-64c330e2b09c", "e8548960-ae21-4f44-a467-ab3a9e3485e9", "ec35cec0-59d3-4405-a9c3-34b684cd6e55", "ca5d608f-6a76-4cd8-931c-53a5bcdcb3d1", "f8175003-6a75-44d6-94e4-b54df2003d00", "09181249-7d8c-4c6a-a831-97cb7b3b9f73", "914a3b82-b51f-49fd-a9a0-a1e5885b5455", "f9fc2bbb-8c32-4c30-bbb9-5887ac0ee6d9", "92cb0d67-d62e-4b38-98df-eeb64760146f"], "metadata": {"file_path": "data\\raw\\api_content.txt", "file_name": "api_content.txt", "file_type": "text/plain", "file_size": 185030, "creation_date": "2024-10-30", "last_modified_date": "2024-10-30"}}}}